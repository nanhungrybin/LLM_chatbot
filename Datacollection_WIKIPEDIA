import requests
from bs4 import BeautifulSoup
import json
import os
from datetime import datetime
from urllib.parse import quote

def get_wikipedia_content(keyword, user_agent, language='ko'):
    encoded_keyword = quote(keyword)
    url = f"https://{language}.wikipedia.org/wiki/{encoded_keyword}"
    headers = {'User-Agent': user_agent}

    response = requests.get(url, headers=headers)
    if response.status_code == 200:
        soup = BeautifulSoup(response.text, 'html.parser')

        # HTML에서 텍스트 콘텐츠 추출
        # 'mw-parser-output' 클래스는 위키피디아 페이지의 본문 콘텐츠를 포함하고 있음
        content_div = soup.find('div', class_='mw-parser-output')
        paragraphs = content_div.find_all('p')
        text_content = ' '.join([paragraph.get_text() for paragraph in paragraphs])

        # 메타데이터 생성
        metadata = {
            "Topic": keyword,
            "Source URL": url,
            "Date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "Language": language
        }

        # 결과 데이터와 메타데이터를 포함한 딕셔너리 생성
        result = {
            "Content": text_content,
            "Metadata": metadata
        }

        return result
    else:
        return {"Error": "Page not found or other error occurred"}


def save_data_to_json(data, directory, filename):
    if not os.path.exists(directory):
        os.makedirs(directory)

    filepath = os.path.join(directory, filename)
    with open(filepath, 'w', encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)
    print(f"Data saved in {filepath}")


if __name__ == "__main__":
    
    user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36'
    language_names = {"ko": "Korean", "en": "English"}
    languages = {"Korean": "ko", "English": "en"}
    language_label = "Korean"  # Or "English"
    language = languages[language_label]

    TopicKeywords = {
        "Korean": ["사출성형", "용접", "단조", "프레스"],
        "English": ["Injection Molding", "Welding", "Forging", "Pressing"]
    }

    selected_keywords = TopicKeywords[language_label]

    for keyword in selected_keywords:
        data = get_wikipedia_content(keyword, language)
        directory = os.path.join(language_label, keyword.replace(" ", "_"))  # Replace spaces with underscores for directory names
        filename = f"{keyword}_wikipedia_ALLcontents.json"
        save_data_to_json(data, directory, filename)
