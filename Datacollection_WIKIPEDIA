"""https://github.com/martin-majlis/Wikipedia-API/tree/master"""
import os
from datetime import datetime
from urllib.parse import quote
import openai
import wikipediaapi
import json

def is_english(text):
    try:
        text.encode(encoding='utf-8').decode('ascii')
    except UnicodeDecodeError:
        return False
    else:
        return True

def translate_to_korean(text, chatmodel):
    response = openai.chat.completions.create(
        model=chatmodel,
        messages=[
            {"role": "system", "content": "Translate the following English text to Korean:"},
            {"role": "user", "content": text}
        ]
    )
    response_text = response.choices[0].message.content

    return response_text

def get_wikipedia_content(keyword, language):
    encoded_keyword = quote(keyword)
    url = f"https://{language}.wikipedia.org/wiki/{encoded_keyword}"
    
    wiki_wiki = wikipediaapi.Wikipedia(
    user_agent='MyProjectName (merlin@example.com)',
        language='en',
        extract_format=wikipediaapi.ExtractFormat.WIKI
    )

    p_wiki = wiki_wiki.page(keyword)
    text_content = p_wiki.text

    if text_content:
        metadata = {
            "Topic": keyword,
            "Source URL": p_wiki.fullurl,
            "Date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "Language": language
        }

        result = {
            "Content": text_content,
            "Metadata": metadata
        }

        return result
    else:
        return {"Error": "Page not found or other error occurred"}

def save_data_to_json(data, directory, filename):
    if not os.path.exists(directory):
        os.makedirs(directory)

    filepath = os.path.join(directory, filename)
    with open(filepath, 'w', encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)
    print(f"Data saved in {filepath}")

if __name__ == "__main__":
    language_names = {"ko": "Korean", "en": "English"}
    languages = {"Korean": "ko", "English": "en"}

    language_label = "English"

    TopicKeywords = {
        "Korean": ["사출성형"],
        "English": ["Injection Molding"]
    }

    selected_keywords = TopicKeywords[language_label]

    
    OPENAI_API_KEY = "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"
    os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY
    chatmodel = "gpt-3.5-turbo"

    for keyword in selected_keywords:
        # Determine if the keyword is in English
        if is_english(keyword):
            data = get_wikipedia_content(keyword, languages["English"])
            # Translate to Korean if the content is originally in English
            if "Error" not in data:
                translated_content = translate_to_korean(data['Content'], chatmodel)
                data['Content'] = translated_content  # Translate
                data['Metadata']['Language'] = "ko"  # Update language in metadata
        else:
            # If the keyword is already in Korean, fetch the content without translating
            data = get_wikipedia_content(keyword, languages["Korean"])

        if "Error" not in data:
            directory = os.path.join("Wikipedia_Content", keyword.replace(" ", "_"))
            filename = f"{keyword}_wikipedia_content.json"
            save_data_to_json(data, directory, filename)
        else:
            print(f"Failed to retrieve content for {keyword}")
