{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"//anaconda/envs/azureml_py38/bin/python\")\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertTokenizerFast, BertForMaskedLM, BertConfig, AdamW\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "from datasets import load_dataset\n",
        "import os\n",
        "from datetime import datetime\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/llm-rag-embeddings/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1715216215361
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 경로 설정 : 데이터를 읽어오는 곳\n",
        "    \n",
        "path = \"/home/azureuser/cloudfiles/code/Users/hb.suh/사출성형/tokenizer_data/\"\n",
        "\n",
        "all_texts = []\n",
        "\n",
        "# 폴더 내의 모든 txt 파일의 데이터를 모음\n",
        "for file in os.listdir(path):\n",
        "    if file.endswith('.txt'):\n",
        "        file_path = os.path.join(path, file)\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            all_texts.append(f.read())\n",
        "\n",
        "file_paths = [os.path.join(path, file) for file in os.listdir(path) if file.endswith('.txt')] #list\n",
        "\n",
        "\n",
        "# 데이터셋 로드\n",
        "dataset = load_dataset('text', data_files=file_paths, cache_dir='/home/azureuser/cloudfiles/code/Users/hb.suh/cache')"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1715216217489
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import BertWordPieceTokenizer\n",
        "\n",
        "# BertWordPieceTokenizer 인스턴스 생성\n",
        "tokenizer = BertWordPieceTokenizer(clean_text=True, handle_chinese_chars=True, strip_accents=False, lowercase=False, wordpieces_prefix=\"##\")\n",
        "\n",
        "# 훈련하기\n",
        "tokenizer.train(\n",
        "    file_paths,\n",
        "    vocab_size=10000,\n",
        "    min_frequency=10,\n",
        "    show_progress=True,\n",
        "    special_tokens = [\"[PAD]\", \"[CLS]\", \"[UNK]\", \"[SEP]\", \"[MASK]\"],\n",
        "    wordpieces_prefix=\"##\",\n",
        ")\n",
        "\n",
        "\n",
        "# 저장할 디렉토리 경로    \n",
        "save_path = \"/home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/trained_tokenizer/0508test_bertwordpiece\"\n",
        "\n",
        "# 디렉토리가 없으면 생성\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "\n",
        "# 토크나이저 모델 저장\n",
        "tokenizer.save_model(save_path)\n",
        "\n",
        "# from tokenizers import ByteLevelBPETokenizer\n",
        "\n",
        "# # ByteLevelBPETokenizer 인스턴스 생성\n",
        "# tokenizer = ByteLevelBPETokenizer()\n",
        "\n",
        "# # 훈련하기\n",
        "# tokenizer.train(\n",
        "#     files=file_paths,  # 훈련할 텍스트 파일 경로 리스트\n",
        "#     vocab_size=10000,  # 어휘 사전의 최대 크기\n",
        "#     min_frequency=10,  # 토큰이 어휘 사전에 포함되기 위한 최소 빈도\n",
        "#     show_progress=True,  # 진행 상황 표시\n",
        "#     special_tokens=[\"[PAD]\", \"[CLS]\", \"[UNK]\", \"[SEP]\", \"[MASK]\"]  # 특수 토큰 추가\n",
        "# )\n",
        "\n",
        "# # 저장할 디렉토리 경로\n",
        "# save_path = \"/home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/trained_tokenizer/0508test_bertwordpiece\"\n",
        "\n",
        "# # 디렉토리가 없으면 생성\n",
        "# if not os.path.exists(save_path):\n",
        "#     os.makedirs(save_path)\n",
        "\n",
        "# # tokenizer 모델 저장\n",
        "# tokenizer.save_model(save_path)\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\n\n\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "['/home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/trained_tokenizer/0508test_bertwordpiece/vocab.txt']"
          },
          "metadata": {}
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1715175047255
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#vocab_file_path = \"/home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/trained_tokenizer/vocab.txt\"\n",
        "\n",
        "# 새로운 토크나이저 로드\n",
        "new_tokenizer = BertTokenizerFast.from_pretrained('beomi/kcbert-base')\n",
        "model = BertForMaskedLM.from_pretrained('beomi/kcbert-base')\n",
        "len(new_tokenizer)\n",
        "\n",
        "new_tokens_added = 0\n",
        "new_vocab = {}  # 새로운 토큰과 아이디를 담을 딕셔너리\n",
        "\n",
        "path = \"/home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/trained_tokenizer/0508test_bertwordpiece/vocab.txt\"\n",
        "# 기존의 토크나이저 vocab에서 새로운 토큰들을 제외하고 아이디 부여\n",
        "with open(path, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        token = line.strip()  # 줄바꿈 문자를 제거하여 토큰만 가져옴\n",
        "        if token not in new_tokenizer.get_vocab():\n",
        "            new_tokenizer.add_tokens(token)\n",
        "            new_tokens_added += 1\n",
        "            new_vocab[token] = len(new_tokenizer) - 1  # 새로운 토큰에 새로운 아이디 부여\n",
        "\n",
        "        else:\n",
        "            # 토큰이 이미 존재하면, 기존 ID 사용\n",
        "            new_vocab[token] = new_tokenizer.convert_tokens_to_ids(token)\n",
        "\n",
        "print(\"new_tokens_added : \", new_tokens_added)\n",
        "print(len(new_tokenizer))\n",
        "\n",
        "# 토크나이저 모델 파일 저장\n",
        "tokenizer_model_path = \"/home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/trained_tokenizer/0508test_bertwordpiece\"\n",
        "new_tokenizer.save_pretrained(tokenizer_model_path)\n",
        "\n",
        "len(new_tokenizer)\n",
        "\n",
        "print(f\"Added {new_tokens_added} new tokens\")\n",
        "\n",
        "#  vocab.txt 또는 tokenizer.json 파일 중 하나만 있어도 토크나이저를 성공적으로 로드하고 사용할 수 있습니다. \n",
        "\n",
        "# # 새로운 토큰과 아이디를 vocab.txt 파일에 저장\n",
        "# with open(tokenizer_model_path + \"/vocab.txt\", 'a', encoding='utf-8') as lf:\n",
        "#     for token, idx in new_vocab.items():\n",
        "#         lf.write(f\"{token}\\t{idx}\\n\")\n",
        "\n",
        "# print(f\"Added {new_tokens_added} new tokens\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "new_tokens_added :  6196\n36196\nAdded 6196 new tokens\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1715175344927
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TEST"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import BertTokenizerFast\n",
        "# 토큰화에 사용된 토큰라이저를 로드\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"/home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/trained_tokenizer/0508test_bertwordpiece/\", do_lower_case=False)\n",
        "\n",
        "def tokenize_data(tokenizer, dataset):\n",
        "    def tokenize_function(examples):\n",
        "        # tokenizer의 결과에서 'input_ids'와 'attention_mask'만 반환하도록 수정 #128\n",
        "        tokenized_output = tokenizer(examples['text'], max_length=300, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "        return {'input_ids': tokenized_output['input_ids'], 'attention_mask': tokenized_output['attention_mask']}\n",
        "    return dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n",
        "    \n",
        "train_dataset = dataset['train']\n",
        "tokenized_datasets = tokenize_data(tokenizer, train_dataset)\n",
        "\n",
        "\n",
        "\n",
        "# 이미 토큰화된 데이터셋을 불러옵니다. 예를 들어, 이전 단계에서 정의한 `tokenized_datasets` 변수 사용\n",
        "# 토큰화된 데이터셋의 일부 샘플을 추출합니다.\n",
        "samples = tokenized_datasets.shuffle(seed=42).select(range(5))  # 5개의 무작위 샘플을 추출\n",
        "\n",
        "\n",
        "\n",
        "# 각 샘플에 대해 토큰 ID와 디코딩된 텍스트를 출력\n",
        "for i, sample in enumerate(samples):\n",
        "    input_ids = sample['input_ids']\n",
        "    print(f\"Sample {i+1} -- Input IDs: {input_ids}\")\n",
        "    decoded_text = tokenizer.decode(input_ids)\n",
        "    print(f\"Decoded Text: {decoded_text}\\n\")\n",
        "\n",
        "# 이 코드는 데이터셋이 이미 메모리에 로드되어 있어야 하며, `select` 및 `shuffle` 메소드를 사용할 수 있는 상황이어야 합니다.\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Sample 1 -- Input IDs: [2, 8436, 15, 24258, 9810, 4008, 30894, 33989, 20277, 4113, 11212, 10302, 17283, 1463, 27195, 17, 32102, 30126, 9679, 32387, 12197, 11017, 15, 33605, 1476, 15705, 25294, 12091, 13001, 8455, 30630, 31262, 30327, 12905, 8422, 17, 10384, 15, 30630, 22664, 4113, 8738, 21837, 30126, 30678, 30299, 8215, 8422, 17, 17026, 8122, 15, 30972, 30302, 30630, 9961, 15, 21837, 30419, 31331, 30862, 9846, 8455, 21837, 33138, 9846, 8021, 8422, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nDecoded Text: [CLS] 먼저, 재료 처리는 사출성형의 품질과 성능에 매우 중요한 영향을 미칩니다. 재료는 사출성형 전에 건조 되어야 하며, 수분 및 기타 불순물을 제거하여 재료의 품질을 향상 시켜야 합니다. 또한, 재료의 성질에 따라 적절한 사출성형 조건을 설정 해야 합니다. 예를 들어, 열가소성 고분자 재료의 경우, 적절한 가열 온도와 압력을 유지하여 적절한 유동성을 유지해야 합니다. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n\nSample 2 -- Input IDs: [2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nDecoded Text: [CLS] [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n\nSample 3 -- Input IDs: [2, 30126, 11, 30061, 30052, 30067, 30057, 15, 30163, 34927, 12, 2420, 31688, 902, 31515, 2424, 2627, 23929, 24396, 24704, 12670, 8741, 28658, 18352, 8717, 12939, 34802, 17, 2355, 16122, 8738, 35244, 97, 34575, 902, 2979, 4057, 30296, 11, 30165, 30173, 31130, 12, 9056, 28897, 15784, 4091, 31606, 786, 17, 32168, 34584, 11, 73, 31220, 30736, 12, 9056, 7996, 2979, 4027, 653, 4008, 313, 4128, 31334, 1265, 35042, 686, 8185, 30419, 3167, 15, 686, 4118, 33296, 9962, 9185, 32527, 15, 11499, 4047, 31340, 32765, 30276, 8741, 34883, 31945, 8120, 17, 9496, 2979, 4017, 34584, 11614, 2458, 32158, 2154, 32404, 9215, 30419, 3167, 7965, 8413, 8790, 17, 33630, 32527, 1201, 1469, 8086, 19237, 686, 4327, 8790, 17, 686, 4118, 34806, 26329, 30118, 30752, 36078, 2627, 4572, 18040, 34230, 786, 17, 2627, 24408, 2195, 18337, 30153, 9215, 31640, 34230, 34806, 306, 30488, 11, 1, 30053, 12, 9215, 10273, 8070, 34238, 2627, 4572, 4023, 36078, 3196, 4349, 4413, 10399, 17, 32168, 2451, 21423, 10273, 8070, 12535, 8304, 8249, 27107, 8251, 28621, 17, 2451, 19738, 8251, 8195, 30185, 30771, 750, 13821, 8481, 2536, 4039, 97, 3016, 97, 2219, 14911, 4180, 2398, 4039, 8455, 29565, 4167, 4203, 97, 26741, 13888, 963, 8741, 9016, 4009, 21511, 4020, 17, 31003, 14518, 30875, 750, 29971, 2219, 4163, 36193, 1478, 4027, 8278, 2177, 1931, 32143, 29971, 4017, 34230, 3518, 8805, 8032, 8544, 27152, 4020, 17, 8530, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nDecoded Text: [CLS] 사출성형 ( 射 出 成 型, injection moulding ) 은 용해 된 물질 을 주형에 주입시킴으로써 여러 부분을 만들기 위한 제조 공정이다. 용도에 따라 착색 · 배합 된 칩은 사출성형기 ( Injection molding machine ) 라고 불리는 기계로 성형된 다. 사출성형기는 호퍼 ( h op per ) 라고 하는 칩을 넣는 곳과 원료 를 가열하여 녹이는 가열 통, 녹인 플라스틱을 밀어내는 스크루, 목적한 형태로 성형하는 금형의 여러 부분으로 이루어져 있다. 우선 칩이 호퍼 에서 일 정량 씩 계량 되어 가열 통으로 보내진다. 여기에서 스크루 로 밀리는 사이에 녹여진다. 녹인 플라스틱은 다음으로 사출 노즐 로부터 주형 속으로 사출된 다. 주형은 애당초 냉각 되어 있으므로 사출된 플라스틱은 곧 고화 ( [UNK] 化 ) 되어 자동적으로 성형품이 주형으 로부터 튀겨져 나온다. 사출성형기는 이 조작을 자동적으로 반복하면서 계속 제품을 만들어 낸다. 이 방법으로 만들어지는 제품의 종류 는 다양하며 접시 · 컵 · 양동이를 위시하여 텔레비전 · 라디오 부품 등 여러 가지가 만들어진다. 사출성형에 의한 제품에 는 플라스틱 양동 이의 밑을 보면 알 수 있듯이 플라스틱이 사출된 흔적이 있는 것이 특징이다. 역사 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n\nSample 4 -- Input IDs: [2, 30126, 31545, 20187, 2270, 4087, 15, 30869, 15, 30302, 30501, 963, 18262, 20090, 4128, 10070, 4027, 35642, 8982, 17, 9250, 15, 20187, 2270, 4087, 8097, 32591, 750, 32381, 17193, 4128, 17977, 30949, 24913, 15, 30869, 20090, 4057, 30419, 321, 30153, 35128, 1843, 4042, 2275, 15559, 1355, 4598, 4198, 4569, 4027, 8226, 7966, 847, 8539, 8063, 17, 10384, 15, 30302, 34747, 8014, 21106, 32495, 33328, 321, 31583, 30583, 8226, 7966, 847, 16220, 4022, 8074, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nDecoded Text: [CLS] 사출성형 공정은 유체 역학, 열전달, 고분자 물성 등 다양한 이론과 개념을 포함하고 있습니다. 특히, 유체 역학적인 측면에서 는 유동의 흐름과 압력 분포를 이해하고, 열전달 이론은 가열 과 냉각 과정에 서의 열 전달 메커니즘을 이해하는 데 중요합니다. 또한, 고분자 물성에 대한 이해는 플라스틱의 유동성 과 경화 특성을 이해하는 데 필수적입니다. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n\nSample 5 -- Input IDs: [2, 30126, 32393, 14402, 16708, 7971, 10302, 31042, 26408, 8982, 17, 2451, 32393, 30972, 30302, 32033, 30368, 18262, 27107, 32044, 847, 9021, 9345, 17, 16702, 30875, 750, 11621, 13888, 15, 29411, 10519, 15, 8774, 22422, 15, 11928, 14657, 21597, 10427, 9345, 17, 30126, 32393, 27107, 35212, 32271, 1931, 8711, 15, 33986, 26079, 4078, 15, 18247, 4042, 29406, 4009, 18705, 4008, 32406, 8569, 8982, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nDecoded Text: [CLS] 사출성형 기술은 글로벌 제조업에서 중요한 위치를 차지하고 있습니다. 이 기술은 열가소성 고분자 소재를 사용하여 다양한 제품을 생산하는 데 사용됩니다. 이러한 제품에 는 자동차 부품, 가전제품, 의료기기, 포장 소재 등이 포함됩니다. 사출성형 기술은 제품을 대량으로 생산할 수 있고, 생산성이 뛰어나며, 디자인의 자유도가 높다는 장점을 가지고 있습니다. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1715216612106
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. 데이터셋 토큰화\n",
        "\n",
        "def tokenize_data(tokenizer, dataset):\n",
        "    def tokenize_function(examples):\n",
        "        # tokenizer의 결과에서 'input_ids'와 'attention_mask'만 반환하도록 수정 #128\n",
        "        tokenized_output = tokenizer(examples['text'], max_length=300, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "        return {'input_ids': tokenized_output['input_ids'], 'attention_mask': tokenized_output['attention_mask']}\n",
        "    return dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n",
        "\n",
        "    \n",
        "\n",
        "def resize_and_initialize_embeddings(model, tokenizer, old_tokenizer=None):\n",
        "    # 모델의 원래 토큰 임베딩 크기\n",
        "    original_num_tokens = model.config.vocab_size if old_tokenizer is None else len(old_tokenizer)\n",
        "    \n",
        "    # 새 토크나이저에 따른 임베딩 크기 조정\n",
        "    new_num_tokens = len(tokenizer)\n",
        "    model.resize_token_embeddings(new_num_tokens)\n",
        "\n",
        "    # 임베딩 레이어에 접근\n",
        "    embeddings = model.get_input_embeddings()\n",
        "    \n",
        "    # 새로운 토큰이 추가되었을 경우\n",
        "    if new_num_tokens > original_num_tokens:\n",
        "        # 기존 임베딩의 평균과 표준편차 계산\n",
        "        with torch.no_grad():\n",
        "            old_embeddings = embeddings.weight[:original_num_tokens]\n",
        "            mean, std = old_embeddings.mean(dim=0), old_embeddings.std(dim=0)\n",
        "            # 새로운 토큰의 임베딩 초기화\n",
        "            new_embeddings = torch.randn(new_num_tokens - original_num_tokens, embeddings.embedding_dim)\n",
        "            new_embeddings = mean + std * new_embeddings\n",
        "            embeddings.weight[original_num_tokens:] = new_embeddings\n",
        "\n",
        "    return model\n",
        "\n",
        "# import torch.nn as nn\n",
        "# import torch\n",
        "# # def resize_and_initialize_embedding_matrix(model, tokenizer, emb_mtx_name='bert.embeddings.word_embeddings.weight'):\n",
        "\n",
        "\n",
        "# after_add_Trained_Tokenizer = BertTokenizerFast.from_pretrained(\"/home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/trained_tokenizer/TEST_bertwordpiece/\")\n",
        "# emb_mtx_name='bert.embeddings.word_embeddings.weight'\n",
        "# # 모델의 원래 토큰 임베딩 크기\n",
        "# num_orig_tokens = model.config.vocab_size\n",
        "\n",
        "# # 새 토크나이저에 따른 임베딩 크기 조정\n",
        "# new_num_tokens = len(after_add_Trained_Tokenizer)\n",
        "# model.resize_token_embeddings(new_num_tokens)\n",
        "\n",
        "# #########################################################\n",
        "# # 임베딩 레이어의 현재 크기 확인\n",
        "# current_embedding_shape = model.get_input_embeddings().weight.size()\n",
        "# print(\"Current embedding shape:\", current_embedding_shape)\n",
        "\n",
        "# # 조정된 크기와 예상 크기 비교\n",
        "# expected_shape = (new_num_tokens, model.config.hidden_size)\n",
        "# print(\"Expected embedding shape:\", expected_shape)\n",
        "# #########################################################\n",
        "\n",
        "\n",
        "# # emb_mtx_name을 '.'을 기준으로 분할하여 각 부분을 참조\n",
        "# parts = emb_mtx_name.split('.')\n",
        "\n",
        "# # parts 리스트를 사용하여 모델에서 원하는 객체를 순차적으로 얻음\n",
        "# embedding_layer = model\n",
        "# for part in parts[:-1]:  # 마지막 부분(weight)은 제외하고 객체를 얻음\n",
        "#     embedding_layer = getattr(embedding_layer, part)\n",
        "\n",
        "# # embeddings 변수는 이미 임베딩 레이어를 참조\n",
        "# embeddings = embedding_layer\n",
        "# old_num_embeddings, embedding_dim = embeddings.weight.size()\n",
        "\n",
        "# # 새로운 임베딩 레이어 생성\n",
        "# new_embedding_layer = nn.Embedding(new_num_tokens, embedding_dim)\n",
        "\n",
        "# # 기존 임베딩 레이어의 가중치를 새 레이어에 복사\n",
        "# with torch.no_grad():\n",
        "#     new_embedding_layer.weight[:old_num_embeddings] = embeddings.weight\n",
        "\n",
        "# # 추가된 토큰 수 계산\n",
        "# num_added_tokens = new_num_tokens - old_num_embeddings\n",
        "\n",
        "# # 기존 임베딩의 통계치를 이용하여 새로운 임베딩 초기화\n",
        "# if num_added_tokens > 0:\n",
        "#     pre_expansion_embeddings = embeddings.weight[:num_orig_tokens, :]\n",
        "#     mu = pre_expansion_embeddings.mean(dim=0)\n",
        "#     std = pre_expansion_embeddings.std(dim=0)\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         new_embedding_layer.weight[old_num_embeddings:] = mu +  std * torch.randn(num_added_tokens, embedding_dim)\n",
        "\n",
        "# # 새로운 임베딩 레이어로 모델 업데이트\n",
        "# setattr(embedding_layer, 'weight', new_embedding_layer.weight)\n",
        "\n",
        "# #return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###############################################################\n",
        "after_add_Trained_Tokenizer = BertTokenizerFast.from_pretrained(\"/home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/trained_tokenizer/0508test_bertwordpiece/\",do_lower_case=False)\n",
        "\n",
        "model = resize_and_initialize_embeddings(model, after_add_Trained_Tokenizer )\n",
        "#model = resize_and_initialize_embedding_matrix(model, after_add_Trained_Tokenizer,emb_mtx_name='bert.embeddings.word_embeddings.weight')\n",
        "print(len(after_add_Trained_Tokenizer))\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "36196\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1715175403974
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "def train_model(model, train_dataset, tokenizer, EPOCH, early_stopping_patience = None):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    \n",
        "    # 옵티마이저와 스케줄러를 설정합니다.\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6)\n",
        "    total_steps = len(train_dataset) * EPOCH \n",
        "    \n",
        "    # 학습률이 주기적으로 높아졌다가 낮아지도록 설정되며, \n",
        "    # 처음에는 낮은 학습률에서 시작하여 점차 학습률을 높인 뒤 다시 낮추는 방식으로 진행\n",
        "    scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, \n",
        "                                                  base_lr=1e-6,  # 최소 학습률\n",
        "                                                  max_lr=5e-5,  # 최대 학습률\n",
        "                                                  step_size_up=total_steps // 4,  # 학습률이 증가하는 스텝 수\n",
        "                                                  step_size_down=total_steps - total_steps // 4,  # 감소하는 스텝 수\n",
        "                                                  mode='triangular',\n",
        "                                                  cycle_momentum=False)  # 'triangular', 'triangular2' 또는 'exp_range'\n",
        "    \n",
        "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=16, collate_fn=data_collator)\n",
        "    model.train()\n",
        "\n",
        "    losses = []\n",
        "    best_loss = float('inf')\n",
        "    patience_counter = 0  # for the early stopping\n",
        "\n",
        "    for epoch in range(EPOCH):\n",
        "\n",
        "        epoch_losses = []  \n",
        "\n",
        "        for batch in train_loader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "            \n",
        "            loss.backward()\n",
        "            \n",
        "            # 그라디언트 클리핑을 적용하여 그라디언트 폭발을 방지합니다.\n",
        "            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            \n",
        "            optimizer.step()\n",
        "            scheduler.step()  # 학습률을 업데이트합니다.\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            epoch_losses.append(loss.item())  # 현재 배치의 loss 값을 저장\n",
        "\n",
        "        # 현재 epoch의 평균 loss 값을 계산하여 출력\n",
        "        epoch_loss_mean = sum(epoch_losses) / len(epoch_losses)\n",
        "        print(f\"Epoch {epoch + 1} completed, Loss: {loss.item()}, Current LR: {scheduler.get_last_lr()[0]}\")\n",
        "        losses.extend(epoch_losses)\n",
        "\n",
        "        # early stopping 체크\n",
        "        if epoch_loss_mean < best_loss:\n",
        "            best_loss = epoch_loss_mean\n",
        "            patience_counter = 0  # 성능이 개선되면 카운터 초기화\n",
        "        else:\n",
        "            patience_counter += 1  # 성능이 개선되지 않으면 카운터 증가\n",
        "\n",
        "        if patience_counter >= early_stopping_patience:\n",
        "            print(f\"Early stopping at epoch {epoch + 1} as no improvement in loss.\")\n",
        "            break\n",
        "\n",
        "\n",
        "    # Loss 함수 시각화\n",
        "    plt.plot(losses)\n",
        "    plt.xlabel('Batch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Loss Trend')\n",
        "    plt.show()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def save_model(model, tokenizer, pretrained_model_name):\n",
        "\n",
        "    safe_model_name = pretrained_model_name.replace(\"/\", \"-\")\n",
        "    # 현재 날짜와 시간을 포함한 디렉토리 이름을 생성\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "    directory = f\"/home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/MLM_trained_model/{safe_model_name}_CyclicLRtriangular-{timestamp}\"\n",
        "\n",
        "    # 디렉토리가 존재하지 않으면 생성\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "    # 모델과 토크나이저를 사전 훈련된 디렉토리에 저장\n",
        "    model.save_pretrained(directory)\n",
        "    tokenizer.save_pretrained(directory)\n",
        "\n",
        "\n",
        "train_dataset = dataset['train']\n",
        "tokenized_datasets = tokenize_data(after_add_Trained_Tokenizer, train_dataset)\n",
        "\n",
        "# 모델 학습 \n",
        "model = train_model(model, tokenized_datasets, after_add_Trained_Tokenizer , 100, early_stopping_patience=5)\n",
        "\n",
        "print(len(after_add_Trained_Tokenizer))\n",
        "\n",
        "# 모델 저장\n",
        "save_model(model, after_add_Trained_Tokenizer, \"kcbert-base\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Map: 100%|██████████| 942/942 [00:09<00:00, 95.28 examples/s]\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1 completed, Loss: 5.416138172149658, Current LR: 1.1227600849256998e-06\nEpoch 2 completed, Loss: 5.624236106872559, Current LR: 1.2455201698513996e-06\nEpoch 3 completed, Loss: 5.355045318603516, Current LR: 1.3682802547770558e-06\nEpoch 4 completed, Loss: 5.138874053955078, Current LR: 1.4910403397027556e-06\nEpoch 5 completed, Loss: 4.933863162994385, Current LR: 1.6138004246284554e-06\nEpoch 6 completed, Loss: 5.288161754608154, Current LR: 1.7365605095541552e-06\nEpoch 7 completed, Loss: 4.97595739364624, Current LR: 1.8593205944798116e-06\nEpoch 8 completed, Loss: 5.589491844177246, Current LR: 1.982080679405511e-06\nEpoch 9 completed, Loss: 5.293214797973633, Current LR: 2.104840764331211e-06\nEpoch 10 completed, Loss: 4.690286159515381, Current LR: 2.2276008492569107e-06\nEpoch 11 completed, Loss: 5.132070064544678, Current LR: 2.3503609341826105e-06\nEpoch 12 completed, Loss: 5.292284965515137, Current LR: 2.473121019108267e-06\nEpoch 13 completed, Loss: 4.534366130828857, Current LR: 2.595881104033967e-06\nEpoch 14 completed, Loss: 4.835070610046387, Current LR: 2.7186411889596667e-06\nEpoch 15 completed, Loss: 4.780516147613525, Current LR: 2.8414012738853666e-06\nEpoch 16 completed, Loss: 4.955456733703613, Current LR: 2.9641613588110223e-06\nEpoch 17 completed, Loss: 5.40580940246582, Current LR: 3.086921443736722e-06\nEpoch 18 completed, Loss: 4.406534194946289, Current LR: 3.209681528662422e-06\nEpoch 19 completed, Loss: 4.597288131713867, Current LR: 3.332441613588122e-06\nEpoch 20 completed, Loss: 4.547402858734131, Current LR: 3.4552016985138216e-06\nEpoch 21 completed, Loss: 5.010339736938477, Current LR: 3.5779617834394782e-06\nEpoch 22 completed, Loss: 4.596078395843506, Current LR: 3.700721868365178e-06\nEpoch 23 completed, Loss: 4.833019733428955, Current LR: 3.823481953290878e-06\nEpoch 24 completed, Loss: 3.984130382537842, Current LR: 3.946242038216578e-06\nEpoch 25 completed, Loss: 4.731934070587158, Current LR: 4.0690021231422335e-06\nEpoch 26 completed, Loss: 4.735256671905518, Current LR: 4.191762208067933e-06\nEpoch 27 completed, Loss: 4.767232418060303, Current LR: 4.314522292993633e-06\nEpoch 28 completed, Loss: 4.9054670333862305, Current LR: 4.437282377919333e-06\nEpoch 29 completed, Loss: 4.59790563583374, Current LR: 4.560042462845033e-06\nEpoch 30 completed, Loss: 4.261028289794922, Current LR: 4.682802547770689e-06\nEpoch 31 completed, Loss: 4.658421993255615, Current LR: 4.805562632696389e-06\nEpoch 32 completed, Loss: 4.434822082519531, Current LR: 4.928322717622089e-06\nEpoch 33 completed, Loss: 4.140870094299316, Current LR: 5.051082802547789e-06\nEpoch 34 completed, Loss: 4.241423606872559, Current LR: 5.1738428874734446e-06\nEpoch 35 completed, Loss: 4.160433769226074, Current LR: 5.296602972399144e-06\nEpoch 36 completed, Loss: 4.2798309326171875, Current LR: 5.419363057324844e-06\nEpoch 37 completed, Loss: 4.5770087242126465, Current LR: 5.542123142250544e-06\nEpoch 38 completed, Loss: 4.13612699508667, Current LR: 5.664883227176201e-06\nEpoch 39 completed, Loss: 4.1920084953308105, Current LR: 5.7876433121019005e-06\nEpoch 40 completed, Loss: 4.225115776062012, Current LR: 5.9104033970276e-06\nEpoch 41 completed, Loss: 4.083554267883301, Current LR: 6.0331634819533e-06\nEpoch 42 completed, Loss: 3.988969326019287, Current LR: 6.155923566879e-06\nEpoch 43 completed, Loss: 3.9459662437438965, Current LR: 6.278683651804656e-06\nEpoch 44 completed, Loss: 4.169326305389404, Current LR: 6.4014437367303555e-06\nEpoch 45 completed, Loss: 4.070165157318115, Current LR: 6.524203821656055e-06\nEpoch 46 completed, Loss: 3.8247692584991455, Current LR: 6.646963906581755e-06\nEpoch 47 completed, Loss: 4.106493949890137, Current LR: 6.769723991507412e-06\nEpoch 48 completed, Loss: 3.8754453659057617, Current LR: 6.8924840764331116e-06\nEpoch 49 completed, Loss: 3.7512035369873047, Current LR: 7.015244161358811e-06\nEpoch 50 completed, Loss: 3.92214298248291, Current LR: 7.138004246284511e-06\nEpoch 51 completed, Loss: 3.8723907470703125, Current LR: 7.260764331210211e-06\nEpoch 52 completed, Loss: 3.844837188720703, Current LR: 7.383524416135867e-06\nEpoch 53 completed, Loss: 3.8058078289031982, Current LR: 7.506284501061567e-06\nEpoch 54 completed, Loss: 3.831043243408203, Current LR: 7.6290445859872664e-06\nEpoch 55 completed, Loss: 3.938011884689331, Current LR: 7.751804670912966e-06\nEpoch 56 completed, Loss: 3.5023372173309326, Current LR: 7.874564755838623e-06\nEpoch 57 completed, Loss: 3.4661922454833984, Current LR: 7.997324840764324e-06\nEpoch 58 completed, Loss: 3.688948392868042, Current LR: 8.120084925690023e-06\nEpoch 59 completed, Loss: 3.3445279598236084, Current LR: 8.242845010615723e-06\nEpoch 60 completed, Loss: 3.5676169395446777, Current LR: 8.365605095541422e-06\nEpoch 61 completed, Loss: 3.7684872150421143, Current LR: 8.488365180467079e-06\nEpoch 62 completed, Loss: 3.4395086765289307, Current LR: 8.611125265392778e-06\nEpoch 63 completed, Loss: 3.078331708908081, Current LR: 8.733885350318478e-06\nEpoch 64 completed, Loss: 3.6601600646972656, Current LR: 8.856645435244179e-06\nEpoch 65 completed, Loss: 3.841081380844116, Current LR: 8.979405520169834e-06\nEpoch 66 completed, Loss: 3.652981758117676, Current LR: 9.102165605095535e-06\nEpoch 67 completed, Loss: 3.29304575920105, Current LR: 9.224925690021234e-06\nEpoch 68 completed, Loss: 3.449864149093628, Current LR: 9.347685774946934e-06\nEpoch 69 completed, Loss: 3.4679863452911377, Current LR: 9.470445859872633e-06\nEpoch 70 completed, Loss: 3.753070831298828, Current LR: 9.59320594479829e-06\nEpoch 71 completed, Loss: 3.306871175765991, Current LR: 9.71596602972399e-06\nEpoch 72 completed, Loss: 3.219059467315674, Current LR: 9.83872611464969e-06\nEpoch 73 completed, Loss: 3.4766671657562256, Current LR: 9.96148619957539e-06\nEpoch 74 completed, Loss: 3.0852723121643066, Current LR: 1.0084246284501045e-05\nEpoch 75 completed, Loss: 3.4448864459991455, Current LR: 1.0207006369426746e-05\nEpoch 76 completed, Loss: 3.13436222076416, Current LR: 1.0329766454352445e-05\nEpoch 77 completed, Loss: 2.9681856632232666, Current LR: 1.0452526539278145e-05\nEpoch 78 completed, Loss: 3.3226094245910645, Current LR: 1.0575286624203802e-05\nEpoch 79 completed, Loss: 2.953632354736328, Current LR: 1.0698046709129501e-05\nEpoch 80 completed, Loss: 3.0412583351135254, Current LR: 1.0820806794055202e-05\nEpoch 81 completed, Loss: 2.945753574371338, Current LR: 1.09435668789809e-05\nEpoch 82 completed, Loss: 3.1104562282562256, Current LR: 1.1066326963906601e-05\nEpoch 83 completed, Loss: 2.860131025314331, Current LR: 1.1189087048832256e-05\nEpoch 84 completed, Loss: 3.072817087173462, Current LR: 1.1311847133757957e-05\nEpoch 85 completed, Loss: 3.0189785957336426, Current LR: 1.1434607218683656e-05\nEpoch 86 completed, Loss: 2.6357574462890625, Current LR: 1.1557367303609357e-05\nEpoch 87 completed, Loss: 2.6958043575286865, Current LR: 1.1680127388535013e-05\nEpoch 88 completed, Loss: 3.135761260986328, Current LR: 1.1802887473460712e-05\nEpoch 89 completed, Loss: 3.040928602218628, Current LR: 1.1925647558386413e-05\nEpoch 90 completed, Loss: 2.9597158432006836, Current LR: 1.2048407643312112e-05\nEpoch 91 completed, Loss: 2.8686227798461914, Current LR: 1.2171167728237812e-05\nEpoch 92 completed, Loss: 2.899676561355591, Current LR: 1.2293927813163467e-05\nEpoch 93 completed, Loss: 2.8842716217041016, Current LR: 1.2416687898089168e-05\nEpoch 94 completed, Loss: 2.998769521713257, Current LR: 1.2539447983014867e-05\nEpoch 95 completed, Loss: 2.6823716163635254, Current LR: 1.2662208067940568e-05\nEpoch 96 completed, Loss: 3.138432502746582, Current LR: 1.2784968152866224e-05\nEpoch 97 completed, Loss: 2.6307411193847656, Current LR: 1.2907728237791923e-05\nEpoch 98 completed, Loss: 2.8165290355682373, Current LR: 1.3030488322717624e-05\nEpoch 99 completed, Loss: 2.6997482776641846, Current LR: 1.3153248407643323e-05\nEpoch 100 completed, Loss: 2.820880174636841, Current LR: 1.3276008492569024e-05\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 640x480 with 1 Axes>",
            "image/png": "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"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "36196\n"
        }
      ],
      "execution_count": 34,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1715179376486
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 기본 : 98 epoch, 0.079\n",
        "- 표준편차를 조정 : 30 epoch, 0.093\n",
        "- weights freeze 시키면 : 20 epoch , 0.136295\n",
        "- 레퍼런스 코드 따라한 기본: 188 epoch , 0.07938"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## for test"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "토큰학습"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "토큰 추가 후 최종 토크나이저"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizerFast\n",
        "text = '  공정변수를 진공성형의 사출성형해석에 요인배치로써 공급부에 중요한 역할을 제공한다. '\n",
        "tokenizer =BertTokenizerFast.from_pretrained(\"/home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/trained_tokenizer/0508test_bertwordpiece/\",do_lower_case=False)\n",
        "# 텍스트 토큰화\n",
        "tokenized_output = tokenizer.encode(text, add_special_tokens=False)\n",
        "print(tokenized_output)\n",
        "print(\"tokenizer len:\",len(tokenizer))\n",
        "# 토큰화 결과 출력\n",
        "for output in tokenized_output:\n",
        "    decoded_output = tokenizer.decode(output)\n",
        "    print(\"Decoded Text:\", decoded_output)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[34904, 31027, 33480, 30994, 2255, 31029, 17238, 14639, 11144, 15328, 10302, 17572, 13248, 8008, 17]\ntokenizer len: 36196\nDecoded Text: 공정변수를\nDecoded Text: 진공\nDecoded Text: 성형의\nDecoded Text: 사출성형해석\nDecoded Text: 에\nDecoded Text: 요인\nDecoded Text: 배치\nDecoded Text: ##로써\nDecoded Text: 공급\nDecoded Text: ##부에\nDecoded Text: 중요한\nDecoded Text: 역할을\nDecoded Text: 제공\nDecoded Text: ##한다\nDecoded Text: .\n"
        }
      ],
      "execution_count": 35,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1715179613825
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "text = [\"공정변수가 진공성형의 사출성형해석에 요인배치로써 공급부에 중요한 역할을 제공한다.\"]\n",
        "tokenizer = BertTokenizer.from_pretrained('/home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/trained_tokenizer/bertwordpiece', do_basic_tokenize=False)\n",
        "# 텍스트 토큰화\n",
        "tokenized_output = tokenizer.encode(str(text), add_special_tokens=True)\n",
        "print(tokenized_output)\n",
        "\n",
        "# 토큰화 결과 출력\n",
        "for output in tokenized_output:\n",
        "    decoded_output = tokenizer.decode(output, skip_special_tokens=True)\n",
        "    print(\"Decoded Text:\", decoded_output)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[2, 1, 1, 1785, 4208, 4093, 4572, 4032, 27667, 1, 1, 10302, 17572, 1, 3]\nDecoded Text: [ C L S ]\nDecoded Text: [ U N K ]\nDecoded Text: [ U N K ]\nDecoded Text: 사\nDecoded Text: # # 출\nDecoded Text: # # 성\nDecoded Text: # # 형\nDecoded Text: # # 해\nDecoded Text: # # 석 에\nDecoded Text: [ U N K ]\nDecoded Text: [ U N K ]\nDecoded Text: 중 요 한\nDecoded Text: 역 할 을\nDecoded Text: [ U N K ]\nDecoded Text: [ S E P ]\n"
        }
      ],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1715046387913
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizerFast\n",
        "text = '  공정변수를 진공성형의 사출성형해석에 요인배치로써 공급부에 중요한 역할을 제공한다. '\n",
        "tokenizer =BertTokenizerFast.from_pretrained(\"/home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/trained_tokenizer/0508test_bertwordpiece/\",do_lower_case=False)\n",
        "# 텍스트 토큰화\n",
        "tokenized_output = tokenizer.encode(text, add_special_tokens=False)\n",
        "print(tokenized_output)\n",
        "print(\"tokenizer len:\",len(tokenizer))\n",
        "# 토큰화 결과 출력\n",
        "for output in tokenized_output:\n",
        "    decoded_output = tokenizer.decode(output)\n",
        "    print(\"Decoded Text:\", decoded_output)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[34904, 31027, 33480, 30994, 2255, 31029, 17238, 14639, 11144, 15328, 10302, 17572, 13248, 8008, 17]\ntokenizer len: 36196\nDecoded Text: 공정변수를\nDecoded Text: 진공\nDecoded Text: 성형의\nDecoded Text: 사출성형해석\nDecoded Text: 에\nDecoded Text: 요인\nDecoded Text: 배치\nDecoded Text: ##로써\nDecoded Text: 공급\nDecoded Text: ##부에\nDecoded Text: 중요한\nDecoded Text: 역할을\nDecoded Text: 제공\nDecoded Text: ##한다\nDecoded Text: .\n"
        }
      ],
      "execution_count": 20,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1715176094596
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 새 토큰의 ID 확인\n",
        "token_id = tokenizer.convert_tokens_to_ids(\"공정변수\")\n",
        "print(\"Token ID for '공정변수':\", token_id)\n",
        "\n",
        "# 직접 인코딩과 디코딩을 통한 검증\n",
        "encoded_text = tokenizer.encode(\"공정변수\", add_special_tokens=False)\n",
        "decoded_text = tokenizer.decode(encoded_text)\n",
        "print(\"Encoded and Decoded Text:\", decoded_text)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Token ID for '공정변수': 31021\nEncoded and Decoded Text: 공정변수\n"
        }
      ],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1715175503784
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_str = ' [CLS] 공정변수를 진공성형의 사출성형해석에 요인배치로써 공급부에 중요한 역할을 제공한다. [SEP]'\n",
        "print('테스트 문장: ',test_str)\n",
        "\n",
        "encoded_str = tokenizer.encode(test_str,add_special_tokens=False)\n",
        "print('문장 인코딩: ',encoded_str)\n",
        "\n",
        "decoded_str = tokenizer.decode(encoded_str)\n",
        "print('문장 디코딩: ',decoded_str)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "테스트 문장:   [CLS] 공정변수를 진공성형의 사출성형해석에 요인배치로써 공급부에 중요한 역할을 제공한다. [SEP]\n문장 인코딩:  [2, 34904, 31027, 33480, 30994, 2255, 31029, 17238, 14639, 11144, 15328, 10302, 17572, 13248, 8008, 17, 3]\n문장 디코딩:  [CLS] 공정변수를 진공 성형의 사출성형해석 에 요인 배치로써 공급부에 중요한 역할을 제공한다. [SEP]\n"
        }
      ],
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1715175558144
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained(\"/home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/trained_tokenizer/0508test_bertwordpiece/\")\n",
        "def chunk_text(text, chunk_size=5100):\n",
        "    token_ids = tokenizer.encode(text, add_special_tokens=False)  # 특수 토큰을 제외하고 인코딩\n",
        "    return [token_ids[i:i + chunk_size] for i in range(0, len(token_ids), chunk_size)]\n",
        "\n",
        "# 예시 텍스트\n",
        "text = \"공정변수를 진공성형의 사출성형해석에 요인배치로써 공급부에 중요한 역할을 제공한다.\"\n",
        "\n",
        "# 텍스트를 청크로 나누기\n",
        "chunks = chunk_text(text)\n",
        "\n",
        "# 각 청크를 별도로 처리\n",
        "for chunk in chunks:\n",
        "    inputs = tokenizer.decode(chunk, skip_special_tokens=True)  # 토큰 ID를 다시 텍스트로 디코드\n",
        "    print(inputs)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "사출성형해석 에 중요한 역할을 제공한다.\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1715143978330
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## vocab add 안하고 우리 데이터로 학습만 한 토크나이저 결과"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "text = ' [CLS] 공정변수를 진공성형의 사출성형해석에 요인배치로써 공급부에 중요한 역할을 제공한다. [SEP]'\n",
        "tokenizer = BertTokenizer(\"/home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/trained_tokenizer/CUSTOM_Tokenizer/vocab.txt\",do_lower_case=False)\n",
        "#tokenizer =BertTokenizerFast.from_pretrained(\"/home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/trained_tokenizer/0508test_bertwordpiece/vocab.txt\",do_lower_case=False)# 텍스트 토큰화\n",
        "tokenized_output = tokenizer.encode(text, add_special_tokens=False)\n",
        "print(tokenized_output)\n",
        "\n",
        "print(\"tokenizer len:\",len(tokenizer))\n",
        "# 토큰화 결과 출력\n",
        "for output in tokenized_output:\n",
        "    decoded_output = tokenizer.decode(output)\n",
        "    print(\"Decoded Text:\", decoded_output)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[1, 8456, 14466, 1040, 14881, 14467, 5104, 14469, 1073, 2768, 3854, 11219, 18, 3]\ntokenizer len: 30000\nDecoded Text: [ C L S ]\nDecoded Text: 공 정 변 수 를\nDecoded Text: 진 공 성 형\nDecoded Text: # # 의\nDecoded Text: 사 출 성 형 해 석 에\nDecoded Text: 요 인 배 치\nDecoded Text: # # 로 써\nDecoded Text: 공 급 부\nDecoded Text: # # 에\nDecoded Text: 중 요 한\nDecoded Text: 역 할 을\nDecoded Text: 제 공 한 다\nDecoded Text: .\nDecoded Text: [ S E P ]\n"
        }
      ],
      "execution_count": 31,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1715176815672
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.get_vocab()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "{'우수하여': 13450,\n '작동': 3975,\n '영업': 7323,\n '##되어있다': 18397,\n 'ASME': 10102,\n '##FRP': 26927,\n '##ences': 4572,\n '##92': 10148,\n '냉각효율을': 22847,\n '나가는': 25020,\n 'Seo': 8274,\n '##인공신경망': 24749,\n '론본': 4401,\n 'Noh': 15645,\n '##oving': 6388,\n '신흥': 26734,\n '##문의': 20647,\n 'HIPS': 7829,\n '19': 1984,\n '##펜': 1645,\n '##치면서': 26958,\n 'prepar': 6736,\n '301': 14053,\n 'only': 6764,\n '제조하는': 7518,\n 'stationary': 16886,\n '수치해석의': 29667,\n '##oost': 27404,\n '이축': 25112,\n '10°C': 12635,\n '4번': 19516,\n '##alled': 9686,\n '균형을': 8550,\n '평균휘도': 27375,\n '##약적인': 28888,\n '##echn': 2251,\n '포함하여': 9807,\n 'defined': 11405,\n '고강도': 13891,\n '분석된다': 14027,\n '함유된': 9704,\n '##밸브': 4778,\n 'hy': 3769,\n 'Che': 6278,\n '가스채널을': 9152,\n '인수': 13314,\n '입구에서의': 22109,\n '이내': 8965,\n '회사': 7276,\n '수리': 12560,\n '440': 18079,\n '9월에': 28290,\n '##08': 28776,\n '##Engineers': 13092,\n 'd1': 19565,\n '직전의': 26800,\n '65MPa': 24392,\n 'dec': 3894,\n 'Viscoelastic': 13003,\n '높': 303,\n 'image': 5299,\n '사슬의': 23423,\n 'Lines': 24897,\n 'rem': 5753,\n 'Polymer': 2684,\n '##FC': 9770,\n '경험적': 10845,\n '사출금형을': 16188,\n '장착하여': 12260,\n 'Cent': 23557,\n 'Body': 21298,\n 'BA': 15407,\n '000톤': 12467,\n '164': 19178,\n 'Pmax': 19550,\n '동년': 22493,\n '이론밀도': 27519,\n '##기': 1085,\n '방식이': 8816,\n '##본': 1358,\n 'Prism': 17822,\n '다음이': 19896,\n 'jp': 13555,\n '6월에': 22356,\n '상에서': 12124,\n '처리에': 29648,\n '이렇': 5154,\n '수행된': 5690,\n '##Jong': 28949,\n '특수': 6836,\n '##80': 3185,\n 'Foaming': 21174,\n '##촉매': 17095,\n '##판의': 4725,\n 'Lai': 28339,\n '##onent': 22778,\n 'Jersey': 26486,\n '##개발이': 21895,\n '월드라인': 11308,\n '끊': 264,\n 'Ret': 22402,\n 'agent': 14512,\n '##비중': 25329,\n '알기': 15936,\n '나': 268,\n '##17': 5158,\n '내로': 9618,\n '1974': 11806,\n '좁은': 7926,\n '주효과': 10282,\n '경기': 20397,\n '##processes': 24433,\n '대표적이다': 29570,\n '답': 329,\n '영향은': 7714,\n '장성': 14649,\n '70mm': 24292,\n '설계': 1994,\n '경화온도': 23196,\n '##램프': 14346,\n '변화가': 3662,\n 'Qu': 8944,\n '##ears': 7524,\n '마찰이': 25982,\n '##생산을': 14422,\n 'Mechanical': 3359,\n '##사출성형공정에서': 25504,\n 'tog': 10972,\n 'DSC': 10880,\n '타이바에': 17334,\n '외': 680,\n '차지': 4303,\n '공정으로서': 24023,\n '증대되고': 29934,\n '##광': 1266,\n 'Proced': 29146,\n 'CD': 6397,\n 'Was': 19563,\n '때문이라': 24091,\n 'flowability': 18446,\n '370': 18544,\n '##각을': 13354,\n '단': 325,\n 'dimensionless': 11601,\n '##오차가': 16034,\n '##hen': 2604,\n '##평가관리': 16775,\n '폴리에틸렌': 8044,\n 'patterning': 27585,\n '반응': 2806,\n '2와': 3973,\n '박막': 5924,\n '저장소': 27693,\n '변수는': 14431,\n '패널': 8241,\n '절에서': 18927,\n '충전하고': 25473,\n '분석해': 29181,\n 'single': 10112,\n '##간에': 12158,\n '##anc': 4897,\n '핏': 940,\n '헵탄용매': 17373,\n '기구설계': 20159,\n '##기계': 2842,\n 'graphen': 24964,\n '##펄': 1800,\n '연구내용': 21712,\n 'pstatic': 18833,\n '본격': 13032,\n 'Fusion': 19532,\n 'Make': 19545,\n '분석하였다': 4078,\n '##uel': 12150,\n '버를': 26695,\n 'Structural': 14453,\n 'KSAE': 14145,\n '360': 10366,\n '##짧': 1851,\n '##me': 12811,\n '##모양': 9306,\n '차이인': 29256,\n '힌': 1001,\n '잔류벽': 11182,\n '입주': 20498,\n 'Computational': 17823,\n '물체를': 23705,\n 'No': 2218,\n '관찰되었다': 11591,\n 'Z': 62,\n '주안': 23779,\n '쌍대': 16990,\n '용융시킨': 29196,\n '##시스템이': 12231,\n '##프로락': 13757,\n 'Roth': 26510,\n '향을': 26841,\n '##남을': 5660,\n '##발생하게': 27543,\n '발생하여': 5851,\n '##모터를': 12617,\n '토글': 6480,\n '##ethod': 2379,\n '않았다': 4537,\n '##께': 1462,\n 'elasticity': 19442,\n '안동규': 20473,\n '기계공학': 11971,\n '고품질의': 9592,\n '발생될': 14005,\n '실링엣': 23385,\n '##ij': 7704,\n 'Loc': 9835,\n '##75': 5226,\n '22일': 25665,\n '접철': 28651,\n 'enc': 25846,\n '그릴': 6400,\n '##pon': 3296,\n '##tan': 25194,\n '##라면': 18961,\n '조건의': 5510,\n '##산화': 6369,\n '커버의': 10387,\n '그래프를': 11822,\n '##arrow': 20686,\n '##에이터': 7446,\n '1930': 21716,\n '모두에서': 20873,\n '이송': 5348,\n '##sec로': 21986,\n '차단': 10283,\n '##ee': 6332,\n '덴': 341,\n '고정한': 16155,\n '##그네': 3988,\n '##Techn': 12601,\n '폴리카프로락톤': 21271,\n '설립하였고': 24481,\n '##수지의': 4140,\n '##eometr': 4396,\n 'swell을': 15770,\n 'Digital': 20339,\n '##ook': 4729,\n '섬유함유율의': 21062,\n 'Styrene': 22952,\n '##다른': 8159,\n 'Bas': 11861,\n '성형해석에': 14903,\n '입력변수': 14441,\n '##흡수': 15157,\n '수지온도와': 14846,\n '##우수': 17667,\n '복굴절을': 8192,\n '##보정': 13950,\n '##종': 1431,\n 'Chiu': 29944,\n '개발이': 6966,\n '코팅이': 27645,\n '로부터': 9858,\n '§': 100,\n '도료': 25037,\n '치수안정성이': 28059,\n '##ann': 2687,\n '마찬가지로': 6782,\n '초래한다': 23362,\n '배향각': 24201,\n '다르지만': 24585,\n '##닛': 1770,\n '사이를': 19927,\n '기계학습': 25511,\n '##an': 1934,\n '외관품질을': 20202,\n '##ok': 5139,\n '가지': 2192,\n 'tool': 5823,\n '정확도가': 25705,\n '녀': 294,\n '##전원': 26983,\n '##Key': 3558,\n '##ove': 6076,\n '심사완료': 18749,\n '끝단부의': 26035,\n '495': 23203,\n '##물을': 4502,\n 'requir': 5594,\n '폴리아미드계': 24640,\n '검토하였다': 8544,\n '선형회귀': 27635,\n '불완전': 23721,\n '##ultaneous': 9460,\n '##충전': 2896,\n '클래': 9757,\n 'inserts': 24682,\n '단축하기': 29554,\n '##뮬': 1609,\n '##방법에': 9244,\n '끝나': 13296,\n '비례하는': 20107,\n '손실': 4594,\n 'nodal': 14597,\n '##게이트는': 17230,\n '##차적으로': 8086,\n '초박판': 15805,\n '투자': 8776,\n '168': 12665,\n '섬유배향상태가': 27467,\n '021': 28245,\n '##roduction': 5430,\n '1B': 28250,\n '##전자현미경': 15315,\n '겔': 203,\n '##았고': 15994,\n '##yan': 22717,\n '##공식을': 26995,\n '증가하지만': 19870,\n 'more': 5025,\n '패션': 28696,\n '절': 734,\n '할': 945,\n '##위': 1194,\n '충분': 3365,\n '마커': 15464,\n '목표로': 10079,\n '가정하면': 14857,\n '활용': 2499,\n '급속냉각': 17254,\n 'increased': 5889,\n 'Zh': 5786,\n '1초로': 24839,\n '##리게': 8152,\n '##TR': 26950,\n '⌀': 115,\n 'Modulus': 23295,\n '##i는': 16500,\n '핵': 951,\n '##껑': 1726,\n '카프로락': 22600,\n '##EX': 26913,\n 'Basic': 16880,\n '고화온도': 21875,\n '닫': 326,\n 'ofThermoplastic': 22789,\n '##Cooling': 18021,\n '유형': 12565,\n '프로그램에서': 18498,\n 'AN': 4448,\n '##inuity': 15352,\n '보압해석': 16609,\n 'SUS316L': 20258,\n '젝': 740,\n 'themold': 14771,\n '실온': 28589,\n '593': 29854,\n '##므': 1278,\n '##주기': 16000,\n '알루미늄': 3937,\n '##이론을': 27945,\n '설': 574,\n '강화재': 11422,\n '신경회로망에': 18004,\n '확보하였다': 24333,\n '엔지니어링': 4361,\n '밀착': 13584,\n '##mechanical': 28757,\n 'Parameter': 19156,\n '남게': 18862,\n '##엘': 1807,\n '설비의': 10861,\n 'ofs': 20682,\n '##벡터': 13125,\n '평평하게': 24738,\n 'Refr': 16246,\n '##퀼': 1677,\n '##mm로': 5105,\n 'membrane': 13847,\n '평면응력': 18576,\n '##였습니다': 26963,\n '활발하게': 9461,\n '분말에': 20783,\n '평가하기': 7598,\n 'variations': 24278,\n '김준': 28479,\n '##phen': 13641,\n '##단순': 28815,\n '축': 806,\n '3차원으로': 20017,\n '15mm': 22919,\n 'their': 5962,\n '##시스템으로': 20814,\n '##irefring': 5323,\n 'polypropylene': 5455,\n '모두를': 21909,\n '매립': 17513,\n '116': 9427,\n 'Se': 3105,\n '변수들이': 15688,\n '##싱크마크': 21683,\n '##actice': 15596,\n '##elligence': 12725,\n '균일한': 3865,\n '멜트': 15060,\n '383': 25924,\n '주지': 8151,\n 'indicated': 24488,\n 'respectively': 5146,\n '코팅': 3810,\n 'theta': 9896,\n '##ae': 3356,\n '직경이': 7355,\n '##모델과': 12890,\n '##용량을': 25312,\n 'Ceram': 8294,\n '##anti': 25395,\n '##uide': 4853,\n 'has': 2919,\n '한국하이티엔': 26403,\n '초간': 26810,\n 'Kg': 23580,\n 'moldedproducts': 27440,\n '느리게': 14617,\n '성형품과': 12663,\n '##값으로': 7449,\n '##quisition': 22329,\n '##러스터': 13631,\n 'pres': 10492,\n '##iton': 25412,\n '내구성을': 14152,\n 'industr': 5559,\n '완화': 8861,\n '온도까지': 24020,\n '##경우': 3445,\n '##Es': 28799,\n '단계를': 14427,\n 'overcome': 20102,\n 'ofM': 5383,\n 'function': 4041,\n '관심': 4994,\n 'P3': 11867,\n '공정시간을': 20713,\n '##따': 1279,\n '결정되면': 19153,\n 'Center': 7097,\n '##정유': 23829,\n '##력의': 5769,\n '기공은': 29659,\n 'Serv': 13515,\n '##많이': 10961,\n '##ynthesis': 12963,\n '계기': 28455,\n 'Friedl': 15832,\n '성형제품을': 25408,\n 'dispersion': 14178,\n '압착': 18215,\n 'MEMS': 15420,\n '휘도를': 28719,\n '재고': 13317,\n '구조적인': 15204,\n 'Efficient': 25895,\n '확산이': 20132,\n 'prior': 21779,\n '1989': 5089,\n '우수하다는': 29420,\n '56': 5073,\n '1999년': 9047,\n '##계': 1005,\n '글로': 25014,\n '##속도는': 7079,\n '식료공업': 21127,\n '##eet': 21563,\n 'rapidly': 10872,\n 'Determination': 12733,\n '명확하게': 22210,\n 'CRT': 22375,\n '##AD': 19743,\n 'meas': 3764,\n '배럴': 4743,\n 'LI': 9483,\n '##현': 1141,\n '##하기가': 12847,\n '우리': 4673,\n '##°': 1070,\n '##속성': 9524,\n 'Advani': 15316,\n '##생년월일': 18256,\n '##재질': 13106,\n '격자의': 26598,\n 'li': 16949,\n '구비': 23669,\n 'Br': 7034,\n 'His': 14205,\n 'Lee': 2370,\n 'frame': 18139,\n '##실험의': 27371,\n 'BOSCH': 22336,\n '##유동에': 13192,\n 'carnauba': 19365,\n '42': 4382,\n '발생은': 17120,\n '유동장의': 20722,\n '##시킨다': 4555,\n '##pos': 3551,\n 'buckling': 26532,\n '핵심': 4777,\n '해석시간': 29031,\n '천천히': 28234,\n '##ayat': 25568,\n '##겠다': 6895,\n 'causes': 21121,\n '항목을': 26179,\n 'PU': 18113,\n '제어의': 25593,\n '내부가': 29240,\n '##ather': 16836,\n '때문인': 19875,\n 'features': 5871,\n '사실은': 19369,\n '27일': 24247,\n '##가소성': 3217,\n '##겐': 1627,\n 'br': 6313,\n 'Guide': 7205,\n 'quality': 3472,\n '고정된': 12436,\n '분류된다': 20942,\n '가스사출에서는': 20187,\n '##시켰': 4045,\n 'Ak': 16910,\n '충전불균형': 5775,\n 'Pt': 28357,\n '직경': 3176,\n '이루어졌다': 11188,\n 'Bor': 28296,\n '마찰열': 21017,\n 'fronts': 24824,\n '5mm에서': 24398,\n '쇼': 593,\n 'researchers': 21057,\n '##밍': 1555,\n '부터': 5703,\n 'per': 2994,\n '신호로부터': 18570,\n '##상형': 21611,\n '7MPa': 26434,\n '유변': 4163,\n '##같은': 5085,\n 'exact': 27275,\n '##lip': 13086,\n 'attention': 19492,\n '##대학교': 2292,\n 'them': 5332,\n '사출온도와': 19308,\n '162': 21843,\n '##품은': 21585,\n 'cav': 24947,\n '나은': 19593,\n '형체력을': 7353,\n '제조한': 8898,\n '지연시간을': 26036,\n '##pylene': 3705,\n 'Crystall': 23186,\n '##il': 2003,\n 'cess': 23630,\n '모양과': 20064,\n '절감을': 10370,\n '끼치게': 26632,\n '##ous': 2530,\n 'frames': 28397,\n '##때문에': 5047,\n '##참고문헌': 11452,\n 'extruder': 19357,\n '배향은': 14425,\n '발표자': 10087,\n '##신호를': 12500,\n '1300': 15654,\n '분자량을': 25839,\n '속충수지외': 16862,\n 'automobile': 8833,\n 'Light': 6095,\n '확률신경망': 23426,\n '##V의': 26998,\n '방법에는': 24094,\n '##백': 1729,\n '전략기술인력양성사업': 18034,\n '##로겐': 21594,\n '881': 29874,\n '사출성형품의': 3309,\n 'Extrusion': 19467,\n '모조': 18192,\n '선택과': 19997,\n 'thermoforming': 22252,\n '026': 22012,\n '##용량': 9103,\n '學': 158,\n 'vent': 11476,\n '간에': 17490,\n '##명대학교': 13989,\n '##evens': 22890,\n '변수의': 8726,\n '플런저를': 20233,\n '시트를': 13480,\n '탐': 866,\n '갖고': 4848,\n '보였고': 17331,\n 'higher': 6815,\n '전에': 5350,\n '##텔': 1444,\n '진행한다': 19899,\n '미세사출성형': 7019,\n '결측': 21399,\n '수소': 3931,\n '이경': 22562,\n '##eep': 23845,\n '##니켈': 25309,\n '조작에': 27843,\n 'Prof': 14804,\n 'BEM': 15004,\n 'measurements': 26193,\n '##옥래': 25367,\n 'verify': 15339,\n '##ima': 19831,\n '##났': 1669,\n '사출공정의': 13222,\n '##값에': 8998,\n '엘라스토머': 10279,\n '스프링을': 26223,\n '플라스틱이나': 27197,\n '##스탯': 15139,\n '기술이다': 10490,\n '도광판': 3709,\n '##v': 1244,\n 'Tools': 22224,\n 'methods': 6269,\n '실시하는': 24357,\n '한국산업기술진흥원의': 18741,\n '베럴': 9860,\n '유공압': 15490,\n '##이산화탄소': 20557,\n '##지금까지': 26387,\n '예측되는': 29211,\n '끝부분': 15912,\n '최소인': 29246,\n 'Do': 5980,\n '##periment': 2512,\n '충전을': 7460,\n 'regard': 14139,\n '사이클을': 25855,\n '##Car': 26897,\n '##몸': 1869,\n 'm': 80,\n 'produced': 8190,\n '계산을': 10809,\n '##감에': 20624,\n 'Pty': 17451,\n '사출성형에서는': 9899,\n '##셔서': 21664,\n '바이': 15062,\n '할당': 20542,\n 'STANS': 22082,\n '갖추': 7151,\n 'PMMA': 4463,\n '연결되는': 18546,\n '##xx': 22715,\n '평판의': 12481,\n '##전형': 21637,\n '게이트의': 2813,\n 'Electron': 21071,\n '표면을': 6492,\n '##자재': 17680,\n '최적의': 2954,\n 'Matsu': 17963,\n '고온에서의': 25850,\n '일정량의': 19197,\n '서보모터의': 14511,\n '##플랫폼': 28934,\n '결합을': 13724,\n '사용했으며': 25429,\n '##Works': 10617,\n '##제를': 4442,\n 'Vol': 2114,\n '심층': 18900,\n '기계가공으로': 27906,\n '클립': 25157,\n '하측에': 28707,\n '모듈을': 9941,\n 'extrusion': 12490,\n '##강성': 10154,\n '끝단의': 18635,\n '세로': 7988,\n '이소시아네이트': 23502,\n '냉각온도에': 29079,\n '##저울': 26907,\n '##탐색': 12383,\n '6Al': 18782,\n 'ofPoly': 27074,\n 'RIM성': 24417,\n '밝니다': 22505,\n '화학': 3622,\n '수있을': 27586,\n '##점에서': 4281,\n '49±0': 27698,\n 'Mould': 7483,\n 'embedded': 26242,\n '쁜': 555,\n '##ditional': 12870,\n 'HOUSING': 19539,\n '제품에는': 22806,\n '##한국': 4549,\n '##성능이': 21045,\n 'CNFs': 22315,\n 'CAN': 24103,\n '연속적인': 11820,\n 'holes': 26545,\n '양상이': 28602,\n 'platform': 18704,\n '도달한': 22088,\n '##공간': 9408,\n '고밀도': 13565,\n '메카니즘': 8937,\n '##산업의': 10201,\n '춘계': 20524,\n '##전략': 26984,\n 'side': 7554,\n '유체역학': 20092,\n '없이도': 24415,\n 'Line에': 29893,\n '불균일한': 5745,\n '사출성형제품': 25414,\n '주관하는': 20270,\n '스타': 18897,\n '불균형에': 22064,\n '일정한': 3663,\n '체적수축률': 5019,\n '유동해석은': 19320,\n '##겹': 1693,\n '관한연구': 14832,\n '##시스템에서': 19186,\n 'Lho': 21319,\n '##투명': 12381,\n '논문에서는': 3368,\n '프라스틱': 17595,\n '295': 12451,\n '##우레탄': 6294,\n 'poor': 25926,\n '유량의': 17550,\n '고려하여': 3040,\n '##깨': 1826,\n 'scheme': 9162,\n '##기존의': 11638,\n '##없다': 28937,\n '설계변수는': 23204,\n '##하지만': 4053,\n '##부위는': 12932,\n '일반적': 16104,\n 'Ring': 16926,\n '대응하는': 16267,\n '그래프에서': 17931,\n '##즈': 1305,\n '금형Fig': 29006,\n '##SN': 5708,\n '사출성형해석을': 4792,\n '연구하였으며': 17113,\n '대책을': 21171,\n '##udi': 4548,\n 'KAIST': 13521,\n '결과물임을': 26355,\n 'Simp': 29987,\n '197': 2999,\n '294': 18517,\n '##팬': 1597,\n '금속분말': 4765,\n '##esium': 7454,\n '성형법은': 27088,\n '고안된': 24620,\n '##되도록': 5819,\n 'stress': 3013,\n '발생하기': 7286,\n '구체적인': 11247,\n '추정': 5281,\n '이점이': 16461,\n '원하는': 4415,\n '##치의': 8878,\n 'SUM': 26228,\n '의약품': 25111,\n '판단하여': 20833,\n '유발': 4912,\n 'Ep': 26465,\n '중앙위원회': 29473,\n '신뢰성을': 6734,\n '충전시간': 3649,\n '2022': 7790,\n '제작하게': 29123,\n '##ori': 8016,\n 'double': 11222,\n '복합재료는': 11594,\n 'Four': 16375,\n '##ATE': 17323,\n '된다고': 21786,\n '요구에': 9799,\n '##GP': 23908,\n '설치가': 16709,\n '가공하여': 9921,\n '있다면': 15595,\n '405': 24237,\n 'Tg': 10411,\n '보압절환': 8340,\n '젖': 738,\n '이호상': 17560,\n 'capt': 26041,\n '##춘계학술대회': 28962,\n '기관': 17496,\n '이는': 2365,\n '##가로': 17631,\n '용도가': 18224,\n 'typical': 27955,\n '##천': 1501,\n 'acceleration': 20300,\n 'Johnson': 29931,\n '##이형': 25198,\n '향상시켜': 17167,\n '종류의': 6387,\n '직교배열표를': 14519,\n '불량률을': 16138,\n 'MechanicalProperties': 23109,\n '계산하는': 14044,\n '1959': 23317,\n '##appropriate': 29935,\n '닿': 331,\n '##ath': 6584,\n '160': 6911,\n 'Approximation': 26236,\n '##anyang': 14484,\n '270p': 19428,\n '효과와': 25634,\n '유동성과': 19100,\n '완성하였다': 23190,\n '조작을': 20155,\n '결과도': 22824,\n 'phot': 10074,\n '부피': 4153,\n '##oxy': 7744,\n '삭': 557,\n '블로잉': 13910,\n '##램': 1460,\n 'improve': 7262,\n 'dental': 14226,\n '기계공학부대학원': 27789,\n '상수들을': 29832,\n 'l': 79,\n '##mo': 25215,\n '##arge': 4898,\n '##관': 1349,\n 'larg': 21370,\n '휘발': 23818,\n '사출방향과': 23984,\n '##Condition': 27816,\n '지점의': 20138,\n 'rheological': 11838,\n '분석하였으며': 16106,\n 'Slide': 24606,\n '향상시키며': 29258,\n '일으켜': 16815,\n 'Sm': 6876,\n '##eon': 5309,\n '##성형이': 11957,\n '사출성형공정에서의': 27412,\n '파일을': 29977,\n '숙': 596,\n '금형이다': 22799,\n '입출': 26774,\n '사이드': 6427,\n '지배방정식': 5863,\n 'research': 5387,\n '##비용': 5084,\n '##따른': 8513,\n '리': 448,\n '압력': 2016,\n '##def': 14316,\n '##재활용': 21632,\n '골': 217,\n '취출한': 24330,\n '##중요하다': 22216,\n '550': 20326,\n 'DE': 15006,\n '##것': 1383,\n '설계자의': 10624,\n '수용성': 24720,\n 'barrel': 11606,\n '안전성을': 13488,\n '점도에': 13746,\n '##공장에서는': 29334,\n '절감하고': 27600,\n '인자임을': 22971,\n '람': 390,\n '선형적으로': 12258,\n 'DMT': 22380,\n '내마모': 8228,\n '복원': 6564,\n 'images': 18591,\n '70': 3091,\n 'TECH': 16936,\n 'agreement': 12272,\n '생성': 2660,\n 'TGA': 10413,\n '요즘': 26761,\n '##차이에': 15756,\n '접수일': 24544,\n '낸': 278,\n '##ulty': 15600,\n '내장부품': 22139,\n '특정한': 12940,\n '용읍': 26764,\n 'demand': 11840,\n 'microstructure': 13188,\n 'connector': 10393,\n '##트롤': 5535,\n '##전히': 12168,\n '용량을': 13916,\n '사출성형품을': 14491,\n 'Zr': 26527,\n '직전': 14287,\n '##서히': 8319,\n '모재의': 10218,\n 'blend': 6188,\n '형성을': 12575,\n '≤': 114,\n '사출성형된': 7399,\n '신뢰도': 17895,\n 'Tensile': 8475,\n '##WGV': 21649,\n '원이다': 21487,\n 'optim': 2931,\n '##중소': 28912,\n '##anel': 7511,\n '##ousing': 7348,\n '2m': 16903,\n '들면': 11893,\n '내경': 7558,\n '##원과': 17670,\n 'BP': 21295,\n 'Joo': 16917,\n '있기때문에': 25927,\n '사출압축성형을': 27786,\n '040': 28246,\n '방안으로서': 27766,\n 'under': 4160,\n '가스에': 25529,\n 'Equ': 7683,\n '##타텍': 20635,\n '##ern': 6723,\n '##Mo': 25336,\n '크랭크': 28687,\n '표면에': 3572,\n 'Plan': 18595,\n '##Wang': 27004,\n '력포': 28512,\n '##technique': 24731,\n '자성': 3751,\n '##fiber': 13644,\n '##촬': 1723,\n 'Unified': 16699,\n 'NE': 22396,\n '변위는': 27757,\n '푼': 929,\n 'thepro': 9779,\n '집': 771,\n '##ubes': 14049,\n 'interm': 27655,\n 'AbstractThe': 14483,\n 'assembled': 23496,\n 'MAC': 26011,\n 'microscopy': 19462,\n '달라': 4668,\n '굴절율': 10382,\n 'thicknesses': 24380,\n '확보가': 17237,\n '248': 27415,\n '##작업': 4543,\n '##지면': 5957,\n 'introduced': 11645,\n '##itect': 14777,\n '##adke': 29205,\n '펜더의': 14541,\n '불량원인': 19188,\n '##bel': 22648,\n '적합하도록': 20929,\n '##ero': 6896,\n '##경우의': 27572,\n '획득하였다': 14143,\n '##encl': 21928,\n '##함수': 3611,\n '##ully': 12630,\n '연구되었음': 29029,\n '주응력': 10921,\n '##ask': 12625,\n '작은': 3187,\n '##melt': 25217,\n '연락주': 28049,\n '##J': 1543,\n '매출액은': 21157,\n ...}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1715153546667
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import BertWordPieceTokenizer\n",
        "\n",
        "# 어휘 사전 파일 확인\n",
        "vocab_path = \"/home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/trained_tokenizer/CUSTOM_Tokenizer/vocab.txt\"\n",
        "tokenizer = BertWordPieceTokenizer(vocab_path)\n",
        "print(tokenizer)\n",
        "\n",
        "# text = \"공정변수를 진공성형의 사출성형해석에 요인배치로써 공급부에 중요한 역할을 제공한다.\"\n",
        "# # 텍스트 토큰화\n",
        "# tokenized_output = tokenizer.encode(text, add_special_tokens=True)\n",
        "# print(\"Token IDs:\", tokenized_output.ids)\n",
        "# print(\"Tokens:\", tokenized_output.tokens)\n",
        "# # 토큰 ID 배열을 사용하여 전체 텍스트 디코딩\n",
        "# decoded_text = tokenizer.decode(tokenized_output.ids)\n",
        "# print(\"Decoded Text:\", decoded_text)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Tokenizer(vocabulary_size=30000, model=BertWordPiece, unk_token=[UNK], sep_token=[SEP], cls_token=[CLS], pad_token=[PAD], mask_token=[MASK], clean_text=True, handle_chinese_chars=True, strip_accents=None, lowercase=True, wordpieces_prefix=##)\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1715154416519
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "llm-rag-embeddings",
      "language": "python",
      "display_name": "genai"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.19",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "llm-rag-embeddings"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}