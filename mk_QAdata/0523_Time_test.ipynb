{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import uuid\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# from llama_index.core import SimpleDirectoryReader\n",
        "# from llama_index.core.schema import MetadataMode\n",
        "# from llama_index.core.schema import Document\n",
        "# from llama_index.core.node_parser import (\n",
        "#     SentenceSplitter,\n",
        "#     SemanticSplitterNodeParser,\n",
        "# )\n",
        "# from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "\n",
        "#from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "# import sys\n",
        "# sys.path.append(\"//anaconda/envs/azureml_py38/bin/python\")\n",
        "# from tqdm import tqdm\n",
        "\n",
        "# openai key\n",
        "# import os\n",
        "# from openai import OpenAI\n",
        "# OPENAI_API_KEY = \"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\"\n",
        "# os.environ[\"OPENAI_API_KEY\"]=OPENAI_API_KEY\n",
        "# client = OpenAI()\n",
        "\n",
        "\n",
        "import openai\n",
        "from openai import AzureOpenAI\n",
        "from langchain_openai import AzureChatOpenAI\n",
        "#from langchain_openai import AzureChatOpenAI\n",
        "\n",
        "# azure openai key\n",
        "\n",
        "\"\"\"\n",
        "os.environ[\"OPENAI_API_VERSION\"] = \"2024-02-15-preview\"\n",
        "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://ix-genai.openai.azure.com/\"\n",
        "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"24dfb271047e40c4aa018120db0671b0\"\n",
        "\"\"\"\n",
        "os.environ[\"OPENAI_API_VERSION\"] = \"2023-05-15\"\n",
        "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://ix-genai.openai.azure.com/\"\n",
        "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"24dfb271047e40c4aa018120db0671b0\"\n",
        "\n",
        "\n",
        "#client=AzureOpenAI(\n",
        "#  deployment_name=\"gpt-4o\",\n",
        "#)\n",
        "client = AzureOpenAI(\n",
        "  api_key = os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
        "  api_version = os.getenv(\"OPENAI_API_VERSION\"),\n",
        "  azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")  # Your Azure OpenAI resource's endpoint value.\n",
        ")\n",
        "\n",
        "llm = AzureChatOpenAI(\n",
        "    deployment_name=\"genai-4o\",\n",
        ")\n",
        "\n",
        "print(llm.invoke(\"질문 올려주세용\"))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "content='물론이죠! 어떤 질문이든 물어보세요. 도와드릴 수 있는 한 최대한 도움을 드리겠습니다. :)' response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 14, 'total_tokens': 44}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_5f4bad809a', 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {}} id='run-e13bcaee-bbf6-461d-be31-82b7e786b6e6-0'\n"
        }
      ],
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1717977433276
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_openai import AzureOpenAIEmbeddings\n",
        "\n",
        "embeddings=AzureOpenAIEmbeddings(deployment=\"text-embedding-3-large\",\n",
        "                            model=\"text-embedding-3-large\",\n",
        "                            azure_endpoint=\"https://ix-genai.openai.azure.com/\",\n",
        "                            openai_api_type=\"azure\",\n",
        "                            chunk_size=100)"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717977433582
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import re \n",
        "\n",
        "def load_text_data(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        content = file.readlines()\n",
        "    return content\n",
        "\n",
        "\n",
        "def save_to_json(new_data, file_path):\n",
        "    # 파일 존재\n",
        "    if os.path.exists(file_path):\n",
        "        # 업데이트\n",
        "        with open(file_path,\"r\",encoding='utf-8') as f:\n",
        "            data=json.load(f)\n",
        "            data.update(new_data)\n",
        "    \n",
        "    # 데이터 저장 \n",
        "        with open(file_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(data, f, indent=4, ensure_ascii=False)\n",
        "    else:\n",
        "        with open(file_path,'w',encoding='utf-8') as f:\n",
        "            json.dump(new_data,f,indent=4,ensure_ascii=False)\n",
        "    \n",
        "\n",
        "        \n",
        "def split_trainval(text_data):\n",
        "    \n",
        "    # Calculate the index for a 70-30 split\n",
        "    split_index = int(len(text_data) * 0.7)  # 70% for training\n",
        "    train_corpus = text_data[:split_index]\n",
        "    val_corpus = text_data[split_index:]\n",
        "    return train_corpus, val_corpus\n",
        "\n",
        "\n",
        "def save_corpus(corpus, dir_path):\n",
        "    \n",
        "    os.makedirs(dir_path, exist_ok=True)  # Ensure the directory exists\n",
        "    corpus_path = os.path.join(dir_path, 'corpus.txt')\n",
        "    with open(corpus_path, 'w', encoding='utf-8') as file:\n",
        "        for line in corpus:\n",
        "            file.write(str(line) + '\\n')  # Write line as a string, ensuring conversion\n",
        "\n",
        "# 줄바꿈을 제거하는 함수\n",
        "def remove_newlines(text):\n",
        "    # 정규식 패턴을 사용하여 줄바꿈 문자(\\n)를 공백으로 대체\n",
        "    return re.sub(r'\\n', ' ', text)\n",
        "\n",
        "\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain.schema import Document\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "# from langchain.text_splitter import MetadataMode\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "\n",
        "\n",
        "def load_corpus(file_list, verbose=False):\n",
        "    print(\"load_corpus 시작\")\n",
        "    documents = []\n",
        "    # 각 파일 경로에 대한 내용을 읽어 Document 객체를 생성\n",
        "    for file_path in file_list:\n",
        "        if os.path.exists(file_path):\n",
        "            with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                content = file.read()\n",
        "                #documents.append(Document(text=content))\n",
        "                documents.append(Document(page_content=content))\n",
        "                \n",
        "        else:\n",
        "            if verbose:\n",
        "                print(f\"File not found: {file_path}\")\n",
        "    \n",
        "    # parse nodes\n",
        "    # parser = SentenceSplitter()\n",
        "    # nodes = parser.get_nodes_from_documents(documents) # 파일에서 직접 읽은 Document 리스트를 파서에 전달\n",
        "    \n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "    print(\"text_splitter 통과\")\n",
        "    nodes = text_splitter.split_documents(documents)  # 파일에서 직접 읽은 Document 리스트를 파서에 전달\n",
        "    print(\"node 할당\")\n",
        "    chunker = SemanticChunker(embeddings, buffer_size=1, breakpoint_threshold_type=\"interquartile\")\n",
        "    print(\"chunker 통과\")\n",
        "    #nodes = chunker.get_nodes_from_documents(nodes) # 문장 단위로 청킹한 뒤에 semantic chunking\n",
        "    nodes = chunker.split_documents(nodes)  # 문장 단위로 청킹한 뒤에 semantic chunking\n",
        "    print(\"split_document\")\n",
        "    if verbose:\n",
        "        print(f'Parsed {len(nodes)} nodes')\n",
        "\n",
        "    #corpus = {node.node_id: node.get_content(metadata_mode=MetadataMode.NONE) for node in nodes}\n",
        "    #corpus = {node.node_id: node.get_content(metadata_mode='none') for node in nodes}\n",
        "    #i=str(0)\n",
        "\n",
        "    #corpus={}\n",
        "    print(\"corpus 시작\")\n",
        "    corpus = {str(i): node.page_content for i, node in enumerate(nodes)}\n",
        "    #for i,node in enumerate(nodes):\n",
        "        #i=int(i)+1+int(node_ids)\n",
        "        #i=str(i)\n",
        "        #print(i)\n",
        "        #corpus[i]=node.page_content\n",
        "    return corpus"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717977483424
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import uuid\n",
        "\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "\n",
        "#from llama_index.core.llms import OpenAI\n",
        "# from llama_index.core.schema import MetadataMode\n",
        "\n",
        "\n",
        "def generate_three_question_answer_queries(text_data, node_ids,num_questions_per_chunk=3,num_answers_per_chunk=3, prompt_template=None, verbose=False):\n",
        "    \"\"\"\n",
        "    Automatically generate hypothetical questions that could be answered with\n",
        "    doc in the corpus.\n",
        "    \"\"\"\n",
        "    #text_list = list(text_data.values()) if isinstance(text_data, dict) else text_data\n",
        "\n",
        "    if prompt_template is None:\n",
        "        prompt_template = \"Given the context: {context_str}, generate question: {num_questions_per_chunk} and answer: {num_answers_per_chunk} long and detailed answers in Korean. Please generate three question & answer pairs in Korean.\" \n",
        "    \n",
        "    queries = {}\n",
        "    relevant_docs = {}\n",
        "    answers={}\n",
        "\n",
        "    node_id=str(0)\n",
        "    for node_id, text in tqdm(text_data.items()): #코퍼스의 노드 아이디와 텍스트 내용 \n",
        "        node_id=int(node_id)+1+int(node_ids)\n",
        "        node_id=str(node_id)\n",
        "        query=prompt_template.format(context_str=text,num_questions_per_chunk=num_questions_per_chunk,num_answers_per_chunk=num_answers_per_chunk)\n",
        "\n",
        "        # response = client.completions.create(\n",
        "        #     model= \"gpt-4-turbo\",\n",
        "        #     prompt=query,\n",
        "        #     max_tokens=100,\n",
        "        #     temperature=0.7\n",
        "        # )\n",
        "        # result = response.choices[0].text.strip().split(\"\\n\")\n",
        "\n",
        "        # API 호출\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\", #IX-GPT4\",  # Azure에서 사용 가능한 모델 명시\n",
        "            messages=[\n",
        "                    {\"role\":\"user\",\"content\":query + \"\"\" Please Generate long and more detailed answers. 1. Q1, Q2, Q3가 최대한 겹치지 않게 생성해라.\n",
        "                2. 줄바꿈은 무조건 한 번씩만 해라. 출력은 다음 양식을 따른다:\n",
        "                \n",
        "                Q1: {{첫 번째 생성된 질문}}\n",
        "                A1: {{첫 번째 질문에 대한 답변}}\n",
        "                Q2: {{두 번째 생성된 질문}}\n",
        "                A2: {{두 번째 질문에 대한 답변}}\n",
        "                Q3: {{세 번째 생성된 질문}}\n",
        "                A3: {{세 번째 질문에 대한 답변}} \"\"\"}\n",
        "                ],\n",
        "                max_tokens=1000,\n",
        "                temperature=0\n",
        "        )\n",
        "        #response=client.chat.completions.create(\n",
        "        #    model=\"gpt-4o\",\n",
        "        #    messages=[\n",
        "        #        {\"role\":\"user\",\"content\":query}\n",
        "        #    ],\n",
        "        #    max_tokens=4000,\n",
        "        #    temperature=0\n",
        "        #)\n",
        "        #response=client.invoke(query)\n",
        "\n",
        "\n",
        "        \n",
        "        \n",
        "        result = response.choices[0].message.content\n",
        "        #q_as = [re.sub(r\"^\\d+\\.\\s*\", \"\", question).strip() for question in result if question.strip()]\n",
        "        \n",
        "        q_as = re.split(r'\\n(?=\\d+\\.\\s)', result.strip())\n",
        "        q_as = [qa.strip() for qa in q_as if qa.strip()]\n",
        "        \n",
        "        for q_a in q_as:\n",
        "            #if total_generated>=100:\n",
        "            #    break\n",
        "\n",
        "            q_a_id = str(uuid.uuid4())\n",
        "            #전처리\n",
        "            q_a=re.sub(\"(Q1:\\s|Q2:\\s|Q3:|A1:\\s|A2:\\s|A3:)\", \"\", q_a)\n",
        "            q_a = re.sub('\\n+', '\\n', q_a).split('\\n')\n",
        "            #print(\"q_a: \", q_a)\n",
        "            \n",
        "            if len(q_a)==6:\n",
        "                q1,a1,q2,a2,q3,a3=q_a\n",
        "                queries[q_a_id] = [q1,q2,q3]\n",
        "                answers[q_a_id]=[a1,a2,a3]\n",
        "            elif len(q_a)==4:\n",
        "                q1,a1,q2,a2=q_a\n",
        "                queries[q_a_id] = [q1,q2]\n",
        "                answers[q_a_id]=[a1,a2]\n",
        "            elif len(q_a)==2:\n",
        "                q1,a1=q_a\n",
        "                queries[q_a_id] = [q1]\n",
        "                answers[q_a_id]=[a1]\n",
        "            relevant_docs[q_a_id] = [node_id]\n",
        "            #print(node_id)\n",
        "            #total_generated+=1\n",
        "            #print(\"queries: \",queries)\n",
        "            #print(\"relevant_docs:\",relevant_docs)\n",
        "            #print(\"answers:\",answers)\n",
        "\n",
        "                    \n",
        "\n",
        "        \n",
        "    return queries, relevant_docs, answers,node_id\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717977490895
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import uuid\n",
        "\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import datetime\n",
        "import time\n",
        "\n",
        "\n",
        "# 10분단위로 azureapi 사용 멈추게 만들\n",
        "#from llama_index.core.llms import OpenAI\n",
        "# from llama_index.core.schema import MetadataMode\n",
        "\n",
        "\n",
        "def generate_three_question_answer_queries(text_data,node_ids, num_questions_per_chunk=3,num_answers_per_chunk=3, prompt_template=None, verbose=False):\n",
        "    \"\"\"\n",
        "    Automatically generate hypothetical questions that could be answered with\n",
        "    doc in the corpus.\n",
        "    \"\"\"\n",
        "    #text_list = list(text_data.values()) if isinstance(text_data, dict) else text_data\n",
        "    \n",
        "    if prompt_template is None:\n",
        "        prompt_template = \"Given the context: {context_str}, generate question: {num_questions_per_chunk} and answer: {num_answers_per_chunk} long and detailed answers in Korean. Please generate three question & answer pairs in Korean.\" \n",
        "    \n",
        "    queries = {}\n",
        "    relevant_docs = {}\n",
        "    answers={}\n",
        "\n",
        "    start_time=time.time()\n",
        "    pause_interval=1*60\n",
        "    pause_duration=1*60\n",
        "    #node_id=str(0)\n",
        "    for node_id, text in tqdm(text_data.items()): #코퍼스의 노드 아이디와 텍스트 내용 \n",
        "        #node_id=int(node_id)+1+int(node_ids)\n",
        "        #node_id=str(node_id)\n",
        "        query=prompt_template.format(context_str=text,num_questions_per_chunk=num_questions_per_chunk,num_answers_per_chunk=num_answers_per_chunk)\n",
        "\n",
        "        # response = client.completions.create(\n",
        "        #     model= \"gpt-4-turbo\",\n",
        "        #     prompt=query,\n",
        "        #     max_tokens=100,\n",
        "        #     temperature=0.7\n",
        "        # )\n",
        "        # result = response.choices[0].text.strip().split(\"\\n\")\n",
        "\n",
        "        # API 호출\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\", #IX-GPT4\",  # Azure에서 사용 가능한 모델 명시\n",
        "            messages=[\n",
        "                    {\"role\":\"user\",\"content\":query + \"\"\" Please Generate long and more detailed answers. 1. Q1, Q2, Q3가 최대한 겹치지 않게 생성해라.\n",
        "                2. 줄바꿈은 무조건 한 번씩만 해라. 출력은 다음 양식을 따른다:\n",
        "                \n",
        "                Q1: {{첫 번째 생성된 질문}}\n",
        "                A1: {{첫 번째 질문에 대한 답변}}\n",
        "                Q2: {{두 번째 생성된 질문}}\n",
        "                A2: {{두 번째 질문에 대한 답변}}\n",
        "                Q3: {{세 번째 생성된 질문}}\n",
        "                A3: {{세 번째 질문에 대한 답변}} \"\"\"}\n",
        "                ],\n",
        "                max_tokens=1000,\n",
        "                temperature=0\n",
        "        )\n",
        "        #response=client.chat.completions.create(\n",
        "        #    model=\"gpt-4o\",\n",
        "        #    messages=[\n",
        "        #        {\"role\":\"user\",\"content\":query}\n",
        "        #    ],\n",
        "        #    max_tokens=4000,\n",
        "        #    temperature=0\n",
        "        #)\n",
        "        #response=client.invoke(query)\n",
        "\n",
        "\n",
        "        \n",
        "        \n",
        "        result = response.choices[0].message.content\n",
        "        #q_as = [re.sub(r\"^\\d+\\.\\s*\", \"\", question).strip() for question in result if question.strip()]\n",
        "        \n",
        "        q_as = re.split(r'\\n(?=\\d+\\.\\s)', result.strip())\n",
        "        q_as = [qa.strip() for qa in q_as if qa.strip()]\n",
        "        \n",
        "        for q_a in q_as:\n",
        "            #if total_generated>=100:\n",
        "            #    break\n",
        "\n",
        "            q_a_id = str(uuid.uuid4())\n",
        "            #전처리\n",
        "            q_a=re.sub(\"(Q1:\\s|Q2:\\s|Q3:|A1:\\s|A2:\\s|A3:)\", \"\", q_a)\n",
        "            q_a = re.sub('\\n+', '\\n', q_a).split('\\n')\n",
        "            #print(\"q_a: \", q_a)\n",
        "            \n",
        "            if len(q_a)==6:\n",
        "                q1,a1,q2,a2,q3,a3=q_a\n",
        "                queries[q_a_id] = [q1,q2,q3]\n",
        "                answers[q_a_id]=[a1,a2,a3]\n",
        "            elif len(q_a)==4:\n",
        "                q1,a1,q2,a2=q_a\n",
        "                queries[q_a_id] = [q1,q2]\n",
        "                answers[q_a_id]=[a1,a2]\n",
        "            elif len(q_a)==2:\n",
        "                q1,a1=q_a\n",
        "                queries[q_a_id] = [q1]\n",
        "                answers[q_a_id]=[a1]\n",
        "            relevant_docs[q_a_id] = [node_id]\n",
        "        elapsed_time=time.time()-start_time\n",
        "        if elapsed_time > pause_interval:\n",
        "            \n",
        "            print(\"Pause for 1 minutes...\")\n",
        "            time.sleep(pause_duration)\n",
        "            start_time=time.time()\n",
        "            #print(node_id)\n",
        "            #total_generated+=1\n",
        "            #print(\"queries: \",queries)\n",
        "            #print(\"relevant_docs:\",relevant_docs)\n",
        "            #print(\"answers:\",answers)\n",
        "\n",
        "                    \n",
        "\n",
        "        \n",
        "    return queries, relevant_docs, answers\n"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717479974741
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ ==\"__main__\":\n",
        "    # 파일 경로 설정\n",
        "    DATA_FILE_PATH='/home/azureuser/cloudfiles/code/Users/hb.suh/(0528part)_OnlyTranslatedData'\n",
        "    OUTPUT_DIR=\"/home/azureuser/cloudfiles/code/Users/hm.lim/Gen/generated_QAdata/SemanticChunking_time_test/0603\" # relevant dataset 저장 경로\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "    all_texts=[]\n",
        "\n",
        "    # 폴더 내의 모든 txt 파일의 데이터를 모음\n",
        "    for file_name in os.listdir(DATA_FILE_PATH):\n",
        "        file_path = os.path.join(DATA_FILE_PATH, file_name)\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "            \n",
        "            # \"author:\", \"text:\", \"reference:\" 부분을 분리\n",
        "            #sections = {\"author\": \"\", \"text\": \"\", \"reference\": \"\"}\n",
        "            sections={\"text\": \"\",\"reference\": \"\"}\n",
        "            current_section = None\n",
        "            \n",
        "            for line in text.splitlines():\n",
        "                \n",
        "                #if line.lower().startswith(\"author:\"):\n",
        "                #    current_section = \"author\"\n",
        "                #elif line.lower().startswith(\"text:\"):\n",
        "                #    current_section = \"text\"\n",
        "                #elif line.lower().startswith(\"References:\"):\n",
        "                #    current_section = \"reference\"\n",
        "                if line.lower().startswith(\"main_content:\"):\n",
        "                    current_section=\"text\"\n",
        "                if line.startswith(\"Main Content:\"):\n",
        "                    current_section=\"text\"\n",
        "                elif line.lower().startswith(\"References:\"):\n",
        "                    current_section=\"reference\"\n",
        "                elif line.startswith(\"References:\"):\n",
        "                    current_section=\"reference\"\n",
        "                if current_section:\n",
        "                    sections[current_section] += line +\"\\n\"\n",
        "                \n",
        "            text_section = sections[\"text\"].replace(\"main_content:\", \"\").strip()\n",
        "            text_section=sections[\"text\"].replace(\"Main Content:\",\"\").strip()\n",
        "            \n",
        "            all_texts.append(text_section)\n",
        "\n",
        "        # 학습 , 검증으로 나누기\n",
        "    train, val = split_trainval(all_texts)\n",
        "    \n",
        "    # 각각의 디렉터리(train_corpus, val_corpus)에 corpus.txt라는 이름으로 저장\n",
        "    TRAIN_CORPUS_DIR = os.path.join(OUTPUT_DIR, 'train_corpus')\n",
        "    VAL_CORPUS_DIR = os.path.join(OUTPUT_DIR, 'val_corpus')\n",
        "    os.makedirs(TRAIN_CORPUS_DIR, exist_ok=True)\n",
        "    os.makedirs(VAL_CORPUS_DIR, exist_ok=True)\n",
        "\n",
        "    save_corpus(train, TRAIN_CORPUS_DIR)\n",
        "    save_corpus(val, VAL_CORPUS_DIR)  "
      ],
      "outputs": [],
      "execution_count": 41,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717139616372
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_FILE_PATH='/home/azureuser/cloudfiles/code/Users/hm.lim/Gen/generated_QAdata/SemanticChunking_time_test/0531'\n",
        "OUTPUT_DIR=\"/home/azureuser/cloudfiles/code/Users/hm.lim/Gen/generated_QAdata/SemanticChunking_time_test/0531\" # relevant dataset 저장 경로\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "all_texts=[]\n",
        "\n",
        "# 폴더 내의 모든 txt 파일의 데이터를 모음\n",
        "for file in os.listdir(DATA_FILE_PATH):\n",
        "    file_path=os.path.join(DATA_FILE_PATH,file)\n",
        "    with open(file_path,'r',encoding='utf-8') as file:\n",
        "        all_texts.extend(file.readlines())\n",
        "            \n",
        "train, val = split_trainval(all_texts)\n",
        "    \n",
        "# 각각의 디렉터리(train_corpus, val_corpus)에 corpus.txt라는 이름으로 저장\n",
        "TRAIN_CORPUS_DIR = os.path.join(OUTPUT_DIR, 'train_corpus')\n",
        "VAL_CORPUS_DIR = os.path.join(OUTPUT_DIR, 'val_corpus')\n",
        "os.makedirs(TRAIN_CORPUS_DIR, exist_ok=True)\n",
        "os.makedirs(VAL_CORPUS_DIR, exist_ok=True)\n",
        "\n",
        "save_corpus(train, TRAIN_CORPUS_DIR)\n",
        "save_corpus(val, VAL_CORPUS_DIR)  "
      ],
      "outputs": [],
      "execution_count": 44,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717140849415
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT_DIR=\"/home/azureuser/cloudfiles/code/Users/hm.lim/Gen/generated_QAdata/SemanticChunking_time_test/0605\""
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717977506317
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5번째 수행할 차례\n",
        "TRAIN_FILES=[f\"/home/azureuser/cloudfiles/code/Users/hm.lim/Gen/generated_QAdata/SemanticChunking_time_test/0605/train_corpus/corpus.txt\"]\n",
        "    \n",
        "train_corpus=load_corpus(TRAIN_FILES,verbose=True)\n",
        "\n",
        "\n",
        "train_queries,train_relevant_docs,train_answers=generate_three_question_answer_queries(train_corpus)\n",
        "\n",
        "save_to_json(train_queries,os.path.join(OUTPUT_DIR,'train_queries.json'))\n",
        "save_to_json(train_relevant_docs, os.path.join(OUTPUT_DIR, 'train_relevant_docs.json'))\n",
        "save_to_json(train_answers, os.path.join(OUTPUT_DIR, 'train_answers.json'))\n",
        "\"\"\"\n",
        "node_ids=-1\n",
        "for i in range(1,7):\n",
        "    print(f\"{i}번째 수행\")\n",
        "    TRAIN_FILES=[f\"/home/azureuser/cloudfiles/code/Users/hm.lim/Gen/generated_QAdata/SementicChunking_new/train_corpus_split/corpus{i}.txt\"]\n",
        "    \n",
        "    train_corpus=load_corpus(TRAIN_FILES,verbose=True)\n",
        "\n",
        "\n",
        "    train_queries,train_relevant_docs,train_answers,node_id=generate_three_question_answer_queries(train_corpus,node_ids)\n",
        "    \n",
        "    save_to_json(train_queries,os.path.join(OUTPUT_DIR,'train_queries.json'))\n",
        "    save_to_json(train_relevant_docs, os.path.join(OUTPUT_DIR, 'train_relevant_docs.json'))\n",
        "    save_to_json(train_answers, os.path.join(OUTPUT_DIR, 'train_answers.json'))\n",
        "    node_ids=node_id    \n",
        "\"\"\"\n",
        "\"\"\"\n",
        "    TRAIN_DATASET_FPATH = '/home/azureuser/cloudfiles/code/Users/hm.lim/Gen/generated_QAdata/Q_A/0528/train_dataset.json'\n",
        "\n",
        "    train_dataset = {\n",
        "        'queries': train_queries,\n",
        "        'corpus': train_corpus,\n",
        "        'relevant_docs': train_relevant_docs,\n",
        "        'answers':train_answers,\n",
        "    }\n",
        "\n",
        "\n",
        "    os.makedirs('/home/azureuser/cloudfiles/code/Users/hm.lim/Gen/generated_QAdata/Q_A/0528', exist_ok=True)\n",
        "    if os.path.exists(TRAIN_DATASET_FPATH):\n",
        "        with open(TRAIN_DATASET_FPATH,\"r\",encoding='utf-8') as f:\n",
        "            data=json.load(f)\n",
        "            data.update(train_dataset)\n",
        "        with open(TRAIN_DATASET_FPATH, 'w+') as f:\n",
        "            json.dump(data, f, ensure_ascii=False)\n",
        "    else:\n",
        "        with open(TRAIN_DATASET_FPATH,'w+') as f:\n",
        "            json.dump(train_dataset,f,ensure_ascii=False)\n",
        "    \"\"\"\n",
        "    #if os.path.exists(VAL_DATASET_FPATH):\n",
        "    #    with open(VAL_DATASET_FPATH,\"r\",encoding='utf-8') as f:\n",
        "    #        val_dataset=json.load(f)\n",
        "    #    val_dataset.update(val_dataset)\n",
        "    #with open(VAL_DATASET_FPATH, 'w+') as f:\n",
        "    #    json.dump(val_dataset, f, ensure_ascii=False)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717480256765
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VAL_FILES=[f\"/home/azureuser/cloudfiles/code/Users/hm.lim/Gen/generated_QAdata/SemanticChunking_time_test/0603/val_corpus/corpus1.txt\"]\n",
        "    \n",
        "val_corpus=load_corpus(VAL_FILES,verbose=True)\n",
        "\n",
        "\n",
        "val_queries,val_relevant_docs,val_answers=generate_three_question_answer_queries(val_corpus)\n",
        "\n",
        "save_to_json(val_queries,os.path.join(OUTPUT_DIR,'val_queries.json'))\n",
        "save_to_json(val_relevant_docs, os.path.join(OUTPUT_DIR, 'val_relevant_docs.json'))\n",
        "save_to_json(val_answers, os.path.join(OUTPUT_DIR, 'val_answers.json'))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717475728068
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "node_ids=-1\n",
        "for i in range(1,4):\n",
        "    print(f\"{i}번째 수행\")\n",
        "    TRAIN_FILES=[f\"/home/azureuser/cloudfiles/code/Users/hm.lim/Gen/generated_QAdata/SemanticChunking_time_test/0605/train_corpus/corpus{i}.txt\"]\n",
        "    train_corpus,node_id=load_corpus(TRAIN_FILES,node_ids,verbose=True)\n",
        "\n",
        "    save_to_json(train_corpus, os.path.join(OUTPUT_DIR, 'train_corpus.json'))\n",
        "    node_ids=node_id "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "1번째 수행\nload_corpus 시작\ntext_splitter 통과\nnode 할당\nchunker 통과\nsplit_document\nParsed 188 nodes\ncorpus 시작\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n2번째 수행\nload_corpus 시작\ntext_splitter 통과\nnode 할당\nchunker 통과\nsplit_document\nParsed 183 nodes\ncorpus 시작\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n3번째 수행\nload_corpus 시작\ntext_splitter 통과\nnode 할당\nchunker 통과\nsplit_document\nParsed 172 nodes\ncorpus 시작\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n500\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n4번째 수행\nload_corpus 시작\ntext_splitter 통과\nnode 할당\nchunker 통과\nsplit_document\nParsed 255 nodes\ncorpus 시작\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\n600\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\n700\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n5번째 수행\nload_corpus 시작\ntext_splitter 통과\nnode 할당\nchunker 통과\nsplit_document\nParsed 356 nodes\ncorpus 시작\n798\n799\n800\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899\n900\n901\n902\n903\n904\n905\n906\n907\n908\n909\n910\n911\n912\n913\n914\n915\n916\n917\n918\n919\n920\n921\n922\n923\n924\n925\n926\n927\n928\n929\n930\n931\n932\n933\n934\n935\n936\n937\n938\n939\n940\n941\n942\n943\n944\n945\n946\n947\n948\n949\n950\n951\n952\n953\n954\n955\n956\n957\n958\n959\n960\n961\n962\n963\n964\n965\n966\n967\n968\n969\n970\n971\n972\n973\n974\n975\n976\n977\n978\n979\n980\n981\n982\n983\n984\n985\n986\n987\n988\n989\n990\n991\n992\n993\n994\n995\n996\n997\n998\n999\n1000\n1001\n1002\n1003\n1004\n1005\n1006\n1007\n1008\n1009\n1010\n1011\n1012\n1013\n1014\n1015\n1016\n1017\n1018\n1019\n1020\n1021\n1022\n1023\n1024\n1025\n1026\n1027\n1028\n1029\n1030\n1031\n1032\n1033\n1034\n1035\n1036\n1037\n1038\n1039\n1040\n1041\n1042\n1043\n1044\n1045\n1046\n1047\n1048\n1049\n1050\n1051\n1052\n1053\n1054\n1055\n1056\n1057\n1058\n1059\n1060\n1061\n1062\n1063\n1064\n1065\n1066\n1067\n1068\n1069\n1070\n1071\n1072\n1073\n1074\n1075\n1076\n1077\n1078\n1079\n1080\n1081\n1082\n1083\n1084\n1085\n1086\n1087\n1088\n1089\n1090\n1091\n1092\n1093\n1094\n1095\n1096\n1097\n1098\n1099\n1100\n1101\n1102\n1103\n1104\n1105\n1106\n1107\n1108\n1109\n1110\n1111\n1112\n1113\n1114\n1115\n1116\n1117\n1118\n1119\n1120\n1121\n1122\n1123\n1124\n1125\n1126\n1127\n1128\n1129\n1130\n1131\n1132\n1133\n1134\n1135\n1136\n1137\n1138\n1139\n1140\n1141\n1142\n1143\n1144\n1145\n1146\n1147\n1148\n1149\n1150\n1151\n1152\n1153\n6번째 수행\nload_corpus 시작\ntext_splitter 통과\nnode 할당\nchunker 통과\nsplit_document\nParsed 125 nodes\ncorpus 시작\n1154\n1155\n1156\n1157\n1158\n1159\n1160\n1161\n1162\n1163\n1164\n1165\n1166\n1167\n1168\n1169\n1170\n1171\n1172\n1173\n1174\n1175\n1176\n1177\n1178\n1179\n1180\n1181\n1182\n1183\n1184\n1185\n1186\n1187\n1188\n1189\n1190\n1191\n1192\n1193\n1194\n1195\n1196\n1197\n1198\n1199\n1200\n1201\n1202\n1203\n1204\n1205\n1206\n1207\n1208\n1209\n1210\n1211\n1212\n1213\n1214\n1215\n1216\n1217\n1218\n1219\n1220\n1221\n1222\n1223\n1224\n1225\n1226\n1227\n1228\n1229\n1230\n1231\n1232\n1233\n1234\n1235\n1236\n1237\n1238\n1239\n1240\n1241\n1242\n1243\n1244\n1245\n1246\n1247\n1248\n1249\n1250\n1251\n1252\n1253\n1254\n1255\n1256\n1257\n1258\n1259\n1260\n1261\n1262\n1263\n1264\n1265\n1266\n1267\n1268\n1269\n1270\n1271\n1272\n1273\n1274\n1275\n1276\n1277\n1278\n7번째 수행\nload_corpus 시작\ntext_splitter 통과\nnode 할당\nchunker 통과\nsplit_document\nParsed 382 nodes\ncorpus 시작\n1279\n1280\n1281\n1282\n1283\n1284\n1285\n1286\n1287\n1288\n1289\n1290\n1291\n1292\n1293\n1294\n1295\n1296\n1297\n1298\n1299\n1300\n1301\n1302\n1303\n1304\n1305\n1306\n1307\n1308\n1309\n1310\n1311\n1312\n1313\n1314\n1315\n1316\n1317\n1318\n1319\n1320\n1321\n1322\n1323\n1324\n1325\n1326\n1327\n1328\n1329\n1330\n1331\n1332\n1333\n1334\n1335\n1336\n1337\n1338\n1339\n1340\n1341\n1342\n1343\n1344\n1345\n1346\n1347\n1348\n1349\n1350\n1351\n1352\n1353\n1354\n1355\n1356\n1357\n1358\n1359\n1360\n1361\n1362\n1363\n1364\n1365\n1366\n1367\n1368\n1369\n1370\n1371\n1372\n1373\n1374\n1375\n1376\n1377\n1378\n1379\n1380\n1381\n1382\n1383\n1384\n1385\n1386\n1387\n1388\n1389\n1390\n1391\n1392\n1393\n1394\n1395\n1396\n1397\n1398\n1399\n1400\n1401\n1402\n1403\n1404\n1405\n1406\n1407\n1408\n1409\n1410\n1411\n1412\n1413\n1414\n1415\n1416\n1417\n1418\n1419\n1420\n1421\n1422\n1423\n1424\n1425\n1426\n1427\n1428\n1429\n1430\n1431\n1432\n1433\n1434\n1435\n1436\n1437\n1438\n1439\n1440\n1441\n1442\n1443\n1444\n1445\n1446\n1447\n1448\n1449\n1450\n1451\n1452\n1453\n1454\n1455\n1456\n1457\n1458\n1459\n1460\n1461\n1462\n1463\n1464\n1465\n1466\n1467\n1468\n1469\n1470\n1471\n1472\n1473\n1474\n1475\n1476\n1477\n1478\n1479\n1480\n1481\n1482\n1483\n1484\n1485\n1486\n1487\n1488\n1489\n1490\n1491\n1492\n1493\n1494\n1495\n1496\n1497\n1498\n1499\n1500\n1501\n1502\n1503\n1504\n1505\n1506\n1507\n1508\n1509\n1510\n1511\n1512\n1513\n1514\n1515\n1516\n1517\n1518\n1519\n1520\n1521\n1522\n1523\n1524\n1525\n1526\n1527\n1528\n1529\n1530\n1531\n1532\n1533\n1534\n1535\n1536\n1537\n1538\n1539\n1540\n1541\n1542\n1543\n1544\n1545\n1546\n1547\n1548\n1549\n1550\n1551\n1552\n1553\n1554\n1555\n1556\n1557\n1558\n1559\n1560\n1561\n1562\n1563\n1564\n1565\n1566\n1567\n1568\n1569\n1570\n1571\n1572\n1573\n1574\n1575\n1576\n1577\n1578\n1579\n1580\n1581\n1582\n1583\n1584\n1585\n1586\n1587\n1588\n1589\n1590\n1591\n1592\n1593\n1594\n1595\n1596\n1597\n1598\n1599\n1600\n1601\n1602\n1603\n1604\n1605\n1606\n1607\n1608\n1609\n1610\n1611\n1612\n1613\n1614\n1615\n1616\n1617\n1618\n1619\n1620\n1621\n1622\n1623\n1624\n1625\n1626\n1627\n1628\n1629\n1630\n1631\n1632\n1633\n1634\n1635\n1636\n1637\n1638\n1639\n1640\n1641\n1642\n1643\n1644\n1645\n1646\n1647\n1648\n1649\n1650\n1651\n1652\n1653\n1654\n1655\n1656\n1657\n1658\n1659\n1660\n8번째 수행\nload_corpus 시작\ntext_splitter 통과\nnode 할당\nchunker 통과\nsplit_document\nParsed 399 nodes\ncorpus 시작\n1661\n1662\n1663\n1664\n1665\n1666\n1667\n1668\n1669\n1670\n1671\n1672\n1673\n1674\n1675\n1676\n1677\n1678\n1679\n1680\n1681\n1682\n1683\n1684\n1685\n1686\n1687\n1688\n1689\n1690\n1691\n1692\n1693\n1694\n1695\n1696\n1697\n1698\n1699\n1700\n1701\n1702\n1703\n1704\n1705\n1706\n1707\n1708\n1709\n1710\n1711\n1712\n1713\n1714\n1715\n1716\n1717\n1718\n1719\n1720\n1721\n1722\n1723\n1724\n1725\n1726\n1727\n1728\n1729\n1730\n1731\n1732\n1733\n1734\n1735\n1736\n1737\n1738\n1739\n1740\n1741\n1742\n1743\n1744\n1745\n1746\n1747\n1748\n1749\n1750\n1751\n1752\n1753\n1754\n1755\n1756\n1757\n1758\n1759\n1760\n1761\n1762\n1763\n1764\n1765\n1766\n1767\n1768\n1769\n1770\n1771\n1772\n1773\n1774\n1775\n1776\n1777\n1778\n1779\n1780\n1781\n1782\n1783\n1784\n1785\n1786\n1787\n1788\n1789\n1790\n1791\n1792\n1793\n1794\n1795\n1796\n1797\n1798\n1799\n1800\n1801\n1802\n1803\n1804\n1805\n1806\n1807\n1808\n1809\n1810\n1811\n1812\n1813\n1814\n1815\n1816\n1817\n1818\n1819\n1820\n1821\n1822\n1823\n1824\n1825\n1826\n1827\n1828\n1829\n1830\n1831\n1832\n1833\n1834\n1835\n1836\n1837\n1838\n1839\n1840\n1841\n1842\n1843\n1844\n1845\n1846\n1847\n1848\n1849\n1850\n1851\n1852\n1853\n1854\n1855\n1856\n1857\n1858\n1859\n1860\n1861\n1862\n1863\n1864\n1865\n1866\n1867\n1868\n1869\n1870\n1871\n1872\n1873\n1874\n1875\n1876\n1877\n1878\n1879\n1880\n1881\n1882\n1883\n1884\n1885\n1886\n1887\n1888\n1889\n1890\n1891\n1892\n1893\n1894\n1895\n1896\n1897\n1898\n1899\n1900\n1901\n1902\n1903\n1904\n1905\n1906\n1907\n1908\n1909\n1910\n1911\n1912\n1913\n1914\n1915\n1916\n1917\n1918\n1919\n1920\n1921\n1922\n1923\n1924\n1925\n1926\n1927\n1928\n1929\n1930\n1931\n1932\n1933\n1934\n1935\n1936\n1937\n1938\n1939\n1940\n1941\n1942\n1943\n1944\n1945\n1946\n1947\n1948\n1949\n1950\n1951\n1952\n1953\n1954\n1955\n1956\n1957\n1958\n1959\n1960\n1961\n1962\n1963\n1964\n1965\n1966\n1967\n1968\n1969\n1970\n1971\n1972\n1973\n1974\n1975\n1976\n1977\n1978\n1979\n1980\n1981\n1982\n1983\n1984\n1985\n1986\n1987\n1988\n1989\n1990\n1991\n1992\n1993\n1994\n1995\n1996\n1997\n1998\n1999\n2000\n2001\n2002\n2003\n2004\n2005\n2006\n2007\n2008\n2009\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n2024\n2025\n2026\n2027\n2028\n2029\n2030\n2031\n2032\n2033\n2034\n2035\n2036\n2037\n2038\n2039\n2040\n2041\n2042\n2043\n2044\n2045\n2046\n2047\n2048\n2049\n2050\n2051\n2052\n2053\n2054\n2055\n2056\n2057\n2058\n2059\n9번째 수행\nload_corpus 시작\ntext_splitter 통과\nnode 할당\nchunker 통과\nsplit_document\nParsed 403 nodes\ncorpus 시작\n2060\n2061\n2062\n2063\n2064\n2065\n2066\n2067\n2068\n2069\n2070\n2071\n2072\n2073\n2074\n2075\n2076\n2077\n2078\n2079\n2080\n2081\n2082\n2083\n2084\n2085\n2086\n2087\n2088\n2089\n2090\n2091\n2092\n2093\n2094\n2095\n2096\n2097\n2098\n2099\n2100\n2101\n2102\n2103\n2104\n2105\n2106\n2107\n2108\n2109\n2110\n2111\n2112\n2113\n2114\n2115\n2116\n2117\n2118\n2119\n2120\n2121\n2122\n2123\n2124\n2125\n2126\n2127\n2128\n2129\n2130\n2131\n2132\n2133\n2134\n2135\n2136\n2137\n2138\n2139\n2140\n2141\n2142\n2143\n2144\n2145\n2146\n2147\n2148\n2149\n2150\n2151\n2152\n2153\n2154\n2155\n2156\n2157\n2158\n2159\n2160\n2161\n2162\n2163\n2164\n2165\n2166\n2167\n2168\n2169\n2170\n2171\n2172\n2173\n2174\n2175\n2176\n2177\n2178\n2179\n2180\n2181\n2182\n2183\n2184\n2185\n2186\n2187\n2188\n2189\n2190\n2191\n2192\n2193\n2194\n2195\n2196\n2197\n2198\n2199\n2200\n2201\n2202\n2203\n2204\n2205\n2206\n2207\n2208\n2209\n2210\n2211\n2212\n2213\n2214\n2215\n2216\n2217\n2218\n2219\n2220\n2221\n2222\n2223\n2224\n2225\n2226\n2227\n2228\n2229\n2230\n2231\n2232\n2233\n2234\n2235\n2236\n2237\n2238\n2239\n2240\n2241\n2242\n2243\n2244\n2245\n2246\n2247\n2248\n2249\n2250\n2251\n2252\n2253\n2254\n2255\n2256\n2257\n2258\n2259\n2260\n2261\n2262\n2263\n2264\n2265\n2266\n2267\n2268\n2269\n2270\n2271\n2272\n2273\n2274\n2275\n2276\n2277\n2278\n2279\n2280\n2281\n2282\n2283\n2284\n2285\n2286\n2287\n2288\n2289\n2290\n2291\n2292\n2293\n2294\n2295\n2296\n2297\n2298\n2299\n2300\n2301\n2302\n2303\n2304\n2305\n2306\n2307\n2308\n2309\n2310\n2311\n2312\n2313\n2314\n2315\n2316\n2317\n2318\n2319\n2320\n2321\n2322\n2323\n2324\n2325\n2326\n2327\n2328\n2329\n2330\n2331\n2332\n2333\n2334\n2335\n2336\n2337\n2338\n2339\n2340\n2341\n2342\n2343\n2344\n2345\n2346\n2347\n2348\n2349\n2350\n2351\n2352\n2353\n2354\n2355\n2356\n2357\n2358\n2359\n2360\n2361\n2362\n2363\n2364\n2365\n2366\n2367\n2368\n2369\n2370\n2371\n2372\n2373\n2374\n2375\n2376\n2377\n2378\n2379\n2380\n2381\n2382\n2383\n2384\n2385\n2386\n2387\n2388\n2389\n2390\n2391\n2392\n2393\n2394\n2395\n2396\n2397\n2398\n2399\n2400\n2401\n2402\n2403\n2404\n2405\n2406\n2407\n2408\n2409\n2410\n2411\n2412\n2413\n2414\n2415\n2416\n2417\n2418\n2419\n2420\n2421\n2422\n2423\n2424\n2425\n2426\n2427\n2428\n2429\n2430\n2431\n2432\n2433\n2434\n2435\n2436\n2437\n2438\n2439\n2440\n2441\n2442\n2443\n2444\n2445\n2446\n2447\n2448\n2449\n2450\n2451\n2452\n2453\n2454\n2455\n2456\n2457\n2458\n2459\n2460\n2461\n2462\n"
        }
      ],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717750685941
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_FILES=[f\"/home/azureuser/cloudfiles/code/Users/hm.lim/Gen/generated_QAdata/SemanticChunking_time_test/0604/train_corpus/corpus.txt\"]\n",
        "train_corpus=load_corpus(TRAIN_FILES,verbose=True)\n",
        "\n",
        "save_to_json(train_corpus, os.path.join(OUTPUT_DIR, 'train_corpus.json'))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717389792322
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "node_ids=-1\n",
        "for i in range(7,10):\n",
        "    print(f\"{i}번째 수행\")\n",
        "    TRAIN_FILES=[f\"/home/azureuser/cloudfiles/code/Users/hm.lim/Gen/generated_QAdata/SemanticChunking_time_test/0605/train_corpus/corpus{i}.txt\"]\n",
        "    \n",
        "    train_corpus=load_corpus(TRAIN_FILES,verbose=True)\n",
        "\n",
        "    \n",
        "    train_queries,train_relevant_docs,train_answers,node_id=generate_three_question_answer_queries(train_corpus,node_ids)\n",
        "    \n",
        "    save_to_json(train_queries,os.path.join(OUTPUT_DIR,'train_queries.json'))\n",
        "    save_to_json(train_relevant_docs, os.path.join(OUTPUT_DIR, 'train_relevant_docs.json'))\n",
        "    save_to_json(train_answers, os.path.join(OUTPUT_DIR, 'train_answers.json'))\n",
        "    node_ids=node_id "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "7번째 수행\nload_corpus 시작\ntext_splitter 통과\nnode 할당\nchunker 통과\nsplit_document\nParsed 245 nodes\ncorpus 시작\n8번째 수행\nload_corpus 시작\ntext_splitter 통과\nnode 할당\nchunker 통과\nsplit_document\nParsed 623 nodes\ncorpus 시작\n9번째 수행\nload_corpus 시작\ntext_splitter 통과\nnode 할당\nchunker 통과\nsplit_document\nParsed 1037 nodes\ncorpus 시작\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "100%|██████████| 245/245 [1:40:38<00:00, 24.65s/it]\n 52%|█████▏    | 321/623 [2:24:13<2:14:23, 26.70s/it]Bad pipe message: %s [b'O\\x96\\x9a991\\\\j4\\x85\\xf45gqg\\xac\\x9e\\xbb \\xf9\\x13\\xee8yEK\\xbd\\x0f\\x06Ur\\xd6\\xa3\\xc0\\x84x\\xb7\\x05\\xe4\\xde\\xcb\\xa7\\xaf\"\\x01\\xac)\\x9d\\xd7qL\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x00\\x1e\\x00', b'\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06\\x01']\nBad pipe message: %s [b'#\\x1b\\x9e\\xdbe6^\\xbc\\xcf)+0\\x85\\x16E\\x89\\xb2\\xb3 \\t?\\x07\\xf7\\xfb\\x8c\\xf7U\\x1e\"\\x19\\x13\\xaf\\xac\\xd9\\xfe\\x91\\nr\\xe7\\x95q\\xa8@mf\\xbb\\xa1_tOG\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x00\\x1e\\x00\\x1c\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01', b'\\x00+\\x00\\x03\\x02', b'\\x00-']\nBad pipe message: %s [b\"e\\xa0\\\\p\\x14\\x19n\\xeb\\xff\\x8d{\\xfe\\xcdb\\x88\\xf0k7\\x00\\x00\\xa6\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0s\\xc0w\\x00\\xc4\\x00\\xc3\\xc0#\\xc0'\\x00g\\x00@\\xc0r\\xc0v\\x00\\xbe\\x00\\xbd\\xc0\\n\\xc0\\x14\\x009\\x008\\x00\\x88\\x00\", b'\\t\\xc0\\x13\\x003\\x002\\x00\\x9a\\x00\\x99\\x00E\\x00D\\xc0\\x07\\xc0\\x11\\xc0\\x08\\xc0\\x12\\x00\\x16\\x00\\x13\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00\\xc0\\x00<\\x00\\xba\\x005\\x00\\x84\\x00/\\x00\\x96\\x00A\\x00\\x05\\x00\\n\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x000\\x00.\\x04\\x03\\x05']\nBad pipe message: %s [b'\\x03\\x08']\nBad pipe message: %s [b'\\x08\\x08\\t\\x08\\n\\x08']\nBad pipe message: %s [b'\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06']\nBad pipe message: %s [b'', b'\\x03\\x03']\nBad pipe message: %s [b'']\nBad pipe message: %s [b'', b'\\x02']\nBad pipe message: %s [b'\\x05\\x02\\x06']\nBad pipe message: %s [b'\\x88\\xa70\\xf6f\\x14P\\xad~\\xae\\xd0\\x1d}\\xdd\\xee\\x9d\\xfa\\n\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00', b'\\x06\\x00\\x17\\x00\\x03\\xc0\\x10']\nBad pipe message: %s [b' \\x04\\xe8o\\x89\\xb4_D/4\\xc0\\xb5\\x85\\xeb\\x1b\\xfe\\x83\\xfa\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0']\nBad pipe message: %s [b'\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03']\nBad pipe message: %s [b\"\\x1fO\\xda\\xb3\\t\\x01\\x13_\\xb8\\xc7\\xdftL\\x19\\xc7D\\xed\\x19\\x00\\x00\\x86\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\"]\nBad pipe message: %s [b\"\\xf4\\x9f\\x055\\xe3\\x06B\\xe2a\\xeb\\x8c\\x08ujV\\x8d7#\\x00\\x00\\xf4\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00\\xa7\\x00m\\x00:\\x00\\x89\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\x00\\x84\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x00\\xa6\\x00l\\x004\\x00\\x9b\\x00F\\xc01\\xc0-\\xc0)\\xc0\"]\nBad pipe message: %s [b'\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00']\nBad pipe message: %s [b'\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f']\n100%|██████████| 623/623 [4:42:09<00:00, 27.17s/it]  \n100%|██████████| 1037/1037 [8:41:55<00:00, 30.20s/it]  \nBad pipe message: %s [b\"2\\x952a\\xc3=\\x87\\x0c{\\xad_\\x84HM\\x1c\\xeco\\xda\\x00\\x00|\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0#\\xc0'\\x00g\\x00@\\xc0\\n\\xc0\\x14\\x009\\x008\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\"]\nBad pipe message: %s [b\";\\xce15\\xd78P\\xeb)\\x0e$\\x06\\xa9\\xec\\x0c\\x11]\\xae\\x00\\x00\\xa6\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0s\\xc0w\\x00\\xc4\\x00\\xc3\\xc0#\\xc0'\\x00g\\x00@\\xc0r\\xc0v\\x00\\xbe\\x00\\xbd\\xc0\\n\\xc0\\x14\\x009\\x008\\x00\\x88\\x00\\x87\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9a\\x00\\x99\\x00E\\x00D\\xc0\\x07\\xc0\\x11\\xc0\\x08\\xc0\\x12\\x00\\x16\\x00\\x13\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00\\xc0\\x00<\\x00\\xba\\x005\\x00\\x84\\x00/\\x00\\x96\\x00A\\x00\\x05\\x00\\n\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\"]\nBad pipe message: %s [b'\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18']\nBad pipe message: %s [b'\\x99\\xeb\\xf1\\xac\\xb9\\xe3\\x9f\\x00\\xf4\\x9bY%t\\xf6\\xfe:\\x105\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15\\xc0\\x0b\\xc0\\x01\\x00\\x02\\x00\\x01\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a']\nBad pipe message: %s [b\"\\xf5\\xecl\\xa0\\xc6\\x8dV\\xe9Q\\xd4\\xb5\\x94'\\x98\\xaa\\x9d:r\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x15\\x03\\x00\\x00\\x02\\x02\"]\nBad pipe message: %s [b'odX\\xe9\\x1e\\x01\\xea\\x97\\xe2\\xa9\\x9bzql\\x86\\xfa)~\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00', b'\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12']\nBad pipe message: %s [b\"\\xc3\\xbb\\x03\\xe0\\xbbnE\\x94 \\x8b\\xf7\\xa00\\xa8\\xcd\\x8a\\xcaC\\x00\\x00\\x86\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00g\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\"]\nBad pipe message: %s [b\"\\xd5}\\\\2\\x93\\n<\\xe4\\t\\xb0:\\xf7|\\xcbf\\x00P-\\x00\\x00\\xf4\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00\\xa7\\x00m\\x00:\\x00\\x89\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\x00\\x84\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x00\\xa6\\x00l\\x00\", b'\\x9b\\x00F\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13']\n"
        }
      ],
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1718032883846
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VAL_FILES=[f\"/home/azureuser/cloudfiles/code/Users/hm.lim/Gen/generated_QAdata/SemanticChunking_time_test/0604/val_corpus/corpus.txt\"]\n",
        "val_corpus=load_corpus(VAL_FILES,verbose=True)\n",
        "\n",
        "save_to_json(val_corpus, os.path.join(OUTPUT_DIR, 'val_corpus.json'))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "load_corpus 시작\ntext_splitter 통과\nnode 할당\nchunker 통과\n"
        }
      ],
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717390570602
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "node_ids=-1\n",
        "for i in range(1,4):\n",
        "    print(f\"{i}번째 수행\")\n",
        "    #TRAIN_FILES=[f\"/home/azureuser/cloudfiles/code/Users/hm.lim/Gen/generated_QAdata/SementicChunking_new/train_corpus_split/corpus{i}.txt\"]\n",
        "    VAL_FILES=[f\"/home/azureuser/cloudfiles/code/Users/hm.lim/Gen/generated_QAdata/SemanticChunking_time_test/0607/val_corpus/corpus{i}.txt\"]\n",
        "\n",
        "    #train_corpus=load_corpus(TRAIN_FILES,verbose=True)\n",
        "    val_corpus=load_corpus(VAL_FILES,verbose=True)\n",
        "\n",
        "    #train_queries,train_relevant_docs,train_answers=generate_three_question_answer_queries(train_corpus)\n",
        "    \n",
        "    #save_to_json(train_queries,os.path.join(OUTPUT_DIR,'train_queries.json'))\n",
        "    #save_to_json(train_relevant_docs, os.path.join(OUTPUT_DIR, 'train_relevant_docs.json'))\n",
        "    #save_to_json(train_answers, os.path.join(OUTPUT_DIR, 'train_answers.json'))\n",
        "    \n",
        "    val_queries, val_relevant_docs, val_answers,node_id = generate_three_question_answer_queries(val_corpus,node_ids)\n",
        "    save_to_json(val_queries, os.path.join(OUTPUT_DIR, 'val_queries.json'))\n",
        "    save_to_json(val_relevant_docs, os.path.join(OUTPUT_DIR, 'val_relevant_docs.json'))\n",
        "    save_to_json(val_answers, os.path.join(OUTPUT_DIR, 'val_answers.json'))\n",
        "\n",
        "    node_ids=node_id    \n",
        "\n",
        "    #torch.cuda.empty_cache()\n",
        "    #TRAIN_DATASET_FPATH = '/home/azureuser/cloudfiles/code/Users/hm.lim/Gen/generated_QAdata/Q_A/train_dataset.json'\n",
        "   # VAL_DATASET_FPATH = '/home/azureuser/cloudfiles/code/Users/hm.lim/Gen/generated_QAdata/Q_A/val_dataset.json'\n",
        "    #train_dataset = {\n",
        "    #    'queries': train_queries,\n",
        "    #    'corpus': train_corpus,\n",
        "    #    'relevant_docs': train_relevant_docs,\n",
        "    #    'answers':train_answers,\n",
        "    #}\n",
        "    \"\"\"\n",
        "    val_dataset = {\n",
        "        'queries': val_queries,\n",
        "        'corpus': val_corpus,\n",
        "        'relevant_docs': val_relevant_docs,\n",
        "        'answers':val_answers,\n",
        "    }\n",
        "    \"\"\"\n",
        "    #os.makedirs('/home/azureuser/cloudfiles/code/Users/hm.lim/Gen/generated_QAdata/Q_A', exist_ok=True)\n",
        "    #if os.path.exists(TRAIN_DATASET_FPATH):\n",
        "    #    with open(TRAIN_DATASET_FPATH,\"r\",encoding='utf-8') as f:\n",
        "    #        train_dataset=json.load(f)\n",
        "    #    train_dataset.update(train_dataset)\n",
        "    #with open(TRAIN_DATASET_FPATH, 'w+') as f:\n",
        "    #    json.dump(train_dataset, f, ensure_ascii=False)\n",
        "    \"\"\"\n",
        "    if os.path.exists(VAL_DATASET_FPATH):\n",
        "        with open(VAL_DATASET_FPATH,\"r\",encoding='utf-8') as f:\n",
        "            data=json.load(f)\n",
        "            data.update(val_dataset)\n",
        "        with open(VAL_DATASET_FPATH, 'w+') as f:\n",
        "            json.dump(data, f, ensure_ascii=False)\n",
        "    else:\n",
        "        with open(VAL_DATASET_FPATH,'w+') as f:\n",
        "            json.dump(val_dataset,f,ensure_ascii=False)\n",
        "    \"\"\""
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "1번째 수행\nload_corpus 시작\ntext_splitter 통과\nnode 할당\nchunker 통과\nsplit_document\nParsed 311 nodes\ncorpus 시작\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "  2%|▏         | 6/311 [02:39<2:14:46, 26.51s/it]\n"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 18\u001b[0m\n\u001b[1;32m     10\u001b[0m val_corpus\u001b[38;5;241m=\u001b[39mload_corpus(VAL_FILES,verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#train_queries,train_relevant_docs,train_answers=generate_three_question_answer_queries(train_corpus)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#save_to_json(train_queries,os.path.join(OUTPUT_DIR,'train_queries.json'))\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#save_to_json(train_relevant_docs, os.path.join(OUTPUT_DIR, 'train_relevant_docs.json'))\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#save_to_json(train_answers, os.path.join(OUTPUT_DIR, 'train_answers.json'))\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m val_queries, val_relevant_docs, val_answers,node_id \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_three_question_answer_queries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_corpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnode_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m save_to_json(val_queries, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(OUTPUT_DIR, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_queries.json\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     20\u001b[0m save_to_json(val_relevant_docs, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(OUTPUT_DIR, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_relevant_docs.json\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
            "Cell \u001b[0;32mIn[4], line 41\u001b[0m, in \u001b[0;36mgenerate_three_question_answer_queries\u001b[0;34m(text_data, node_ids, num_questions_per_chunk, num_answers_per_chunk, prompt_template, verbose)\u001b[0m\n\u001b[1;32m     30\u001b[0m query\u001b[38;5;241m=\u001b[39mprompt_template\u001b[38;5;241m.\u001b[39mformat(context_str\u001b[38;5;241m=\u001b[39mtext,num_questions_per_chunk\u001b[38;5;241m=\u001b[39mnum_questions_per_chunk,num_answers_per_chunk\u001b[38;5;241m=\u001b[39mnum_answers_per_chunk)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# response = client.completions.create(\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#     model= \"gpt-4-turbo\",\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m#     prompt=query,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m \n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# API 호출\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4o\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#IX-GPT4\",  # Azure에서 사용 가능한 모델 명시\u001b[39;49;00m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[38;5;124;43m Please Generate long and more detailed answers. 1. Q1, Q2, Q3가 최대한 겹치지 않게 생성해라.\u001b[39;49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;43m        2. 줄바꿈은 무조건 한 번씩만 해라. 출력은 다음 양식을 따른다:\u001b[39;49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;43m        \u001b[39;49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;43m        Q1: \u001b[39;49m\u001b[38;5;124;43m{{\u001b[39;49m\u001b[38;5;124;43m첫 번째 생성된 질문}}\u001b[39;49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;43m        A1: \u001b[39;49m\u001b[38;5;124;43m{{\u001b[39;49m\u001b[38;5;124;43m첫 번째 질문에 대한 답변}}\u001b[39;49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;43m        Q2: \u001b[39;49m\u001b[38;5;124;43m{{\u001b[39;49m\u001b[38;5;124;43m두 번째 생성된 질문}}\u001b[39;49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;43m        A2: \u001b[39;49m\u001b[38;5;124;43m{{\u001b[39;49m\u001b[38;5;124;43m두 번째 질문에 대한 답변}}\u001b[39;49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;43m        Q3: \u001b[39;49m\u001b[38;5;124;43m{{\u001b[39;49m\u001b[38;5;124;43m세 번째 생성된 질문}}\u001b[39;49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;43m        A3: \u001b[39;49m\u001b[38;5;124;43m{{\u001b[39;49m\u001b[38;5;124;43m세 번째 질문에 대한 답변}} \u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[1;32m     56\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m#response=client.chat.completions.create(\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m#    model=\"gpt-4o\",\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m#    messages=[\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m#)\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m#response=client.invoke(query)\u001b[39;00m\n\u001b[1;32m     70\u001b[0m result \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
            "File \u001b[0;32m/anaconda/envs/hm_env/lib/python3.10/site-packages/openai/_utils/_utils.py:277\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/hm_env/lib/python3.10/site-packages/openai/resources/chat/completions.py:590\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    560\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    589\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 590\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/hm_env/lib/python3.10/site-packages/openai/_base_client.py:1240\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1227\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1228\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1235\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1236\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1237\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1238\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1239\u001b[0m     )\n\u001b[0;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[0;32m/anaconda/envs/hm_env/lib/python3.10/site-packages/openai/_base_client.py:921\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    913\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    914\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    919\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    920\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/hm_env/lib/python3.10/site-packages/openai/_base_client.py:952\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    949\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending HTTP Request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, request\u001b[38;5;241m.\u001b[39mmethod, request\u001b[38;5;241m.\u001b[39murl)\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 952\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    958\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "File \u001b[0;32m/anaconda/envs/hm_env/lib/python3.10/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    906\u001b[0m follow_redirects \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_redirects\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m follow_redirects\n\u001b[1;32m    910\u001b[0m )\n\u001b[1;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
            "File \u001b[0;32m/anaconda/envs/hm_env/lib/python3.10/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[0;32m/anaconda/envs/hm_env/lib/python3.10/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
            "File \u001b[0;32m/anaconda/envs/hm_env/lib/python3.10/site-packages/httpx/_client.py:1015\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1011\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1012\u001b[0m     )\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1015\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1019\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
            "File \u001b[0;32m/anaconda/envs/hm_env/lib/python3.10/site-packages/httpx/_transports/default.py:233\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    220\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    221\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    222\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    231\u001b[0m )\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 233\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    238\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    239\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    240\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    241\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    242\u001b[0m )\n",
            "File \u001b[0;32m/anaconda/envs/hm_env/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:216\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    213\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, Iterable)\n",
            "File \u001b[0;32m/anaconda/envs/hm_env/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:196\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    192\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
            "File \u001b[0;32m/anaconda/envs/hm_env/lib/python3.10/site-packages/httpcore/_sync/connection.py:101\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/hm_env/lib/python3.10/site-packages/httpcore/_sync/http11.py:143\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
            "File \u001b[0;32m/anaconda/envs/hm_env/lib/python3.10/site-packages/httpcore/_sync/http11.py:113\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m    106\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    107\u001b[0m     (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m         trailing_data,\n\u001b[0;32m--> 113\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    115\u001b[0m         http_version,\n\u001b[1;32m    116\u001b[0m         status,\n\u001b[1;32m    117\u001b[0m         reason_phrase,\n\u001b[1;32m    118\u001b[0m         headers,\n\u001b[1;32m    119\u001b[0m     )\n\u001b[1;32m    121\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
            "File \u001b[0;32m/anaconda/envs/hm_env/lib/python3.10/site-packages/httpcore/_sync/http11.py:186\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    183\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[0;32m/anaconda/envs/hm_env/lib/python3.10/site-packages/httpcore/_sync/http11.py:224\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    221\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 224\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
            "File \u001b[0;32m/anaconda/envs/hm_env/lib/python3.10/site-packages/httpcore/_backends/sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/hm_env/lib/python3.10/ssl.py:1292\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1288\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1289\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1290\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1291\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1292\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
            "File \u001b[0;32m/anaconda/envs/hm_env/lib/python3.10/ssl.py:1165\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717923122062
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "node_ids=-1\n",
        "for i in range(1,4):\n",
        "    print(f\"{i}번째 수행\")\n",
        "    VAL_FILES=[f\"/home/azureuser/cloudfiles/code/Users/hm.lim/Gen/generated_QAdata/SemanticChunking_time_test/0607/val_corpus/corpus{i}.txt\"]\n",
        "    val_corpus,node_id=load_corpus(VAL_FILES,node_ids,verbose=True)\n",
        "\n",
        "    save_to_json(val_corpus, os.path.join(OUTPUT_DIR, 'val_corpus.json'))\n",
        "    node_ids=node_id "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "1번째 수행\nload_corpus 시작\ntext_splitter 통과\nnode 할당\nchunker 통과\nsplit_document\nParsed 310 nodes\ncorpus 시작\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n2번째 수행\nload_corpus 시작\ntext_splitter 통과\nnode 할당\nchunker 통과\nsplit_document\nParsed 248 nodes\ncorpus 시작\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n500\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n3번째 수행\nload_corpus 시작\ntext_splitter 통과\nnode 할당\nchunker 통과\nsplit_document\nParsed 184 nodes\ncorpus 시작\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\n600\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\n700\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717741791738
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "TRAIN_DATASET_FPATH = '/home/azureuser/cloudfiles/code/Users/hm.lim/Gen/generated_QAdata/Q_A/0607/train_dataset_ne.json'\n",
        "os.makedirs('/home/azureuser/cloudfiles/code/Users/hm.lim/Gen/generated_QAdata/Q_A/0607', exist_ok=True)\n",
        "TRAIN_QUERY_FPATH=\"/home/azureuser/cloudfiles/code/Users/hm.lim/Gen/generated_QAdata/SemanticChunking_time_test/0607/train_queries.json\"\n",
        "TRAIN_ANSWER_FPATH=\"/home/azureuser/cloudfiles/code/Users/hm.lim/Gen/generated_QAdata/SemanticChunking_time_test/0607/train_answers.json\"\n",
        "TRAIN_RELEVANT_FPATH=\"/home/azureuser/cloudfiles/code/Users/hm.lim/Gen/generated_QAdata/SemanticChunking_time_test/0607/train_relevant_docs.json\"\n",
        "TRAIN_CORPUS_FPATH=\"/home/azureuser/cloudfiles/code/Users/hm.lim/Gen/generated_QAdata/SemanticChunking_time_test/0607/train_corpus.json\"\n",
        "\n",
        "VAL_DATASET_FPATH = '/home/azureuser/cloudfiles/code/Users/hm.lim/Gen/generated_QAdata/Q_A/0607/val_dataset_ne.json'\n",
        "os.makedirs('/home/azureuser/cloudfiles/code/Users/hm.lim/Gen/generated_QAdata/Q_A/0607', exist_ok=True)\n",
        "VAL_QUERY_FPATH=\"/home/azureuser/cloudfiles/code/Users/hm.lim/Gen/generated_QAdata/SemanticChunking_time_test/0607/val_queries.json\"\n",
        "VAL_ANSWER_FPATH=\"/home/azureuser/cloudfiles/code/Users/hm.lim/Gen/generated_QAdata/SemanticChunking_time_test/0607/val_answers.json\"\n",
        "VAL_RELEVANT_FPATH=\"/home/azureuser/cloudfiles/code/Users/hm.lim/Gen/generated_QAdata/SemanticChunking_time_test/0607/val_relevant_docs.json\"\n",
        "VAL_CORPUS_FPATH=\"/home/azureuser/cloudfiles/code/Users/hm.lim/Gen/generated_QAdata/SemanticChunking_time_test/0607/val_corpus.json\"\n",
        "\n",
        "\n",
        "with open(TRAIN_QUERY_FPATH,\"r\",encoding='utf-8') as f:\n",
        "            train_query=json.load(f)\n",
        "with open(TRAIN_ANSWER_FPATH,\"r\",encoding='utf-8') as f:\n",
        "            train_answer=json.load(f)    \n",
        "with open(TRAIN_RELEVANT_FPATH,\"r\",encoding='utf-8') as f:\n",
        "            train_relevant=json.load(f)\n",
        "with open(TRAIN_CORPUS_FPATH,\"r\",encoding='utf-8') as f:\n",
        "            train_corpus=json.load(f)   \n",
        "\n",
        "train_dataset = {\n",
        "        'queries': train_query,\n",
        "        'corpus': train_corpus,\n",
        "        'relevant_docs': train_relevant,\n",
        "        'answers':train_answer,\n",
        "    }\n",
        "\n",
        "with open(TRAIN_DATASET_FPATH,'w+') as f:\n",
        "    json.dump(train_dataset,f,ensure_ascii=False)\n",
        "\n",
        "with open(VAL_QUERY_FPATH,\"r\",encoding='utf-8') as f:\n",
        "            val_query=json.load(f)\n",
        "with open(VAL_ANSWER_FPATH,\"r\",encoding='utf-8') as f:\n",
        "            val_answer=json.load(f)    \n",
        "with open(VAL_RELEVANT_FPATH,\"r\",encoding='utf-8') as f:\n",
        "            val_relevant=json.load(f)\n",
        "with open(VAL_CORPUS_FPATH,\"r\",encoding='utf-8') as f:\n",
        "            val_corpus=json.load(f)   \n",
        "\n",
        "val_dataset = {\n",
        "        'queries': val_query,\n",
        "        'corpus': val_corpus,\n",
        "        'relevant_docs': val_relevant,\n",
        "        'answers':val_answer,\n",
        "    }\n",
        "with open(VAL_DATASET_FPATH,'w+') as f:\n",
        "    json.dump(val_dataset,f,ensure_ascii=False)"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1718081246536
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "hm_env",
      "language": "python",
      "display_name": "hm_env"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "hm_env"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}