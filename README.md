# LLM_chatbot

## ğŸ“ Data collection : 

<img width="821" alt="image" src="https://github.com/nanhungrybin/LLM_chatbot/assets/97181397/4ddab2a2-cf41-43d4-86d7-710a85dc56a4">


## ğŸ“ Retrival model Finetuning ( Domain Adaptation & Text embedding model Fintuning ) : 

<img width="821" alt="image" src="https://github.com/nanhungrybin/LLM_chatbot/assets/97181397/abb16bf1-1d91-4b07-beff-17a1092256d2">


## ğŸ“ Generation model Fintuning :
- í•œêµ­ì–´ì— íŠ¹í™”ëœ Saltlux/Ko-Llama3-Luxia-8B í™œìš©
- GPT-4oë¥¼ ì´ìš©í•œ Q & A datasetìƒì„± (í˜„ì¬ ìƒì„± ìš©ëŸ‰: 16.76MB) 
   - input: Relevant Document, output: Q & A
   -  Q&A dataset (train: 4269ê°œ, validation: 1641ê°œ)
     
- Fintuning Method
  - QuantizedLoRA + FSDP(Fully Shared Data Parallel) ë°©ë²• í™œìš©
  - Instruction Tuning on ì‚¬ì¶œì„±í˜• (Injection Molding) dataset
    - {Question: Relevant Document } -> Answer
      
- í‰ê°€ ì§€í‘œ : ì½”ì‚¬ì¸ ìœ ì‚¬ë„
  - ì§ˆë¬¸ì´ ì£¼ì–´ì¡Œì„ ë•Œ, GPT-4oê°€ ìƒì„±í•œ ë‹µë³€ (A)ì™€ ê° ëª¨ë¸ì´ ìƒì„±í•œ ë‹µë³€(B) ê°„ì˜ ìœ ì‚¬ë„ ì¸¡ì •

- íŒŒì¸íŠœë‹ ê²°ê³¼
<img width="925" alt="image" src="https://github.com/nanhungrybin/LLM_chatbot/assets/97181397/cb1c8557-bac2-4b42-9af0-9cfef61c1f49">

      
