{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.core import Settings\n",
        "import pdb\n",
        "\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "from llama_index.core import ServiceContext, VectorStoreIndex\n",
        "from llama_index.core.schema import TextNode\n",
        "from langchain.schema.document import Document\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1715062899379
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation with LlamaIndex"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 비교할 모델 :\n",
        "1. OpenAI_ada : OpenAI embedding \"ADA\"\n",
        "2. bert : open source \"bert-base-uncased\"\n",
        "3. bert_ST : Only sentence finetuned \"bert-base-uncased\" ( BERT )\n",
        "4. bert_KR : open source \"Beomi/KcBERT\"\n",
        "5. bert_KR_ST : Only sentence finetuned \"Beomi/KcBERT\" ( 한국어로 사전 학습된 BERT )\n",
        "6. bert_KR_DA : Domain Adaptation\n",
        "7. bert_KR_DA_ST : Domain Adaptation + sentence finetuned\n",
        "\n",
        "\n",
        "- 모델 저장할때 : sts_model_save_path = \"output/training_sts-\"+pretrained_model_name.replace(\"/\", \"-\")\\\n",
        "    + '-' + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "- 이름 설정 : DomainAdaptation => _DA SentenceFintuning => _ST"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### llama_index 이용 "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(dataset, embed_model, top_k=5, verbose=False):\n",
        "    \n",
        "    corpus = dataset['corpus']  # 예: {\"18f9717b-1c79-428e-9cc6-43e498abf29e\": \"Text for document\"}\n",
        "    queries = dataset['queries']  # 예: {\"18f9717b-1c79-428e-9cc6-43e498abf29e\": \"질문\"}\n",
        "    relevant_docs = dataset['relevant_docs']  \n",
        "\n",
        "\n",
        "    service_context = ServiceContext.from_defaults(embed_model=embed_model)\n",
        "    nodes = [TextNode(id_=id_, text=text) for id_, text in corpus.items()]\n",
        "    index = VectorStoreIndex(\n",
        "        nodes,\n",
        "        service_context=service_context,\n",
        "        show_progress=True\n",
        "    )\n",
        "\n",
        "    # 임베딩 계산하는 파트\n",
        "    retriever = index.as_retriever(similarity_top_k=top_k)\n",
        "\n",
        "    eval_results = []\n",
        "    for query_id, query in tqdm(queries.items(), desc=\"Evaluating queries\"):\n",
        "        try:\n",
        "            retrieved_nodes = retriever.retrieve(query)\n",
        "            retrieved_ids = [node.node.id_ for node in retrieved_nodes]   \n",
        "\n",
        "            expected_id = relevant_docs[query_id][0]  # 기본값을 None으로 설정\n",
        "            is_hit = expected_id in retrieved_ids\n",
        "\n",
        "            is_hit_numeric = int(is_hit)\n",
        "\n",
        "            eval_result = {\n",
        "                'is_hit': is_hit_numeric,\n",
        "                'retrieved': retrieved_ids,\n",
        "                'expected': expected_id,\n",
        "                'query': query_id,\n",
        "            }\n",
        "            eval_results.append(eval_result)\n",
        "            if verbose:\n",
        "                logging.info(f\"Query ID: {query_id}, Hit: {is_hit}, Expected: {expected_id}, Retrieved: {retrieved_ids}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error processing query ID {query_id}: {str(e)}\")\n",
        "\n",
        "\n",
        "    return eval_results"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1715055483527
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### faiss 이용 "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import faiss\n",
        "\n",
        "def build_index(embeddings):\n",
        "    d = embeddings.shape[1]  # 임베딩 벡터의 차원\n",
        "    # Faiss IndexIDMap 생성\n",
        "    index = faiss.IndexIDMap(faiss.IndexFlatL2(d))\n",
        "    index.add_with_ids(embeddings, np.arange(embeddings.shape[0]))  # 인덱스에 임베딩과 ID 추가\n",
        "    return index\n",
        "\n",
        "def retrieve(query_embedding, id_array, index, top_k=5):\n",
        "    # 쿼리 임베딩과 가장 유사한 문서 ID를 검색\n",
        "    _, retrieved_indices = index.search(query_embedding, top_k)\n",
        "    # 각 인덱스에 해당하는 문서 ID 반환\n",
        "    retrieved_doc_ids = id_array[retrieved_indices]\n",
        "    return retrieved_doc_ids\n",
        "\n",
        "\n",
        "def evaluate(dataset, model, top_k=5, verbose=False):\n",
        "    corpus = dataset['corpus']\n",
        "    queries = dataset['queries']\n",
        "    relevant_docs = dataset['relevant_docs']\n",
        "\n",
        "    corpus_df = pd.DataFrame.from_dict(corpus, orient='index', columns=['text'])\n",
        "    corpus_df.reset_index(inplace=True)\n",
        "    corpus_df.columns = ['ID', 'text']\n",
        "\n",
        "    # 코퍼스의 모든 텍스트를 인코딩\n",
        "    corpus_embeddings = model.encode(list(corpus_df['text']), convert_to_tensor=True)\n",
        "\n",
        "    if torch.cuda.is_available():  # GPU 사용 가능한 경우 CPU로 이동\n",
        "        corpus_embeddings = corpus_embeddings.cpu()\n",
        "    corpus_embeddings = corpus_embeddings.numpy()  # NumPy 배열로 변환\n",
        "\n",
        "    # 인덱스를 빌드하고 ID 배열을 반환\n",
        "    index = build_index(corpus_embeddings)\n",
        "    id_array = corpus_df['ID'].values\n",
        "\n",
        "    eval_results = []\n",
        "\n",
        "    for query_id, query_text in queries.items():\n",
        "        query_embedding = model.encode([query_text], convert_to_tensor=True)\n",
        "\n",
        "        if torch.cuda.is_available():  # GPU 사용 가능한 경우 CPU로 이동\n",
        "            query_embedding = query_embedding.cpu()\n",
        "        query_embedding = query_embedding.numpy()  # NumPy 배열로 변환\n",
        "        \n",
        "        retrieved_doc_ids = retrieve(query_embedding, id_array, index, top_k)  # 비교 해서 찾은 값 변환\n",
        "        # pdb.set_trace() #######################\n",
        "        expected_id = relevant_docs[query_id][0]\n",
        "\n",
        "        is_hit = expected_id in retrieved_doc_ids\n",
        "\n",
        "        eval_result = {\n",
        "            'is_hit': int(is_hit),\n",
        "            'retrieved': retrieved_doc_ids,\n",
        "            'expected': expected_id,\n",
        "            'query': query_id,\n",
        "        }\n",
        "        eval_results.append(eval_result)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Query ID: {query_id}, Hit: {is_hit}, Expected: {expected_id}, Retrieved: {retrieved_doc_ids}\")\n",
        "\n",
        "    return eval_results\n"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1715062907399
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import os\n",
        "\n",
        "def evaluate_st(dataset,model_id,name):\n",
        "\n",
        "    corpus = dataset['corpus']\n",
        "    queries = dataset['queries']\n",
        "    relevant_docs = dataset['relevant_docs']\n",
        "\n",
        "    evaluator = InformationRetrievalEvaluator(queries, corpus, relevant_docs, name=name)\n",
        "    model = SentenceTransformer(model_id)\n",
        "    return evaluator(model, output_path='/home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/EVAL_results/')"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1715062908164
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_API_KEY = \"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\"\n",
        "os.environ[\"OPENAI_API_KEY\"]=OPENAI_API_KEY\n",
        "\n",
        "TRAIN_DATASET_FPATH = '/home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/data/train_dataset.json'\n",
        "VAL_DATASET_FPATH = '/home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/data/val_dataset.json'\n",
        "\n",
        "path = \"/home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/\"\n",
        "\n",
        "with open(TRAIN_DATASET_FPATH, 'r+') as f:\n",
        "    train_dataset = json.load(f)\n",
        "\n",
        "with open(VAL_DATASET_FPATH, 'r+') as f:\n",
        "    val_dataset = json.load(f)"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1715062909614
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. ada : OpenAI embedding \"ADA\""
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "openai_model = OpenAIEmbedding(embed_batch_size=10)#임베딩 생성\n",
        "ada_val_results = evaluate(val_dataset, openai_model)\n",
        "print(\"ING ADA\")\n",
        "df_ada = pd.DataFrame(ada_val_results)\n",
        "df_ada['model'] = 'OpenAI_ada'\n",
        "print(\"ADA Hit Rate:\", df_ada['is_hit'].mean())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/tmp/ipykernel_8565/3937208437.py:8: DeprecationWarning: Call to deprecated function (or staticmethod) from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n  service_context = ServiceContext.from_defaults(embed_model=embed_model)\nGenerating embeddings:   0%|          | 0/1029 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nGenerating embeddings: 100%|██████████| 1029/1029 [01:36<00:00, 10.65it/s]\nEvaluating queries:  18%|█▊        | 354/2003 [02:36<11:50,  2.32it/s]ERROR:root:Error processing query ID 99b7d66f-4bce-4a3e-af68-a021c6c3dcc3: unsupported operand type(s) for *: 'NoneType' and 'float'\nEvaluating queries: 100%|██████████| 2003/2003 [14:46<00:00,  2.26it/s]\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "ING ADA\nADA Hit Rate: 0.5114885114885115\n"
        }
      ],
      "execution_count": 27,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714307605538
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. bert : open source \"bert-base-uncased\""
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "! pip install faiss-gpu==1.6.3"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: faiss-gpu==1.6.3 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (1.6.3)\r\nRequirement already satisfied: numpy in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from faiss-gpu==1.6.3) (1.21.6)\r\n"
        }
      ],
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714371014796
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import torch\n",
        "# import faiss\n",
        "\n",
        "# def build_index(embeddings):\n",
        "#     d = embeddings.shape[1]  # 임베딩 벡터의 차원\n",
        "#     # Faiss IndexIDMap 생성\n",
        "#     index = faiss.IndexIDMap(faiss.IndexFlatL2(d))\n",
        "#     index.add_with_ids(embeddings, np.arange(embeddings.shape[0]))  # 인덱스에 임베딩과 ID 추가\n",
        "#     return index\n",
        "\n",
        "# def retrieve(query_embedding, id_array, index, top_k=5):\n",
        "#     # 쿼리 임베딩과 가장 유사한 문서 ID를 검색\n",
        "#     _, retrieved_indices = index.search(query_embedding, top_k)\n",
        "#     # 각 인덱스에 해당하는 문서 ID 반환\n",
        "#     retrieved_doc_ids = id_array[retrieved_indices]\n",
        "#     return retrieved_doc_ids\n",
        "\n",
        "\n",
        "# def evaluate(dataset, model, top_k=5, verbose=False):\n",
        "#     corpus = dataset['corpus']\n",
        "#     queries = dataset['queries']\n",
        "#     relevant_docs = dataset['relevant_docs']\n",
        "\n",
        "#     corpus_df = pd.DataFrame.from_dict(corpus, orient='index', columns=['text'])\n",
        "#     corpus_df.reset_index(inplace=True)\n",
        "#     corpus_df.columns = ['ID', 'text']\n",
        "\n",
        "#     # 코퍼스의 모든 텍스트를 인코딩\n",
        "#     corpus_embeddings = model.encode(list(corpus_df['text']), convert_to_tensor=True)\n",
        "\n",
        "#     if torch.cuda.is_available():  # GPU 사용 가능한 경우 CPU로 이동\n",
        "#         corpus_embeddings = corpus_embeddings.cpu()\n",
        "#     corpus_embeddings = corpus_embeddings.numpy()  # NumPy 배열로 변환\n",
        "\n",
        "#     # 인덱스를 빌드하고 ID 배열을 반환\n",
        "#     index = build_index(corpus_embeddings)\n",
        "#     id_array = corpus_df['ID'].values\n",
        "\n",
        "#     eval_results = []\n",
        "\n",
        "#     for query_id, query_text in queries.items():\n",
        "#         query_embedding = model.encode([query_text], convert_to_tensor=True)\n",
        "\n",
        "#         if torch.cuda.is_available():  # GPU 사용 가능한 경우 CPU로 이동\n",
        "#             query_embedding = query_embedding.cpu()\n",
        "#         query_embedding = query_embedding.numpy()  # NumPy 배열로 변환\n",
        "        \n",
        "#         retrieved_doc_ids = retrieve(query_embedding, id_array, index, top_k)  # 비교 해서 찾은 값 변환\n",
        "#         # pdb.set_trace() #######################\n",
        "#         expected_id = relevant_docs[query_id][0]\n",
        "\n",
        "#         is_hit = expected_id in retrieved_doc_ids\n",
        "\n",
        "#         eval_result = {\n",
        "#             'is_hit': int(is_hit),\n",
        "#             'retrieved': retrieved_doc_ids,\n",
        "#             'expected': expected_id,\n",
        "#             'query': query_id,\n",
        "#         }\n",
        "#         eval_results.append(eval_result)\n",
        "\n",
        "#         if verbose:\n",
        "#             print(f\"Query ID: {query_id}, Hit: {is_hit}, Expected: {expected_id}, Retrieved: {retrieved_doc_ids}\")\n",
        "\n",
        "#     return eval_results\n"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714443636495
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, models\n",
        "#from llama_index.core.embeddings.utils import MockEmbedding\n",
        "import pandas as pd\n",
        "\n",
        "# 모델을 불러\n",
        "model_name = 'bert-base-uncased'\n",
        "\n",
        "transformer_model = models.Transformer(model_name)\n",
        "\n",
        "# Pooling layer 추가\n",
        "pooling_model = models.Pooling(transformer_model.get_word_embedding_dimension(),\n",
        "                            pooling_mode_mean_tokens=True,\n",
        "                            pooling_mode_cls_token=False,\n",
        "                            pooling_mode_max_tokens=False)\n",
        "\n",
        "# SentenceTransformer에 word_embedding_model과 pooling_model을 추가하여 모델 구성\n",
        "model = SentenceTransformer(modules=[transformer_model, pooling_model])\n",
        "\n",
        "# 평가를 위해 임베딩된 데이터셋과 함께 사용\n",
        "bert_val_results = evaluate(val_dataset, model)\n",
        "\n",
        "# 결과를 DataFrame으로 변환\n",
        "df_bert = pd.DataFrame(bert_val_results)\n",
        "df_bert['model'] = 'Bert'\n",
        "\n",
        "# Hit Rate\n",
        "hit_rate_bge = df_bert['is_hit'].mean()\n",
        "print(hit_rate_bge)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "0.01597603594608088\n"
        }
      ],
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714375257657
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customevaluate_st(val_dataset, model, output_folder, name='bert')"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'customevaluate_st' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcustomevaluate_st\u001b[49m(val_dataset, model, output_folder, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'customevaluate_st' is not defined"
          ]
        }
      ],
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714375617861
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. bert_finetuned : Only sentence finetuned \"bert-base-uncased\" ( BERT )"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_ST = \"local:../EXPERIMENT1_BaseBERT_generated_QAdata/exp_finetune\" \n",
        "val_results_finetuned = evaluate(val_dataset, bert_ST)\n",
        "print(\"ING finetuned\")\n",
        "df_bert_ST = pd.DataFrame(val_results_finetuned)\n",
        "df_bert_ST['model'] = 'Bert_ST'\n",
        "print(\"finetuned Hit Rate:\", df_bert_ST['is_hit'].mean())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/tmp/ipykernel_8565/3937208437.py:8: DeprecationWarning: Call to deprecated function (or staticmethod) from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n  service_context = ServiceContext.from_defaults(embed_model=embed_model)\nGenerating embeddings: 100%|██████████| 1029/1029 [00:12<00:00, 84.12it/s]\nEvaluating queries:  18%|█▊        | 354/2003 [00:25<01:59, 13.85it/s]ERROR:root:Error processing query ID 99b7d66f-4bce-4a3e-af68-a021c6c3dcc3: unsupported operand type(s) for *: 'NoneType' and 'float'\nEvaluating queries: 100%|██████████| 2003/2003 [02:25<00:00, 13.78it/s]\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "ING finetuned\nfinetuned Hit Rate: 0.23926073926073926\n"
        }
      ],
      "execution_count": 19,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714306029598
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"../EXPERIMENT1_BaseBERT_generated_QAdata/exp_finetune\"\n",
        "model = SentenceTransformer(model_id)\n",
        "customevaluate_st(val_dataset, model, output_folder, name='Bert_ST')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Batches: 100%|██████████| 63/63 [00:02<00:00, 22.65it/s]\nCorpus Chunks: 100%|██████████| 1/1 [00:09<00:00,  9.83s/it]\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "<class 'numpy.float64'>\n0.17955108710795925\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 20,
          "data": {
            "text/plain": "          0\n0  0.179551",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.179551</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 20,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714306056346
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Kcbert : open source \"Beomi/KcBERT\""
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from sentence_transformers import SentenceTransformer, models\n",
        "\n",
        "# # Load Embedding Model\n",
        "# embedding_model = models.Transformer(\n",
        "#     model_name_or_path='beomi/kcbert-base', \n",
        "#     max_seq_length=256,\n",
        "#     do_lower_case=True\n",
        "# )\n",
        "\n",
        "# # Only use Mean Pooling -> Pooling all token embedding vectors of sentence.\n",
        "# pooling_model = models.Pooling(\n",
        "#     embedding_model.get_word_embedding_dimension(),\n",
        "#     pooling_mode_mean_tokens=True,\n",
        "#     pooling_mode_cls_token=False,\n",
        "#     pooling_mode_max_tokens=False,\n",
        "# )\n",
        "\n",
        "# model = SentenceTransformer(modules=[embedding_model, pooling_model])\n",
        "\n",
        "# # 평가를 위해 임베딩된 데이터셋과 함께 사용\n",
        "# kcbert_val_results = evaluate(val_dataset, model)"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714375677132
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pdb\n",
        "from sentence_transformers import SentenceTransformer, models\n",
        "model_name = 'beomi/kcbert-base'\n",
        "\n",
        "\n",
        "# 모델과 토크나이저 로드\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "# 모델의 config에서 위치 임베딩 크기 확인\n",
        "print(\"Position embeddings size:\", model.config.max_position_embeddings)\n",
        "print(\"Hidden size (embedding dimension):\", model.config.hidden_size)\n",
        "\n",
        "transformer_model = models.Transformer(model_name) \n",
        "# 일반적으로 BERT 모델에서는 임베딩 차원의 크기가 768. 위치 임베딩의 크기가 300인 것으로 보입니다.\n",
        "# 768로 맞추기\n",
        "\n",
        "# Pooling layer 추가\n",
        "pooling_model = models.Pooling(transformer_model.get_word_embedding_dimension(),\n",
        "                            pooling_mode_mean_tokens=True,\n",
        "                            pooling_mode_cls_token=False,\n",
        "                            pooling_mode_max_tokens=False)\n",
        "#pdb.set_trace()###########################################################\n",
        "# SentenceTransformer에 word_embedding_model과 pooling_model을 추가하여 모델 구성\n",
        "model = SentenceTransformer(modules=[transformer_model, pooling_model])\n",
        "\n",
        "\n",
        "\n",
        "# 평가를 위해 임베딩된 데이터셋과 함께 사용\n",
        "kcbert_val_results = evaluate(val_dataset, model)\n",
        "\n",
        "# 결과를 DataFrame으로 변환\n",
        "df_kcbert = pd.DataFrame(kcbert_val_results)\n",
        "df_kcbert['model'] = 'Bert_KR'\n",
        "\n",
        "# Hit Rate\n",
        "hit_rate_bge = df_kcbert['is_hit'].mean()\n",
        "print(hit_rate_bge)\n",
        "\n",
        "\n",
        "# customevaluate_st(val_dataset, model, output_folder, name='Kcbert')"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (768) must match the size of tensor b (300) at non-singleton dimension 1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 28\u001b[0m\n\u001b[1;32m     14\u001b[0m model \u001b[38;5;241m=\u001b[39m SentenceTransformer(modules\u001b[38;5;241m=\u001b[39m[transformer_model, pooling_model])\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# # 임베딩할 텍스트 추출\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# texts_to_embed = list(val_dataset['corpus'].values())\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m \n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# 평가를 위해 임베딩된 데이터셋과 함께 사용\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m kcbert_val_results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# 결과를 DataFrame으로 변환\u001b[39;00m\n\u001b[1;32m     31\u001b[0m df_kcbert \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(kcbert_val_results)\n",
            "Cell \u001b[0;32mIn[5], line 31\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(dataset, model, top_k, verbose)\u001b[0m\n\u001b[1;32m     28\u001b[0m corpus_df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# 코퍼스의 모든 텍스트를 인코딩\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m corpus_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcorpus_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():  \u001b[38;5;66;03m# GPU 사용 가능한 경우 CPU로 이동\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     corpus_embeddings \u001b[38;5;241m=\u001b[39m corpus_embeddings\u001b[38;5;241m.\u001b[39mcpu()\n",
            "File \u001b[0;32m/anaconda/envs/llm-rag-embeddings/lib/python3.8/site-packages/sentence_transformers/SentenceTransformer.py:371\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    368\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(extra_features)\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 371\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m     out_features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence_embedding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m truncate_embeddings(\n\u001b[1;32m    373\u001b[0m         out_features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence_embedding\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtruncate_dim\n\u001b[1;32m    374\u001b[0m     )\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_value \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
            "File \u001b[0;32m/anaconda/envs/llm-rag-embeddings/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[0;32m/anaconda/envs/llm-rag-embeddings/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/llm-rag-embeddings/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/anaconda/envs/llm-rag-embeddings/lib/python3.8/site-packages/sentence_transformers/models/Transformer.py:98\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[1;32m     96\u001b[0m     trans_features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 98\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    101\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_tokens, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\n",
            "File \u001b[0;32m/anaconda/envs/llm-rag-embeddings/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/llm-rag-embeddings/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/anaconda/envs/llm-rag-embeddings/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:981\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    974\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m    979\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m--> 981\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    988\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m    989\u001b[0m     embedding_output,\n\u001b[1;32m    990\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    998\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    999\u001b[0m )\n\u001b[1;32m   1000\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n",
            "File \u001b[0;32m/anaconda/envs/llm-rag-embeddings/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/llm-rag-embeddings/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/anaconda/envs/llm-rag-embeddings/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:213\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabsolute\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    212\u001b[0m     position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embeddings(position_ids)\n\u001b[0;32m--> 213\u001b[0m     embeddings \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m position_embeddings\n\u001b[1;32m    214\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(embeddings)\n\u001b[1;32m    215\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(embeddings)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (768) must match the size of tensor b (300) at non-singleton dimension 1"
          ]
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714377768815
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. kbert_finetuned : Only sentence finetuned \"Beomi/KcBERT\" ( 한국어로 사전 학습된 BERT )"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kcbert_finetuned = \"local:../EXPERIMENT2_KCBERT_generated_QAdata/exp_finetune\" \n",
        "val_results_finetuned = evaluate(val_dataset, kcbert_finetuned)\n",
        "print(\"ING finetuned\")\n",
        "df_kcbert_ST = pd.DataFrame(val_results_finetuned)\n",
        "df_kcbert_ST['model'] = 'Bert_KR_ST'\n",
        "print(\"finetuned Hit Rate:\", df_kcbert_ST['is_hit'].mean())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/tmp/ipykernel_8565/3937208437.py:8: DeprecationWarning: Call to deprecated function (or staticmethod) from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n  service_context = ServiceContext.from_defaults(embed_model=embed_model)\nGenerating embeddings: 100%|██████████| 1029/1029 [00:07<00:00, 137.79it/s]\nEvaluating queries:  18%|█▊        | 354/2003 [00:26<02:05, 13.09it/s]ERROR:root:Error processing query ID 99b7d66f-4bce-4a3e-af68-a021c6c3dcc3: unsupported operand type(s) for *: 'NoneType' and 'float'\nEvaluating queries: 100%|██████████| 2003/2003 [02:29<00:00, 13.37it/s]\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "ING finetuned\nfinetuned Hit Rate: 0.4515484515484515\n"
        }
      ],
      "execution_count": 22,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714306357409
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"../EXPERIMENT2_KCBERT_generated_QAdata/exp_finetune\"\n",
        "model = SentenceTransformer(model_id)\n",
        "customevaluate_st(val_dataset, model, output_folder,name='Bert_KR_ST')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Batches: 100%|██████████| 63/63 [00:01<00:00, 52.78it/s]\nCorpus Chunks: 100%|██████████| 1/1 [00:05<00:00,  5.60s/it]\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "<class 'numpy.float64'>\n0.3513958932589646\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 23,
          "data": {
            "text/plain": "          0\n0  0.351396",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.351396</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 23,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714306376366
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Domain Adaptation"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, models\n",
        "\n",
        "model_name = '/home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/MLM_trained_model/kcbert-base_CyclicLRtriangular-2024-05-07_06-21-18'\n",
        "#klue-roberta-large_CyclicLRtriangular-2024-04-26_08-42-52'\n",
        "\n",
        "transformer_model = models.Transformer(model_name)\n",
        "\n",
        "# Pooling layer 추가\n",
        "pooling_model = models.Pooling(transformer_model.get_word_embedding_dimension(),\n",
        "                            pooling_mode_mean_tokens=True,\n",
        "                            pooling_mode_cls_token=False,\n",
        "                            pooling_mode_max_tokens=False)\n",
        "\n",
        "# SentenceTransformer에 word_embedding_model과 pooling_model을 추가하여 모델 구성\n",
        "OURS_DA = SentenceTransformer(modules=[transformer_model, pooling_model])\n",
        "\n",
        "val_results_finetuned = evaluate(val_dataset, OURS_DA)\n",
        "print(\"ING finetuned\")\n",
        "df_OURS_DA = pd.DataFrame(val_results_finetuned)\n",
        "df_OURS_DA['model'] = 'Bert_KR_DA'\n",
        "print(\"finetuned Hit Rate:\", df_OURS_DA['is_hit'].mean())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Some weights of BertModel were not initialized from the model checkpoint at /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/MLM_trained_model/kcbert-base_CyclicLRtriangular-2024-05-07_06-21-18 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
        }
      ],
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1715055557963
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, models\n",
        "\n",
        "model_name = '/home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/MLM_trained_model/kcbert-base_CyclicLRtriangular-2024-05-01_16-01-15'\n",
        "#klue-roberta-large_CyclicLRtriangular-2024-04-26_08-42-52'\n",
        "\n",
        "transformer_model = models.Transformer(model_name)\n",
        "\n",
        "# Pooling layer 추가\n",
        "pooling_model = models.Pooling(transformer_model.get_word_embedding_dimension(),\n",
        "                            pooling_mode_mean_tokens=True,\n",
        "                            pooling_mode_cls_token=False,\n",
        "                            pooling_mode_max_tokens=False)\n",
        "\n",
        "# SentenceTransformer에 word_embedding_model과 pooling_model을 추가하여 모델 구성\n",
        "OURS_DA = SentenceTransformer(modules=[transformer_model, pooling_model])\n",
        "\n",
        "val_results_finetuned = evaluate(val_dataset, OURS_DA)\n",
        "print(\"ING finetuned\")\n",
        "df_OURS_DA = pd.DataFrame(val_results_finetuned)\n",
        "df_OURS_DA['model'] = 'Bert_KR_DA'\n",
        "print(\"finetuned Hit Rate:\", df_OURS_DA['is_hit'].mean())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Some weights of BertModel were not initialized from the model checkpoint at /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/MLM_trained_model/kcbert-base_CyclicLRtriangular-2024-05-01_16-01-15 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "ING finetuned\nfinetuned Hit Rate: 0.14628057913130305\n"
        }
      ],
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714579606054
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. OURS : Domain Adaptation + sentence finetuned"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OURS_finetuned = \"local:../EXPERIMENT3_DOMAINKCBERT_generated_QAdata/kcbert-base_CyclicLRtriangular-2024-04-30/exp_finetune\" \n",
        "\n",
        "val_results_finetuned = evaluate(val_dataset, OURS_finetuned)\n",
        "print(\"ING finetuned\")\n",
        "df_OURS_DA_ST = pd.DataFrame(val_results_finetuned)\n",
        "df_OURS_DA_ST['model'] = 'Bert_KR_DA_ST'\n",
        "print(\"finetuned Hit Rate:\", df_OURS_DA_ST['is_hit'].mean())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/tmp/ipykernel_17470/504469346.py:8: DeprecationWarning: Call to deprecated function (or staticmethod) from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n  service_context = ServiceContext.from_defaults(embed_model=embed_model)\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nGenerating embeddings: 100%|██████████| 1029/1029 [00:07<00:00, 137.97it/s]\nEvaluating queries:  18%|█▊        | 354/2003 [00:26<02:01, 13.54it/s]ERROR:root:Error processing query ID 99b7d66f-4bce-4a3e-af68-a021c6c3dcc3: unsupported operand type(s) for *: 'NoneType' and 'float'\nEvaluating queries: 100%|██████████| 2003/2003 [02:28<00:00, 13.46it/s]\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "ING finetuned\nfinetuned Hit Rate: 0.43456543456543456\n"
        }
      ],
      "execution_count": 22,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714572075343
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"../EXPERIMENT3_DOMAINKCBERT_generated_QAdata/kcbert-base_CyclicLRtriangular-2024-04-29_08-12-32/exp_finetune\"\n",
        "model = SentenceTransformer(model_id)\n",
        "customevaluate_st(val_dataset, model, output_folder, name='OURS_DA_ST')"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../EXPERIMENT3_DOMAINKCBERT_generated_QAdata/kcbert-base_CyclicLRtriangular-2024-04-29_08-12-32/exp_finetune\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m customevaluate_st(val_dataset, model, output_folder, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOURS_DA_ST\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[0;32m/anaconda/envs/llm-rag-embeddings/lib/python3.8/site-packages/sentence_transformers/SentenceTransformer.py:197\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[0;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, cache_folder, trust_remote_code, revision, token, use_auth_token, truncate_dim)\u001b[0m\n\u001b[1;32m    194\u001b[0m         model_name_or_path \u001b[38;5;241m=\u001b[39m __MODEL_HUB_ORGANIZATION__ \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m model_name_or_path\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sentence_transformer_model(model_name_or_path, token, cache_folder\u001b[38;5;241m=\u001b[39mcache_folder, revision\u001b[38;5;241m=\u001b[39mrevision):\n\u001b[0;32m--> 197\u001b[0m     modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_sbert_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m     modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_auto_model(\n\u001b[1;32m    206\u001b[0m         model_name_or_path,\n\u001b[1;32m    207\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    210\u001b[0m         trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[1;32m    211\u001b[0m     )\n",
            "File \u001b[0;32m/anaconda/envs/llm-rag-embeddings/lib/python3.8/site-packages/sentence_transformers/SentenceTransformer.py:1296\u001b[0m, in \u001b[0;36mSentenceTransformer._load_sbert_model\u001b[0;34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code)\u001b[0m\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1295\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer_args\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m hub_kwargs\n\u001b[0;32m-> 1296\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1298\u001b[0m     \u001b[38;5;66;03m# Normalize does not require any files to be loaded\u001b[39;00m\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module_class \u001b[38;5;241m==\u001b[39m Normalize:\n",
            "File \u001b[0;32m/anaconda/envs/llm-rag-embeddings/lib/python3.8/site-packages/sentence_transformers/models/Transformer.py:36\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[0;34m(self, model_name_or_path, max_seq_length, model_args, cache_dir, tokenizer_args, do_lower_case, tokenizer_name_or_path)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_lower_case \u001b[38;5;241m=\u001b[39m do_lower_case\n\u001b[1;32m     35\u001b[0m config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args, cache_dir\u001b[38;5;241m=\u001b[39mcache_dir)\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     39\u001b[0m     tokenizer_name_or_path \u001b[38;5;28;01mif\u001b[39;00m tokenizer_name_or_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m model_name_or_path,\n\u001b[1;32m     40\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_args,\n\u001b[1;32m     42\u001b[0m )\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# No max_seq_length set. Try to infer from model\u001b[39;00m\n",
            "File \u001b[0;32m/anaconda/envs/llm-rag-embeddings/lib/python3.8/site-packages/sentence_transformers/models/Transformer.py:65\u001b[0m, in \u001b[0;36mTransformer._load_model\u001b[0;34m(self, model_name_or_path, config, cache_dir, **model_args)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_mt5_model(model_name_or_path, config, cache_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/llm-rag-embeddings/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m )\n",
            "File \u001b[0;32m/anaconda/envs/llm-rag-embeddings/lib/python3.8/site-packages/transformers/modeling_utils.py:3677\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3668\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3669\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3670\u001b[0m     (\n\u001b[1;32m   3671\u001b[0m         model,\n\u001b[1;32m   3672\u001b[0m         missing_keys,\n\u001b[1;32m   3673\u001b[0m         unexpected_keys,\n\u001b[1;32m   3674\u001b[0m         mismatched_keys,\n\u001b[1;32m   3675\u001b[0m         offload_index,\n\u001b[1;32m   3676\u001b[0m         error_msgs,\n\u001b[0;32m-> 3677\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3679\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3680\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3681\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3683\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3684\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3685\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3686\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3687\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3688\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3689\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3692\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3693\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3695\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   3696\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
            "File \u001b[0;32m/anaconda/envs/llm-rag-embeddings/lib/python3.8/site-packages/transformers/modeling_utils.py:4050\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   4040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4041\u001b[0m     \u001b[38;5;66;03m# Whole checkpoint\u001b[39;00m\n\u001b[1;32m   4042\u001b[0m     mismatched_keys \u001b[38;5;241m=\u001b[39m _find_mismatched_keys(\n\u001b[1;32m   4043\u001b[0m         state_dict,\n\u001b[1;32m   4044\u001b[0m         model_state_dict,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4048\u001b[0m         ignore_mismatched_sizes,\n\u001b[1;32m   4049\u001b[0m     )\n\u001b[0;32m-> 4050\u001b[0m     error_msgs \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4051\u001b[0m     offload_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4052\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4053\u001b[0m     \u001b[38;5;66;03m# Sharded checkpoint or whole but low_cpu_mem_usage==True\u001b[39;00m\n\u001b[1;32m   4054\u001b[0m \n\u001b[1;32m   4055\u001b[0m     \u001b[38;5;66;03m# This should always be a list but, just to be sure.\u001b[39;00m\n",
            "File \u001b[0;32m/anaconda/envs/llm-rag-embeddings/lib/python3.8/site-packages/transformers/modeling_utils.py:702\u001b[0m, in \u001b[0;36m_load_state_dict_into_model\u001b[0;34m(model_to_load, state_dict, start_prefix)\u001b[0m\n\u001b[1;32m    699\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    700\u001b[0m             load(child, state_dict, prefix \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 702\u001b[0m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_prefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;66;03m# Delete `state_dict` so it could be collected by GC earlier. Note that `state_dict` is a copy of the argument, so\u001b[39;00m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;66;03m# it's safe to delete it.\u001b[39;00m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m state_dict\n",
            "File \u001b[0;32m/anaconda/envs/llm-rag-embeddings/lib/python3.8/site-packages/transformers/modeling_utils.py:700\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 700\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/llm-rag-embeddings/lib/python3.8/site-packages/transformers/modeling_utils.py:700\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 700\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "    \u001b[0;31m[... skipping similar frames: _load_state_dict_into_model.<locals>.load at line 700 (3 times)]\u001b[0m\n",
            "File \u001b[0;32m/anaconda/envs/llm-rag-embeddings/lib/python3.8/site-packages/transformers/modeling_utils.py:700\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 700\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/llm-rag-embeddings/lib/python3.8/site-packages/transformers/modeling_utils.py:696\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    694\u001b[0m                     module\u001b[38;5;241m.\u001b[39m_load_from_state_dict(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    695\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 696\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_from_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[0;32m/anaconda/envs/llm-rag-embeddings/lib/python3.8/site-packages/torch/nn/modules/module.py:2040\u001b[0m, in \u001b[0;36mModule._load_from_state_dict\u001b[0;34m(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\u001b[0m\n\u001b[1;32m   2038\u001b[0m                 \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, input_param)\n\u001b[1;32m   2039\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2040\u001b[0m             \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_param\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2041\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m   2042\u001b[0m     error_msgs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhile copying the parameter named \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2043\u001b[0m                       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhose dimensions in the model are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2044\u001b[0m                       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhose dimensions in the checkpoint are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_param\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2045\u001b[0m                       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124man exception occurred : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mex\u001b[38;5;241m.\u001b[39margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2046\u001b[0m                       )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714382391887
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SUMMARY"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_all = pd.concat([df_ada, df_bge, df_bge_finetuned, df_bert, df_bert_ST, df_kcbert, df_kcbert_ST, df_OURS_DA_ST])\n",
        "df_all.groupby('model').mean('is_hit')"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 28,
          "data": {
            "text/plain": "              is_hit\nmodel               \nOURS_DA_ST  0.240759\nada         0.511489\nbert        0.004995\nbert_ST     0.239261\nbge         0.004995\nbge_ST      0.092907\nkcbert      0.004995\nkcbert_ST   0.451548",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>is_hit</th>\n    </tr>\n    <tr>\n      <th>model</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>OURS_DA_ST</th>\n      <td>0.240759</td>\n    </tr>\n    <tr>\n      <th>ada</th>\n      <td>0.511489</td>\n    </tr>\n    <tr>\n      <th>bert</th>\n      <td>0.004995</td>\n    </tr>\n    <tr>\n      <th>bert_ST</th>\n      <td>0.239261</td>\n    </tr>\n    <tr>\n      <th>bge</th>\n      <td>0.004995</td>\n    </tr>\n    <tr>\n      <th>bge_ST</th>\n      <td>0.092907</td>\n    </tr>\n    <tr>\n      <th>kcbert</th>\n      <td>0.004995</td>\n    </tr>\n    <tr>\n      <th>kcbert_ST</th>\n      <td>0.451548</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 28,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714307605716
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### InformationRetrievalEvaluator"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 결과를 저장할 디렉토리\n",
        "results_dir = '/home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/EVAL_results'\n",
        "\n",
        "# 디렉토리가 없으면 생성\n",
        "if not os.path.exists(results_dir):\n",
        "    os.makedirs(results_dir)\n",
        "\n",
        "# 각 데이터프레임을 CSV 파일로 저장\n",
        "df_ada.to_csv(os.path.join(results_dir, 'ada_results.csv'))\n",
        "df_bge.to_csv(os.path.join(results_dir, 'bge_results.csv'))\n",
        "df_bge_finetuned.to_csv(os.path.join(results_dir, 'bge_ST_results.csv'))\n",
        "df_bert.to_csv(os.path.join(results_dir, 'bert_results.csv'))\n",
        "df_bert_ST.to_csv(os.path.join(results_dir, 'bert_ST_results.csv'))\n",
        "df_kcbert.to_csv(os.path.join(results_dir, 'kcbert_results.csv'))\n",
        "df_kcbert_ST.to_csv(os.path.join(results_dir, 'kcbert_ST_results.csv'))\n",
        "df_OURS_DA_ST.to_csv(os.path.join(results_dir, 'OURS_DA_ST_results.csv'))"
      ],
      "outputs": [],
      "execution_count": 30,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714308358622
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# InformationRetrievalEvaluator"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_OURS_DA_ST"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 37,
          "data": {
            "text/plain": "      is_hit                                          retrieved  \\\n0          1  [b6cafda1-526b-4309-82cc-b4878195ccfc, 9790f8e...   \n1          0  [c9a32263-4134-4405-979a-e1aee11831cf, ddf3c8d...   \n2          0  [9dbc6e1e-6843-459b-9f91-ceb036fc8ccc, 66c1325...   \n3          0  [66c1325b-bc54-4615-ad8d-af6e8feefd14, 918a11a...   \n4          0  [cf7061ba-6b77-4b2e-a96e-9e6f752d9bd1, dade89a...   \n...      ...                                                ...   \n1997       0  [215b9ff0-2b90-4350-8cda-7a2af4de9bb9, c5da4e9...   \n1998       0  [029b6e20-44ff-4f3a-a005-2f41ede88e34, c2c941d...   \n1999       0  [7922816a-bb11-4569-b77f-f1eb89ed8929, f2df720...   \n2000       0  [24e3c14f-de1d-4f8c-bc7c-454d6825e41c, ded3119...   \n2001       0  [b532ea4b-56f3-4ba7-b588-88b3421bc710, 45266b6...   \n\n                                  expected  \\\n0     b6cafda1-526b-4309-82cc-b4878195ccfc   \n1     b6cafda1-526b-4309-82cc-b4878195ccfc   \n2     0fa29f16-ed5e-4572-a829-f59d7bcc675e   \n3     0fa29f16-ed5e-4572-a829-f59d7bcc675e   \n4     66c1325b-bc54-4615-ad8d-af6e8feefd14   \n...                                    ...   \n1997  ea0cb6c3-bca6-4818-a4df-eaa4fc95881d   \n1998  ea0cb6c3-bca6-4818-a4df-eaa4fc95881d   \n1999  ea5f2696-ca05-4bf3-a07a-dd42da7186ad   \n2000  ea5f2696-ca05-4bf3-a07a-dd42da7186ad   \n2001  d8d46c01-b68c-4af5-92c5-59f7d84b0735   \n\n                                     query       model  \n0     b51fe0a3-e40c-4e29-a855-3d5fee362245  OURS_DA_ST  \n1     66bc7be1-1c44-46da-b054-77e454e2981e  OURS_DA_ST  \n2     f66dfa34-e2ab-4f54-bf57-d765cdaa11d7  OURS_DA_ST  \n3     8f3643f4-a2e9-4b6b-b1ff-aa6d02065a85  OURS_DA_ST  \n4     acd9f2a6-9e59-4dd1-b5ca-370d2ecee8d7  OURS_DA_ST  \n...                                    ...         ...  \n1997  f8790853-e00d-46c9-ac52-17fa9530433c  OURS_DA_ST  \n1998  2f658ded-9416-45c3-aa0a-f82c765331ca  OURS_DA_ST  \n1999  a82d807f-0d46-4eb6-9790-8a563e5918b6  OURS_DA_ST  \n2000  d487a4fe-978c-4c5c-a0cb-18a9003f9b26  OURS_DA_ST  \n2001  53f06e9b-5761-4202-b3c3-f29223aa0c05  OURS_DA_ST  \n\n[2002 rows x 5 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>is_hit</th>\n      <th>retrieved</th>\n      <th>expected</th>\n      <th>query</th>\n      <th>model</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>[b6cafda1-526b-4309-82cc-b4878195ccfc, 9790f8e...</td>\n      <td>b6cafda1-526b-4309-82cc-b4878195ccfc</td>\n      <td>b51fe0a3-e40c-4e29-a855-3d5fee362245</td>\n      <td>OURS_DA_ST</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>[c9a32263-4134-4405-979a-e1aee11831cf, ddf3c8d...</td>\n      <td>b6cafda1-526b-4309-82cc-b4878195ccfc</td>\n      <td>66bc7be1-1c44-46da-b054-77e454e2981e</td>\n      <td>OURS_DA_ST</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>[9dbc6e1e-6843-459b-9f91-ceb036fc8ccc, 66c1325...</td>\n      <td>0fa29f16-ed5e-4572-a829-f59d7bcc675e</td>\n      <td>f66dfa34-e2ab-4f54-bf57-d765cdaa11d7</td>\n      <td>OURS_DA_ST</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>[66c1325b-bc54-4615-ad8d-af6e8feefd14, 918a11a...</td>\n      <td>0fa29f16-ed5e-4572-a829-f59d7bcc675e</td>\n      <td>8f3643f4-a2e9-4b6b-b1ff-aa6d02065a85</td>\n      <td>OURS_DA_ST</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>[cf7061ba-6b77-4b2e-a96e-9e6f752d9bd1, dade89a...</td>\n      <td>66c1325b-bc54-4615-ad8d-af6e8feefd14</td>\n      <td>acd9f2a6-9e59-4dd1-b5ca-370d2ecee8d7</td>\n      <td>OURS_DA_ST</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1997</th>\n      <td>0</td>\n      <td>[215b9ff0-2b90-4350-8cda-7a2af4de9bb9, c5da4e9...</td>\n      <td>ea0cb6c3-bca6-4818-a4df-eaa4fc95881d</td>\n      <td>f8790853-e00d-46c9-ac52-17fa9530433c</td>\n      <td>OURS_DA_ST</td>\n    </tr>\n    <tr>\n      <th>1998</th>\n      <td>0</td>\n      <td>[029b6e20-44ff-4f3a-a005-2f41ede88e34, c2c941d...</td>\n      <td>ea0cb6c3-bca6-4818-a4df-eaa4fc95881d</td>\n      <td>2f658ded-9416-45c3-aa0a-f82c765331ca</td>\n      <td>OURS_DA_ST</td>\n    </tr>\n    <tr>\n      <th>1999</th>\n      <td>0</td>\n      <td>[7922816a-bb11-4569-b77f-f1eb89ed8929, f2df720...</td>\n      <td>ea5f2696-ca05-4bf3-a07a-dd42da7186ad</td>\n      <td>a82d807f-0d46-4eb6-9790-8a563e5918b6</td>\n      <td>OURS_DA_ST</td>\n    </tr>\n    <tr>\n      <th>2000</th>\n      <td>0</td>\n      <td>[24e3c14f-de1d-4f8c-bc7c-454d6825e41c, ded3119...</td>\n      <td>ea5f2696-ca05-4bf3-a07a-dd42da7186ad</td>\n      <td>d487a4fe-978c-4c5c-a0cb-18a9003f9b26</td>\n      <td>OURS_DA_ST</td>\n    </tr>\n    <tr>\n      <th>2001</th>\n      <td>0</td>\n      <td>[b532ea4b-56f3-4ba7-b588-88b3421bc710, 45266b6...</td>\n      <td>d8d46c01-b68c-4af5-92c5-59f7d84b0735</td>\n      <td>53f06e9b-5761-4202-b3c3-f29223aa0c05</td>\n      <td>OURS_DA_ST</td>\n    </tr>\n  </tbody>\n</table>\n<p>2002 rows × 5 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 37,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714309969287
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df_st_ada= pd.read_csv('EVAL_results/Information-Retrieval_evaluation_ada_results.csv')\n",
        "df_st_bge= pd.read_csv('/home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/EVAL_results/bge_evaluation_results.csv')\n",
        "df_st_bge_ST= pd.read_csv('/home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/EVAL_results/bge_ST_evaluation_results.csv')\n",
        "df_st_bert_ST= pd.read_csv('/home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/EVAL_results/bert_ST_evaluation_results.csv')\n",
        "df_st_kcbert_ST= pd.read_csv('/home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/EVAL_results/kcbert_ST_evaluation_results.csv')\n",
        "df_st_OURS_DA_ST= pd.read_csv('/home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/EVAL_results/OURS_DA_ST_evaluation_results.csv')"
      ],
      "outputs": [],
      "execution_count": 34,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714308763906
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_st_all = pd.concat([df_st_bge, df_st_bge_ST, df_st_bert_ST, df_st_kcbert_ST, df_st_OURS_DA_ST])\n",
        "df_st_all = df_st_all.set_index('model')\n",
        "df_st_all"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"None of ['model'] are in the columns\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_8565/3610379573.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_st_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_st_bge\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_st_bge_ST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_st_bert_ST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_st_kcbert_ST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_st_OURS_DA_ST\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf_st_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_st_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf_st_all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/llm-rag-embeddings/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, keys, drop, append, inplace, verify_integrity)\u001b[0m\n\u001b[1;32m   5855\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5856\u001b[0m                         \u001b[0mmissing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5858\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5859\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of {missing} are in the columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5861\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5862\u001b[0m             \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"None of ['model'] are in the columns\""
          ]
        }
      ],
      "execution_count": 35,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714308770841
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip list"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Package                                 Version\n--------------------------------------- ------------\nadal                                    1.2.7\naiohttp                                 3.9.5\naiosignal                               1.3.1\nalembic                                 1.13.1\naniso8601                               9.0.1\nannotated-types                         0.6.0\nanyio                                   4.3.0\napplicationinsights                     0.11.10\nargcomplete                             3.3.0\nasttokens                               2.4.1\nasync-timeout                           4.0.3\nattrs                                   23.2.0\nazure-ai-formrecognizer                 3.3.1\nazure-ai-ml                             1.15.0\nazure-common                            1.1.28\nazure-core                              1.30.1\nazure-graphrbac                         0.61.1\nazure-identity                          1.12.0\nazure-keyvault-secrets                  4.6.0\nazure-mgmt-authorization                4.0.0\nazure-mgmt-cognitiveservices            13.4.0\nazure-mgmt-containerregistry            10.3.0\nazure-mgmt-core                         1.4.0\nazure-mgmt-keyvault                     10.3.0\nazure-mgmt-network                      25.2.0\nazure-mgmt-resource                     23.0.1\nazure-mgmt-storage                      21.1.0\nazure-search-documents                  11.4.0b8\nazure-storage-blob                      12.19.0\nazure-storage-file-datalake             12.14.0\nazure-storage-file-share                12.15.0\nazureml-contrib-services                1.55.0\nazureml-core                            1.55.0.post2\nazureml-dataprep                        5.1.6\nazureml-dataprep-native                 41.0.0\nazureml-dataprep-rslex                  2.22.2\nazureml-dataset-runtime                 1.55.0\nazureml-fsspec                          1.3.0\nazureml-inference-server-http           0.7.6\nazureml-mlflow                          1.55.0\nazureml-rag                             0.2.28\nazureml-telemetry                       1.55.0\nbackcall                                0.2.0\nbackoff                                 2.2.1\nbackports.tempfile                      1.0\nbackports.weakref                       1.0.post1\nbcrypt                                  4.1.2\nbeautifulsoup4                          4.12.3\ncachetools                              5.3.3\ncertifi                                 2024.2.2\ncffi                                    1.16.0\nchardet                                 5.2.0\ncharset-normalizer                      3.3.2\nclick                                   8.1.7\ncloudpickle                             2.2.1\ncolorama                                0.4.6\ncomm                                    0.2.2\ncontextlib2                             21.6.0\ncontourpy                               1.1.1\ncryptography                            42.0.5\ncycler                                  0.12.1\ndatabricks-cli                          0.18.0\ndataclasses-json                        0.5.14\ndatasets                                2.19.0\ndebugpy                                 1.8.1\ndecorator                               5.1.1\nDeprecated                              1.2.14\ndill                                    0.3.6\ndirtyjson                               1.0.8\ndistro                                  1.9.0\ndnspython                               2.6.1\ndocker                                  7.0.0\nemoji                                   2.11.1\nentrypoints                             0.4\nenvirons                                9.5.0\nexceptiongroup                          1.2.1\nexecuting                               2.0.1\nfaiss-cpu                               1.7.4\nfilelock                                3.13.4\nfiletype                                1.2.0\nFlask                                   2.1.3\nFlask-Cors                              3.0.10\nfonttools                               4.51.0\nfrozenlist                              1.4.1\nfsspec                                  2023.9.2\ngitdb                                   4.0.11\nGitPython                               3.1.43\ngoogle-api-core                         2.18.0\ngoogle-auth                             2.29.0\ngoogleapis-common-protos                1.63.0\ngraphene                                3.3\ngraphql-core                            3.2.3\ngraphql-relay                           3.2.0\ngreenlet                                3.0.3\ngrpcio                                  1.56.0\ngunicorn                                20.1.0\nh11                                     0.14.0\nhttpcore                                1.0.5\nhttpx                                   0.27.0\nhuggingface-hub                         0.22.2\nhumanfriendly                           10.0\nidna                                    3.7\nimportlib-metadata                      6.11.0\nimportlib_resources                     6.4.0\ninference-schema                        1.4.0\nipykernel                               6.29.4\nipython                                 8.12.3\nisodate                                 0.6.1\nitsdangerous                            2.2.0\njedi                                    0.19.1\njeepney                                 0.8.0\nJinja2                                  3.1.3\njmespath                                1.0.1\njoblib                                  1.4.0\njsonpatch                               1.33\njsonpath-python                         1.0.6\njsonpickle                              3.0.4\njsonpointer                             2.4\njsonschema                              4.21.1\njsonschema-specifications               2023.12.1\njupyter_client                          8.6.1\njupyter_core                            5.7.2\nkiwisolver                              1.4.5\nknack                                   0.11.0\nlangchain                               0.0.266\nlangchain-community                     0.0.34\nlangchain-core                          0.1.45\nlangchain-text-splitters                0.0.1\nlangdetect                              1.0.9\nlangsmith                               0.0.92\nllama-index                             0.8.5.post2\nllama-index-agent-openai                0.2.3\nllama-index-cli                         0.1.12\nllama-index-core                        0.10.30\nllama-index-embeddings-huggingface      0.2.0\nllama-index-embeddings-openai           0.1.8\nllama-index-indices-managed-llama-cloud 0.1.5\nllama-index-legacy                      0.9.48\nllama-index-llms-openai                 0.1.16\nllama-index-multi-modal-llms-openai     0.1.5\nllama-index-program-openai              0.1.5\nllama-index-question-gen-openai         0.1.3\nllama-index-readers-file                0.1.19\nllama-index-readers-llama-parse         0.1.4\nllama-parse                             0.4.1\nllamaindex-py-client                    0.1.18\nloguru                                  0.7.2\nlxml                                    4.9.4\nMako                                    1.3.3\nMarkdown                                3.6\nMarkupSafe                              2.1.5\nmarshmallow                             3.21.1\nmatplotlib                              3.7.5\nmatplotlib-inline                       0.1.7\nminijinja                               1.0.16\nmlflow                                  2.12.1\nmlflow-skinny                           2.3.2\nmmh3                                    4.1.0\nmpmath                                  1.3.0\nmsal                                    1.22.0\nmsal-extensions                         1.1.0\nmsrest                                  0.7.1\nmsrestazure                             0.6.4\nmultidict                               6.0.5\nmultiprocess                            0.70.14\nmypy-extensions                         1.0.0\nndg-httpsclient                         0.5.1\nnest-asyncio                            1.6.0\nnetworkx                                3.1\nnltk                                    3.8.1\nnumexpr                                 2.8.6\nnumpy                                   1.23.5\nnvidia-cublas-cu12                      12.1.3.1\nnvidia-cuda-cupti-cu12                  12.1.105\nnvidia-cuda-nvrtc-cu12                  12.1.105\nnvidia-cuda-runtime-cu12                12.1.105\nnvidia-cudnn-cu12                       8.9.2.26\nnvidia-cufft-cu12                       11.0.2.54\nnvidia-curand-cu12                      10.3.2.106\nnvidia-cusolver-cu12                    11.4.5.107\nnvidia-cusparse-cu12                    12.1.0.106\nnvidia-nccl-cu12                        2.19.3\nnvidia-nvjitlink-cu12                   12.4.127\nnvidia-nvtx-cu12                        12.1.105\noauthlib                                3.2.2\nopenai                                  1.23.3\nopenapi-schema-pydantic                 1.2.4\nopencensus                              0.11.4\nopencensus-context                      0.1.3\nopencensus-ext-azure                    1.1.13\nopencensus-ext-logging                  0.1.1\norjson                                  3.10.1\npackaging                               23.2\npandas                                  2.0.3\nparamiko                                3.4.0\nparso                                   0.8.4\npathspec                                0.12.1\npexpect                                 4.9.0\npickleshare                             0.7.5\npillow                                  10.3.0\npinecone-client                         2.2.4\npip                                     24.0\npkginfo                                 1.10.0\npkgutil_resolve_name                    1.3.10\nplatformdirs                            4.2.0\npolling2                                0.5.0\nportalocker                             2.8.2\nprompt-toolkit                          3.0.43\nproto-plus                              1.23.0\nprotobuf                                4.25.3\npsutil                                  5.8.0\nptyprocess                              0.7.0\npure-eval                               0.2.2\npyarrow                                 15.0.2\npyarrow-hotfix                          0.6\npyasn1                                  0.6.0\npyasn1_modules                          0.4.0\npycparser                               2.22\npydantic                                1.10.9\npydantic_core                           2.18.2\npydash                                  8.0.0\nPygments                                2.17.2\nPyJWT                                   2.8.0\npymilvus                                2.3.0\npymongo                                 4.6.3\npymssql                                 2.2.7\nPyNaCl                                  1.5.0\npyOpenSSL                               24.1.0\npyparsing                               3.1.2\npypdf                                   4.2.0\nPySocks                                 1.7.1\npython-dateutil                         2.9.0.post0\npython-dotenv                           1.0.1\npython-iso639                           2024.2.7\npython-magic                            0.4.27\npytz                                    2023.4\nPyYAML                                  6.0.1\npyzmq                                   26.0.2\nquerystring-parser                      1.2.4\nrapidfuzz                               3.8.1\nreferencing                             0.34.0\nregex                                   2024.4.16\nrequests                                2.31.0\nrequests-oauthlib                       2.0.0\nresponses                               0.18.0\nrpds-py                                 0.18.0\nrsa                                     4.9\nsafetensors                             0.4.3\nscikit-learn                            1.3.2\nscipy                                   1.10.1\nSecretStorage                           3.3.3\nsentence-transformers                   2.7.0\nsentencepiece                           0.2.0\nsetuptools                              69.5.1\nsix                                     1.16.0\nsmmap                                   5.0.1\nsniffio                                 1.3.1\nsoupsieve                               2.5\nSQLAlchemy                              2.0.29\nsqlparse                                0.5.0\nstack-data                              0.6.3\nstrictyaml                              1.7.3\nstriprtf                                0.0.26\nsympy                                   1.12\ntabulate                                0.9.0\ntenacity                                8.2.3\nthreadpoolctl                           3.4.0\ntika                                    2.6.0\ntiktoken                                0.5.2\ntokenizers                              0.19.1\ntorch                                   2.2.2\ntorchvision                             0.17.2\ntornado                                 6.4\ntqdm                                    4.66.2\ntraitlets                               5.14.3\ntransformers                            4.40.0\ntriton                                  2.2.0\ntyping_extensions                       4.11.0\ntyping-inspect                          0.9.0\ntzdata                                  2024.1\nujson                                   5.9.0\nunstructured                            0.11.8\nunstructured-client                     0.21.0\nurllib3                                 1.26.18\nwcwidth                                 0.2.13\nWerkzeug                                3.0.2\nwheel                                   0.43.0\nwrapt                                   1.12.1\nxxhash                                  3.4.1\nyarl                                    1.9.4\nzipp                                    3.18.1\nNote: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1715062694808
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "llm-rag-embeddings",
      "language": "python",
      "display_name": "genai"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.19",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "llm-rag-embeddings"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}