{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "S-BERT 파인튜닝 => domain adap 하기 전\n",
        "# KCBERT-V2023 S-BERT 파인튜닝"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer, InputExample, losses, models, datasets, evaluation\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sentence_transformers import InputExample, losses, evaluation\n",
        "\n",
        "\n",
        "\n",
        "# Set environment variables\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "os.environ[\"TORCH_USE_CUDA_DSA\"] = '1'\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
        "\n",
        "# Define paths to your dataset\n",
        "TRAIN_DATASET_FPATH = '/home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/0529new_QA/train_dataset.json'\n",
        "VAL_DATASET_FPATH = '/home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/0529new_QA/val_dataset.json'\n",
        "OUTPUT_DIR = '/home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/Sbert_KCBERT2023/exp_finetune_onNEWQA'\n",
        "\n",
        "# Load datasets\n",
        "with open(TRAIN_DATASET_FPATH, 'r') as f:\n",
        "    train_dataset = json.load(f)\n",
        "with open(VAL_DATASET_FPATH, 'r') as f:\n",
        "    val_dataset = json.load(f)\n",
        "\n",
        "train_corpus = train_dataset['corpus']\n",
        "train_queries = train_dataset['queries']\n",
        "train_relevant_docs = train_dataset['relevant_docs']\n",
        "val_corpus = val_dataset['corpus']\n",
        "val_queries = val_dataset['queries']\n",
        "val_relevant_docs = val_dataset['relevant_docs']\n",
        "\n",
        "# Define your model and tokenizer\n",
        "model_name = \"beomi/KcBERT-v2023\"\n",
        "# AutoModel과 AutoTokenizer를 사용하여 sentence-transformers의 형태로 모델을 래핑한 다음 저장\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "# Define SentenceTransformer model\n",
        "# word_embedding_model = models.Transformer(model_name)\n",
        "# pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), pooling_mode_mean_tokens=True)\n",
        "# model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Some weights of RobertaModel were not initialized from the model checkpoint at beomi/KcBERT-v2023 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "RobertaModel(\n  (embeddings): RobertaEmbeddings(\n    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n    (position_embeddings): Embedding(514, 768, padding_idx=1)\n    (token_type_embeddings): Embedding(1, 768)\n    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): RobertaEncoder(\n    (layer): ModuleList(\n      (0-11): 12 x RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (pooler): RobertaPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n  )\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717299511582
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define mean pooling function\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0]\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
        "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "    return sum_embeddings / sum_mask\n",
        "\n",
        "# Define function to encode texts\n",
        "def encode_texts(texts, model, tokenizer, device, max_length=512):\n",
        "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=max_length).to(device)\n",
        "    outputs = model(**inputs)\n",
        "    return mean_pooling(outputs, inputs['attention_mask']).to(device)\n",
        "\n",
        "\n",
        "# Prepare training examples\n",
        "train_examples = []\n",
        "for query_id, queries in train_queries.items():\n",
        "    if query_id in train_relevant_docs:\n",
        "        for onequery in queries:\n",
        "            for doc_id in train_relevant_docs[query_id]:\n",
        "                if doc_id in train_corpus:\n",
        "                    doc_text = train_corpus[doc_id]\n",
        "                    if isinstance(doc_text, str) and isinstance(onequery, str):\n",
        "                        train_examples.append(InputExample(texts=[onequery, doc_text], label=1.0))\n",
        "                    else:\n",
        "                        print(f\"Warning: query or doc_text is not a string. Query: {onequery}, Doc: {doc_text}\")\n",
        "    else:\n",
        "        print(f\"Warning: query_id '{query_id}' not found in train_relevant_docs\")\n",
        "\n",
        "\n",
        "val_examples = []\n",
        "for query_id, queries in val_queries.items():\n",
        "    if query_id in val_relevant_docs:\n",
        "        for onequery in queries:\n",
        "            \n",
        "            for doc_id in val_relevant_docs[query_id]:\n",
        "                if doc_id in val_corpus:\n",
        "                    doc_text = val_corpus[doc_id]\n",
        "\n",
        "                    if isinstance(doc_text, str) and isinstance(onequery, str):\n",
        "                        val_examples.append(InputExample(texts=[onequery, doc_text], label=1.0))\n",
        "                    else:\n",
        "                        print(f\"Warning: query or doc_text is not a string. Query: {onequery}, Doc: {doc_text}\")\n",
        "            \n",
        "    else:\n",
        "        print(f\"Warning: query_id '{query_id}' not found in val_relevant_docs\")\n",
        "\n",
        "        \n",
        "# Custom collate function for DataLoader\n",
        "def custom_collate(batch):\n",
        "    texts1, texts2, labels = zip(*[(example.texts[0], example.texts[1], example.label) for example in batch])\n",
        "    return list(texts1), list(texts2), torch.tensor(labels, dtype=torch.float)\n",
        "\n",
        "# Convert InputExamples to Dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, examples):\n",
        "        self.examples = examples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.examples[idx]\n",
        "\n",
        "train_dataset = TextDataset(train_examples)\n",
        "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=4, collate_fn=custom_collate)\n",
        "\n",
        "# Define Cosine Similarity Loss\n",
        "class CosineSimilarityLoss(torch.nn.Module):\n",
        "    def __init__(self, model, tokenizer, device):\n",
        "        super(CosineSimilarityLoss, self).__init__()\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = device\n",
        "        self.cosine_similarity = torch.nn.CosineSimilarity(dim=1)\n",
        "\n",
        "    def forward(self, texts1, texts2, labels):\n",
        "        \"\"\"\n",
        "        texts는 [onequery, doc_text] 형태의 리스트\n",
        "        \"\"\"\n",
        "        embeddings1 = encode_texts(texts1, self.model, self.tokenizer, self.device)\n",
        "        embeddings2 = encode_texts(texts2, self.model, self.tokenizer, self.device)\n",
        "        similarities = self.cosine_similarity(embeddings1, embeddings2)\n",
        "        return torch.nn.functional.mse_loss(similarities, labels)\n",
        "\n",
        "# Initialize loss\n",
        "train_loss = CosineSimilarityLoss(model, tokenizer, device)\n",
        "\n",
        "# Training function\n",
        "def train_model(model, dataloader, loss_fn, optimizer, epochs):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch in dataloader:\n",
        "            texts1, texts2, labels = batch\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            loss = loss_fn(texts1, texts2, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(dataloader)}\")\n",
        "    return model\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "# Train the model\n",
        "model = train_model(model, train_dataloader, train_loss, optimizer, epochs=12)\n",
        "\n",
        "# Save the model\n",
        "model.save_pretrained(OUTPUT_DIR)\n",
        "#model.save(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "# Evaluation\n",
        "evaluator = evaluation.EmbeddingSimilarityEvaluator.from_input_examples(val_examples, name='val-evaluator')\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1/12, Loss: 0.0001007616694575364\nEpoch 2/12, Loss: 1.405664930161307e-06\nEpoch 3/12, Loss: 8.067726413350186e-07\nEpoch 4/12, Loss: 5.206104806019757e-07\nEpoch 6/12, Loss: 2.2335395309888968e-07\nEpoch 7/12, Loss: 1.1469863654005527e-07\nEpoch 8/12, Loss: 3.87722350432929e-08\nEpoch 9/12, Loss: 1.5137988742839738e-08\nEpoch 10/12, Loss: 7.525394704733149e-09\nEpoch 12/12, Loss: 3.0099128149811864e-09\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717308722057
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "llm-rag-embeddings",
      "language": "python",
      "display_name": "genai"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.19",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "llm-rag-embeddings"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}