{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 폴더 내의 모든 txt 파일의 데이터를 모음\n",
        "path = 'data'  # 실제 경로\n",
        "output_path = 'data_split_by_dot'  # 분할된 파일을 저장할 경로\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "file_paths = [os.path.join(path, file) for file in os.listdir(path) if file.endswith('.txt')]\n",
        "\n",
        "# 각 파일의 데이터 로드 및 분할\n",
        "for file_path in file_paths:\n",
        "    all_paragraphs = []\n",
        "    file_name = os.path.basename(file_path)\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "        # 문장을 기준으로 텍스트를 분할\n",
        "        sentences = text.split('.')\n",
        "        # 각 문장을 다시 결합하여 너무 짧은 문장이 아닌 단락을 만듦\n",
        "        paragraph = \"\"\n",
        "        for sentence in sentences:\n",
        "            if len(paragraph) + len(sentence) > 300:  # 300은 원하는 최대 길이를 나타냄 -> token_size:300 limitation\n",
        "                if paragraph.strip():\n",
        "                    all_paragraphs.append(paragraph.strip())\n",
        "                paragraph = sentence + \".\"\n",
        "            else:\n",
        "                paragraph += sentence + \".\"\n",
        "        if paragraph.strip():  # 남아 있는 문장을 추가\n",
        "            all_paragraphs.append(paragraph.strip())\n",
        "    \n",
        "    # 분할된 단락들을 txt 파일로 저장\n",
        "    split_file_path = os.path.join(output_path, file_name)\n",
        "    with open(split_file_path, 'w', encoding='utf-8') as f:\n",
        "        for para in all_paragraphs:\n",
        "            if para:  # 빈 줄 제거\n",
        "                f.write(para + '\\n')\n",
        "\n",
        "print(\"분할된 단락들이 txt 파일로 저장되었습니다.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "a487b53f-3bc2-40a1-98e4-0eb550de2570"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 추가하는 데이터에 대해서도 동일 진행"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "8a98a523-ffed-4790-801d-a5cfd7132899"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 여러 경로를 리스트에 저장\n",
        "text_paths = [\n",
        "    \"/home/azureuser/cloudfiles/code/Users/hb.suh/(0522part)_번역원본\",\n",
        "    \"/home/azureuser/cloudfiles/code/Users/hb.suh/(0527part)_번역원본\",\n",
        "    \"/home/azureuser/cloudfiles/code/Users/hb.suh/(0524part)_번역원본\",\n",
        "    \"/home/azureuser/cloudfiles/code/Users/hb.suh/(0528part)_번역원본\",\n",
        "    \"/home/azureuser/cloudfiles/code/Users/hb.suh/(0529part)_번역원본\",\n",
        "    \n",
        "    \"/home/azureuser/cloudfiles/code/Users/hb.suh/(0522part)_OnlyTranslatedData/사출성형\",\n",
        "    \"/home/azureuser/cloudfiles/code/Users/hb.suh/(0524part)_OnlyTranslatedData\",\n",
        "    \"/home/azureuser/cloudfiles/code/Users/hb.suh/(0527part)_OnlyTranslatedData\",\n",
        "    \"/home/azureuser/cloudfiles/code/Users/hb.suh/(0528part)_OnlyTranslatedData\",\n",
        "    \"/home/azureuser/cloudfiles/code/Users/hb.suh/(0529part)_OnlyTranslatedData\"\n",
        "]\n",
        "\n",
        "output_path = '/home/azureuser/cloudfiles/code/Users/hb.suh/data_split_by_dot'  # 분할된 파일을 저장할 경로\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "main_content_texts = []\n",
        "\n",
        "# \"main_content\" 추출 및 분할\n",
        "for onepath in text_paths:\n",
        "    for file in os.listdir(onepath):\n",
        "        if file.endswith('.txt'):\n",
        "            with open(os.path.join(onepath, file), 'r', encoding='utf-8') as f:\n",
        "                lines = f.readlines()\n",
        "                main_content = \"\"\n",
        "                extract = False\n",
        "\n",
        "                for line in lines:\n",
        "                    # \"main_content:\" 또는 \"Main Content:\"로 시작\n",
        "                    if line.strip().lower().startswith(\"main_content:\") or line.strip().lower().startswith(\"main content:\"):\n",
        "                        if extract and main_content:\n",
        "                            # 이전에 추출된 main_content를 추가합니다.\n",
        "                            main_content_texts.append(main_content.strip())\n",
        "                            main_content = \"\"\n",
        "                        extract = True\n",
        "                        # \"main_content:\" 이후의 내용을 추출합니다.\n",
        "                        main_content = line.strip().split(\":\", 1)[1].strip()\n",
        "                    elif extract:\n",
        "                        if line.strip() == \"\":\n",
        "                            # 빈 줄이 나오면 추출을 중지하고 main_content를 추가합니다.\n",
        "                            main_content_texts.append(main_content.strip())\n",
        "                            extract = False\n",
        "                            main_content = \"\"\n",
        "                        else:\n",
        "                            # 이어지는 줄을 main_content에 추가합니다.\n",
        "                            main_content += \" \" + line.strip()\n",
        "\n",
        "                # 파일 끝에 도달했을 때 남아있는 main_content를 추가합니다.\n",
        "                if main_content:\n",
        "                    main_content_texts.append(main_content.strip())\n"
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717395601839
        }
      },
      "id": "a3f4df70-4cf0-4e71-a2b6-29c80e8d6afc"
    },
    {
      "cell_type": "code",
      "source": [
        "# 추출된 main_content 텍스트들을 \".\" 단위로 분할하여 저장\n",
        "for idx, content in enumerate(main_content_texts):\n",
        "    sentences = content.split('.')\n",
        "    all_paragraphs = []\n",
        "    paragraph = \"\"\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if len(paragraph) + len(sentence) > 300:  # 300은 원하는 최대 길이를 나타냄\n",
        "            if paragraph.strip():\n",
        "                all_paragraphs.append(paragraph.strip())\n",
        "            paragraph = sentence + \".\"\n",
        "        else:\n",
        "            paragraph += sentence + \".\"\n",
        "    if paragraph.strip():  # 남아 있는 문장을 추가\n",
        "        all_paragraphs.append(paragraph.strip())\n",
        "\n",
        "    split_file_path = os.path.join(output_path, f'main_content_{idx}.txt')\n",
        "    with open(split_file_path, 'w', encoding='utf-8') as f:\n",
        "        for para in all_paragraphs:\n",
        "            if para:  # 빈 줄 제거\n",
        "                f.write(para + '\\n')\n",
        "\n",
        "print(\"분할된 단락들이 txt 파일로 저장되었습니다.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "분할된 단락들이 txt 파일로 저장되었습니다.\n"
        }
      ],
      "execution_count": 19,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717395894890
        }
      },
      "id": "7c707e05-ad8f-4350-9561-28f4d2207064"
    },
    {
      "cell_type": "code",
      "source": [
        "#  논문 데이터 문장 단위로 split하신 데이터 \n",
        "scholar_path = \"/home/azureuser/cloudfiles/code/Users/hb.suh/data_split_by_dot\"  \n",
        "\n",
        "scolar_file_paths = [os.path.join(scholar_path, file) for file in os.listdir(scholar_path) if file.endswith('.txt')] #list\n",
        "\n",
        "len(scolar_file_paths)\n"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 20,
          "data": {
            "text/plain": "4652"
          },
          "metadata": {}
        }
      ],
      "execution_count": 20,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717395895873
        }
      },
      "id": "5940d350-d0e5-4a59-83c7-27862eefb274"
    },
    {
      "cell_type": "code",
      "source": [
        "#  탕칭 연구원님이 split하신 데이터 \n",
        "path = \"/home/azureuser/cloudfiles/code/Users/tangqing/src/OUR_BERT/CODE/사출성형/tokenizer_data_split_by_dot\"  \n",
        "\n",
        "all_texts = []\n",
        "\n",
        "# 폴더 내의 모든 txt 파일의 데이터를 모음\n",
        "for file in os.listdir(path):\n",
        "    if file.endswith('.txt'):\n",
        "        file_path = os.path.join(path, file)\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            all_texts.append(f.read())\n",
        "\n",
        "file_paths = [os.path.join(path, file) for file in os.listdir(path) if file.endswith('.txt')] #list\n",
        "\n",
        "len(file_paths)\n"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 21,
          "data": {
            "text/plain": "9"
          },
          "metadata": {}
        }
      ],
      "execution_count": 21,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717395896865
        }
      },
      "id": "62f06d21-7c71-4274-a99e-22177b0effd0"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "# JSON 파일 경로 설정\n",
        "json_paths = [\n",
        "    \"/home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/\"\n",
        "]\n",
        "\n",
        "output_path = \"/home/azureuser/cloudfiles/code/Users/hb.suh/gptdata_split_by_dot\"\n",
        "\n",
        "\n",
        "def split_and_save(text, base_filename, output_dir):\n",
        "    sentences = text.split('.')\n",
        "    all_paragraphs = []\n",
        "    paragraph = \"\"\n",
        "    for sentence in sentences:\n",
        "        if len(paragraph) + len(sentence) > 300:  # 300은 원하는 최대 길이를 나타냄\n",
        "            if paragraph.strip():\n",
        "                all_paragraphs.append(paragraph.strip())\n",
        "            paragraph = sentence + \".\"\n",
        "        else:\n",
        "            paragraph += sentence + \".\"\n",
        "    if paragraph.strip():  # 남아 있는 문장을 추가\n",
        "        all_paragraphs.append(paragraph.strip())\n",
        "    \n",
        "    # 분할된 단락들을 txt 파일로 저장\n",
        "    split_file_path = os.path.join(output_dir, base_filename)\n",
        "    with open(split_file_path, 'w', encoding='utf-8') as f:\n",
        "        for para in all_paragraphs:\n",
        "            if para:  # 빈 줄 제거\n",
        "                f.write(para + '\\n')\n",
        "\n",
        "# JSON 파일에서 [\"contents\"] 추출 및 분할하여 저장\n",
        "json_contents = []\n",
        "for namefolder in os.listdir(json_paths[0]):\n",
        "    folder_path = os.path.join(json_paths[0], namefolder)\n",
        "    print(f\"Processing folder: {folder_path}\")\n",
        "    if os.path.isdir(folder_path):\n",
        "        for file in os.listdir(folder_path):\n",
        "            if file.endswith('.json'):\n",
        "                json_path = os.path.join(folder_path, file)\n",
        "                print(f\"Processing file: {json_path}\")\n",
        "                try:\n",
        "                    with open(json_path, 'r', encoding='utf-8') as f:\n",
        "                        data = json.load(f)\n",
        "                        contents = data.get(\"contents\", [])\n",
        "                        if not contents:\n",
        "                            print(f\"No contents found in: {json_path}\")\n",
        "                        json_contents.extend(contents)\n",
        "                        # 각 contents 항목을 분할하여 저장\n",
        "                        for i, content in enumerate(contents):\n",
        "                            \n",
        "                            answer = content.get(\"answer\", \"\")\n",
        "                            \n",
        "                            if answer:\n",
        "                                split_and_save(answer, f\"{os.path.basename(file).replace('.json', '')}_answer_{i}.txt\", output_path)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing file {json_path}: {e}\")\n",
        "\n",
        "print(\"JSON 파일의 contents가 분할되어 txt 파일로 저장되었습니다.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Processing folder: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/Metal Injection Molding\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/Metal Injection Molding/QnA_gpt-4_english.json\nProcessing folder: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/Plastic Injection Molding\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/Plastic Injection Molding/Detail_QnA_1_gpt-4_english.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/Plastic Injection Molding/Detail_QnA_2_gpt-4_english.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/Plastic Injection Molding/Detail_QnA_3_gpt-4_english.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/Plastic Injection Molding/Detail_QnA_4_gpt-4_english.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/Plastic Injection Molding/Detail_QnA_5_gpt-4_english.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/Plastic Injection Molding/QnA_gpt-4_english.json\nProcessing folder: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/금속 사출성형\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/금속 사출성형/Detail_QnA_10_gpt-4_Korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/금속 사출성형/Detail_QnA_11_gpt-4_Korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/금속 사출성형/Detail_QnA_12_gpt-4_Korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/금속 사출성형/Detail_QnA_13_gpt-4_Korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/금속 사출성형/Detail_QnA_14_gpt-4_Korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/금속 사출성형/Detail_QnA_1_gpt-4_Korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/금속 사출성형/Detail_QnA_2_gpt-4_Korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/금속 사출성형/Detail_QnA_3_gpt-4_Korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/금속 사출성형/Detail_QnA_4_gpt-4_Korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/금속 사출성형/Detail_QnA_5_gpt-4_Korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/금속 사출성형/Detail_QnA_6_gpt-4_Korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/금속 사출성형/Detail_QnA_7_gpt-4_Korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/금속 사출성형/Detail_QnA_8_gpt-4_Korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/금속 사출성형/Detail_QnA_9_gpt-4_Korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/금속 사출성형/QnA_gpt-4_Korean.json\nProcessing folder: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/다이캐스팅 사출성형\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/다이캐스팅 사출성형/QnA_gpt-4_korean.json\nProcessing folder: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/사출성형 공정\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/사출성형 공정/Detail_QnA_10_gpt-4_korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/사출성형 공정/Detail_QnA_11_gpt-4_korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/사출성형 공정/Detail_QnA_12_gpt-4_korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/사출성형 공정/Detail_QnA_13_gpt-4_korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/사출성형 공정/Detail_QnA_14_gpt-4_korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/사출성형 공정/Detail_QnA_15_gpt-4_korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/사출성형 공정/Detail_QnA_16_gpt-4_korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/사출성형 공정/Detail_QnA_17_gpt-4_korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/사출성형 공정/Detail_QnA_18_gpt-4_korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/사출성형 공정/Detail_QnA_1_gpt-4_korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/사출성형 공정/Detail_QnA_2_gpt-4_korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/사출성형 공정/Detail_QnA_3_gpt-4_korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/사출성형 공정/Detail_QnA_4_gpt-4_korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/사출성형 공정/Detail_QnA_5_gpt-4_korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/사출성형 공정/Detail_QnA_6_gpt-4_korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/사출성형 공정/Detail_QnA_7_gpt-4_korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/사출성형 공정/Detail_QnA_8_gpt-4_korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/사출성형 공정/Detail_QnA_9_gpt-4_korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/사출성형 공정/QnA_gpt-4_korean.json\nProcessing folder: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/사출성형기\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/사출성형기/QnA_gpt-4_korean.json\nProcessing folder: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/플라스틱 사출성형\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/플라스틱 사출성형/Detail_QnA_10_gpt-4_Korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/플라스틱 사출성형/Detail_QnA_11_gpt-4_Korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/플라스틱 사출성형/Detail_QnA_12_gpt-4_Korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/플라스틱 사출성형/Detail_QnA_13_gpt-4_Korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/플라스틱 사출성형/Detail_QnA_14_gpt-4_Korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/플라스틱 사출성형/Detail_QnA_15_gpt-4_Korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/플라스틱 사출성형/Detail_QnA_16_gpt-4_Korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/플라스틱 사출성형/Detail_QnA_17_gpt-4_Korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/플라스틱 사출성형/Detail_QnA_18_gpt-4_Korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/플라스틱 사출성형/Detail_QnA_19_gpt-4_Korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/플라스틱 사출성형/Detail_QnA_1_gpt-4_Korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/플라스틱 사출성형/Detail_QnA_20_gpt-4_Korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/플라스틱 사출성형/Detail_QnA_21_gpt-4_Korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/플라스틱 사출성형/Detail_QnA_22_gpt-4_Korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/플라스틱 사출성형/Detail_QnA_23_gpt-4_Korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/플라스틱 사출성형/Detail_QnA_24_gpt-4_Korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/플라스틱 사출성형/Detail_QnA_25_gpt-4_Korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/플라스틱 사출성형/Detail_QnA_2_gpt-4_Korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/플라스틱 사출성형/Detail_QnA_3_gpt-4_Korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/플라스틱 사출성형/Detail_QnA_4_gpt-4_Korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/플라스틱 사출성형/Detail_QnA_5_gpt-4_Korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/플라스틱 사출성형/Detail_QnA_6_gpt-4_Korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/플라스틱 사출성형/Detail_QnA_7_gpt-4_Korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/플라스틱 사출성형/Detail_QnA_8_gpt-4_Korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/플라스틱 사출성형/Detail_QnA_9_gpt-4_Korean.json\nProcessing file: /home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/CODE/TestGPT_data/플라스틱 사출성형/QnA_gpt-4_Korean.json\nJSON 파일의 contents가 분할되어 txt 파일로 저장되었습니다.\n"
        }
      ],
      "execution_count": 22,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717395980862
        }
      },
      "id": "da485a09-e754-41c0-96ef-ddcff2ccfc59"
    },
    {
      "cell_type": "code",
      "source": [
        "#  지피티 생성 데이터 문장 단위로 split하신 데이터 \n",
        "gptdata_path = \"/home/azureuser/cloudfiles/code/Users/hb.suh/gptdata_split_by_dot\"  \n",
        "\n",
        "gptdata_file_paths = [os.path.join(gptdata_path, file) for file in os.listdir(gptdata_path) if file.endswith('.txt')] #list\n",
        "\n",
        "len(gptdata_file_paths)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 24,
          "data": {
            "text/plain": "509"
          },
          "metadata": {}
        }
      ],
      "execution_count": 24,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717396074064
        }
      },
      "id": "75c0168e-172b-423b-83b6-93720b0d1fc2"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from datasets import Dataset, DatasetDict, load_dataset\n",
        "\n",
        "\n",
        "# 기존 텍스트 파일 데이터셋 로드 ( 탕칭 연구원님이 split하신 데이터 )\n",
        "text_dataset = load_dataset('text', data_files=file_paths, cache_dir='/home/azureuser/cloudfiles/code/Users/hb.suh/cache')\n",
        "\n",
        "# JSON 파일에서 추출한 contents를 데이터셋으로 변환\n",
        "#json_dataset = Dataset.from_dict({\"text\": json_contents})\n",
        "json_dataset = load_dataset('text', data_files=gptdata_file_paths, cache_dir='/home/azureuser/cloudfiles/code/Users/hb.suh/cache')\n",
        "\n",
        "# text_path의 main_content 데이터를 데이터셋으로 변환\n",
        "#main_content_dataset = Dataset.from_dict({\"text\": main_content_texts})\n",
        "main_content_dataset = load_dataset('text', data_files=scolar_file_paths, cache_dir='/home/azureuser/cloudfiles/code/Users/hb.suh/cache')\n",
        "\n",
        "# 텍스트와 JSON 데이터셋 병합\n",
        "combined_dataset = DatasetDict({\n",
        "    \"text_files\": text_dataset['train'],\n",
        "    \"json_contents\": json_dataset['train'],\n",
        "    \"main_contents\": main_content_dataset['train']\n",
        "})\n",
        "\n",
        "# 데이터셋 확인 (예시)\n",
        "print(combined_dataset)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Downloading data: 100%|██████████| 509/509 [00:00<00:00, 31027.83files/s]\nGenerating train split: 8270 examples [00:20, 413.07 examples/s]\nDownloading data: 100%|██████████| 4652/4652 [00:00<00:00, 57745.12files/s]\nGenerating train split: 319509 examples [03:13, 1648.80 examples/s]\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "DatasetDict({\n    text_files: Dataset({\n        features: ['text'],\n        num_rows: 70368\n    })\n    json_contents: Dataset({\n        features: ['text'],\n        num_rows: 8270\n    })\n    main_contents: Dataset({\n        features: ['text'],\n        num_rows: 319509\n    })\n})\n"
        }
      ],
      "execution_count": 25,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717396732933
        }
      },
      "id": "7ee3b956-7d9f-4c25-b4c0-01951f1de464"
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 로드\n",
        "\n",
        "from datasets import DatasetDict, Dataset\n",
        "\n",
        "# 각 데이터셋을 train과 test로 분할\n",
        "text_files_split = combined_dataset['text_files'].train_test_split(test_size=0.2)\n",
        "json_contents_split = combined_dataset['json_contents'].train_test_split(test_size=0.2)\n",
        "main_contents_split = combined_dataset['main_contents'].train_test_split(test_size=0.2)\n",
        "\n",
        "# 분할된 데이터셋을 다시 DatasetDict로 구성\n",
        "train_test_combined_dataset = DatasetDict({\n",
        "    'train': DatasetDict({\n",
        "        'text_files': text_files_split['train'],\n",
        "        'json_contents': json_contents_split['train'],\n",
        "        'main_contents': main_contents_split['train']\n",
        "    }),\n",
        "    'val': DatasetDict({\n",
        "        'text_files': text_files_split['test'],\n",
        "        'json_contents': json_contents_split['test'],\n",
        "        'main_contents': main_contents_split['test']\n",
        "    })\n",
        "})\n",
        "\n",
        "print(train_test_combined_dataset)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "DatasetDict({\n    train: DatasetDict({\n        text_files: Dataset({\n            features: ['text'],\n            num_rows: 56294\n        })\n        json_contents: Dataset({\n            features: ['text'],\n            num_rows: 6616\n        })\n        main_contents: Dataset({\n            features: ['text'],\n            num_rows: 255607\n        })\n    })\n    val: DatasetDict({\n        text_files: Dataset({\n            features: ['text'],\n            num_rows: 14074\n        })\n        json_contents: Dataset({\n            features: ['text'],\n            num_rows: 1654\n        })\n        main_contents: Dataset({\n            features: ['text'],\n            num_rows: 63902\n        })\n    })\n})\n"
        }
      ],
      "execution_count": 26,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717396734767
        }
      },
      "id": "8377b208-4239-419b-ba09-1b94c3752081"
    },
    {
      "cell_type": "code",
      "source": [
        "# 텍스트 파일 경로를 파일로 저장\n",
        "\n",
        "merged_file_path = \"/home/azureuser/cloudfiles/code/Users/hb.suh/merged_data.txt\"\n",
        "with open(merged_file_path, 'w', encoding='utf-8') as f:\n",
        "    # text_files 데이터셋 저장\n",
        "    for item in combined_dataset['text_files']:\n",
        "        text = item['text']\n",
        "        if isinstance(text, dict):\n",
        "            text = json.dumps(text)\n",
        "        f.write(text + '\\n')\n",
        "    \n",
        "    # json_contents 데이터셋 저장\n",
        "    for item in combined_dataset['json_contents']:\n",
        "        text = item['text']\n",
        "        if isinstance(text, dict):\n",
        "            text = json.dumps(text)\n",
        "        f.write(text + '\\n')\n",
        "    \n",
        "    # main_contents 데이터셋 저장\n",
        "    for item in combined_dataset['main_contents']:\n",
        "        text = item['text']\n",
        "        if isinstance(text, dict):\n",
        "            text = json.dumps(text)\n",
        "        f.write(text + '\\n')"
      ],
      "outputs": [],
      "execution_count": 27,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717396746925
        }
      },
      "id": "cb585377-54c2-416c-b9d5-c0024c404a70"
    },
    {
      "cell_type": "code",
      "source": [
        "# 텍스트 파일 경로를 파일로 저장\n",
        "\n",
        "merged_file_path = \"/home/azureuser/cloudfiles/code/Users/hb.suh/merged_data.txt\"\n",
        "\n",
        "\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "\n",
        "# BertWordPieceTokenizer 인스턴스 생성\n",
        "tokenizer = BertWordPieceTokenizer(clean_text=True, handle_chinese_chars=True, strip_accents=False, lowercase=False, wordpieces_prefix=\"##\")\n",
        "\n",
        "# 훈련하기\n",
        "tokenizer.train(\n",
        "    merged_file_path,\n",
        "    vocab_size=10000,\n",
        "    min_frequency=3,\n",
        "    show_progress=True,\n",
        "    special_tokens = [\"[PAD]\", \"[CLS]\", \"[UNK]\", \"[SEP]\", \"[MASK]\"],\n",
        "    wordpieces_prefix=\"##\",\n",
        ")\n",
        "\n",
        "# 저장할 디렉토리 경로    \n",
        "save_path = \"/home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/trained_tokenizer/0603_AddData_bertwordpiece\"\n",
        "\n",
        "# 디렉토리가 없으면 생성\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "\n",
        "# 토크나이저 모델 저장\n",
        "tokenizer.save_model(save_path)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\n\n\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 28,
          "data": {
            "text/plain": "['/home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/trained_tokenizer/0603_AddData_bertwordpiece/vocab.txt']"
          },
          "metadata": {}
        }
      ],
      "execution_count": 28,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717397884835
        }
      },
      "id": "ccd59e81-cfd1-416e-8b10-3461eaf72468"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertTokenizerFast, BertForMaskedLM, BertConfig, AdamW\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "from datasets import load_dataset\n",
        "import os\n",
        "from datetime import datetime\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# 새로운 토크나이저 로드\n",
        "new_tokenizer = BertTokenizerFast.from_pretrained('beomi/kcbert-base')\n",
        "model = BertForMaskedLM.from_pretrained('beomi/kcbert-base')\n",
        "len(new_tokenizer)\n",
        "\n",
        "new_tokens_added = 0\n",
        "new_vocab = {}  # 새로운 토큰과 아이디를 담을 딕셔너리\n",
        "\n",
        "path = os.path.join(save_path, \"vocab.txt\")\n",
        "# 기존의 토크나이저 vocab에서 새로운 토큰들을 제외하고 아이디 부여\n",
        "with open(path, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        token = line.strip()  # 줄바꿈 문자를 제거하여 토큰만 가져옴\n",
        "        if token not in new_tokenizer.get_vocab():\n",
        "            new_tokenizer.add_tokens(token)\n",
        "            new_tokens_added += 1\n",
        "            new_vocab[token] = len(new_tokenizer) - 1  # 새로운 토큰에 새로운 아이디 부여\n",
        "\n",
        "        else:\n",
        "            # 토큰이 이미 존재하면, 기존 ID 사용\n",
        "            new_vocab[token] = new_tokenizer.convert_tokens_to_ids(token)\n",
        "\n",
        "print(\"new_tokens_added : \", new_tokens_added)\n",
        "print(len(new_tokenizer))\n",
        "\n",
        "# 토크나이저 모델 파일 저장\n",
        "tokenizer_model_path = \"/home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/trained_tokenizer/0603_AddData_bertwordpiece\"\n",
        "new_tokenizer.save_pretrained(tokenizer_model_path,do_lower_case=False)\n",
        "\n",
        "len(new_tokenizer)\n",
        "\n",
        "print(f\"Added {new_tokens_added} new tokens\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "new_tokens_added :  6924\n36924\nAdded 6924 new tokens\n"
        }
      ],
      "execution_count": 30,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717398199906
        }
      },
      "id": "7a2f974b-41c7-4939-91f7-b868c1de0433"
    },
    {
      "cell_type": "code",
      "source": [
        "# 기존 가중치 freeze\n",
        "def resize_and_initialize_embeddings(model, tokenizer, old_tokenizer=None):\n",
        "    # 모델의 원래 토큰 임베딩 크기\n",
        "    original_num_tokens = model.config.vocab_size if old_tokenizer is None else len(old_tokenizer)\n",
        "    \n",
        "    # 새 토크나이저에 따른 임베딩 크기 조정\n",
        "    new_num_tokens = len(tokenizer)\n",
        "    model.resize_token_embeddings(new_num_tokens)\n",
        "\n",
        "    # 임베딩 레이어에 접근\n",
        "    embeddings = model.get_input_embeddings()\n",
        "    \n",
        "    # 기존 임베딩의 그라디언트 업데이트 비활성화\n",
        "    embeddings.weight.requires_grad = False\n",
        "\n",
        "    # 새로운 토큰이 추가되었을 경우\n",
        "    if new_num_tokens > original_num_tokens:\n",
        "        # 새로운 토큰의 임베딩만 그라디언트 업데이트 활성화\n",
        "        embeddings.weight[original_num_tokens:].requires_grad = True\n",
        "        \n",
        "        # 기존 임베딩의 평균과 표준편차 계산\n",
        "        with torch.no_grad():\n",
        "            old_embeddings = embeddings.weight[:original_num_tokens]\n",
        "            mean, std = old_embeddings.mean(dim=0), old_embeddings.std(dim=0)\n",
        "            # 새로운 토큰의 임베딩 초기화\n",
        "            new_embeddings = torch.randn(new_num_tokens - original_num_tokens, embeddings.embedding_dim, device=old_embeddings.device)\n",
        "            new_embeddings = mean + std * new_embeddings\n",
        "            embeddings.weight[original_num_tokens:].data = new_embeddings\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# model = BertForMaskedLM.from_pretrained('beomi/kcbert-base')\n",
        "# 드롭아웃 비율 조정\n",
        "for name, module in model.named_modules():\n",
        "    if isinstance(module, torch.nn.Dropout):\n",
        "        module.p = 0.3  # 드롭아웃 확률을 0.3으로 조정\n",
        "###############################################################\n",
        "after_add_Trained_Tokenizer = BertTokenizerFast.from_pretrained(\"/home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/trained_tokenizer/0603_AddData_bertwordpiece/\",do_lower_case=False)\n",
        "\n",
        "model = resize_and_initialize_embeddings(model, after_add_Trained_Tokenizer )\n",
        "print(len(after_add_Trained_Tokenizer))\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "36924\n"
        }
      ],
      "execution_count": 31,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717398216803
        }
      },
      "id": "513180b0-a5a7-4b8c-9253-2a386532606d"
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model, tokenizer, pretrained_model_name):\n",
        "\n",
        "    safe_model_name = pretrained_model_name.replace(\"/\", \"-\")\n",
        "    # 현재 날짜와 시간을 포함한 디렉토리 이름을 생성\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "    directory = f\"/home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/MLM_trained_model/{safe_model_name}_CyclicLRtriangular-{timestamp}\"\n",
        "\n",
        "    # 디렉토리가 존재하지 않으면 생성\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "    # 모델과 토크나이저를 사전 훈련된 디렉토리에 저장\n",
        "    model.save_pretrained(directory)\n",
        "    tokenizer.save_pretrained(directory)"
      ],
      "outputs": [],
      "execution_count": 32,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717398217033
        }
      },
      "id": "1c321515-06b0-47ab-92f8-a163aa7160c9"
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_data(tokenizer, dataset):\n",
        "    def tokenize_function(examples):\n",
        "        if isinstance(examples['text'], list):\n",
        "            examples['text'] = [str(x) for x in examples['text']]\n",
        "        else:\n",
        "            examples['text'] = [str(examples['text'])]\n",
        "        tokenized_output = tokenizer(examples['text'], max_length=300, truncation=True, padding=\"max_length\")\n",
        "        return {'input_ids': tokenized_output['input_ids'], 'attention_mask': tokenized_output['attention_mask']}\n",
        "    \n",
        "    return dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n",
        "\n",
        "# train과 validation 데이터셋을 개별적으로 토큰화\n",
        "train_test_combined_dataset = DatasetDict({\n",
        "    'train': {\n",
        "        'text_files': tokenize_data(after_add_Trained_Tokenizer, train_test_combined_dataset['train']['text_files']),\n",
        "        'json_contents': tokenize_data(after_add_Trained_Tokenizer, train_test_combined_dataset['train']['json_contents']),\n",
        "        'main_contents': tokenize_data(after_add_Trained_Tokenizer, train_test_combined_dataset['train']['main_contents'])\n",
        "    },\n",
        "    'valid': {\n",
        "        'text_files': tokenize_data(after_add_Trained_Tokenizer, train_test_combined_dataset['val']['text_files']),\n",
        "        'json_contents': tokenize_data(after_add_Trained_Tokenizer, train_test_combined_dataset['val']['json_contents']),\n",
        "        'main_contents': tokenize_data(after_add_Trained_Tokenizer, train_test_combined_dataset['val']['main_contents'])\n",
        "    }\n",
        "})\n",
        "\n",
        "\n",
        "from datasets import concatenate_datasets\n",
        "# 모든 데이터셋을 하나로 병합\n",
        "train_dataset = concatenate_datasets([\n",
        "    train_test_combined_dataset['train']['text_files'],\n",
        "    train_test_combined_dataset['train']['json_contents'],\n",
        "    train_test_combined_dataset['train']['main_contents']\n",
        "])\n",
        "val_dataset = concatenate_datasets([\n",
        "    train_test_combined_dataset['valid']['text_files'],\n",
        "    train_test_combined_dataset['valid']['json_contents'],\n",
        "    train_test_combined_dataset['valid']['main_contents']\n",
        "])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Map: 100%|██████████| 56294/56294 [00:11<00:00, 4748.39 examples/s]\nMap: 100%|██████████| 6616/6616 [00:01<00:00, 4985.29 examples/s]\nMap: 100%|██████████| 255607/255607 [00:47<00:00, 5335.07 examples/s]\nMap: 100%|██████████| 14074/14074 [00:03<00:00, 4619.33 examples/s]\nMap: 100%|██████████| 1654/1654 [00:00<00:00, 3842.17 examples/s]\nMap: 100%|██████████| 63902/63902 [00:12<00:00, 4967.29 examples/s]\n"
        }
      ],
      "execution_count": 33,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717398677792
        }
      },
      "id": "0bc5915c-2c6b-4cac-9b82-534190a6ce84"
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model, tokenizer, pretrained_model_name=\"kcbert-base\", best_model=False):\n",
        "    relative_directory_path = f\"MLM_trained_model/{pretrained_model_name}_CyclicLRtriangular_lr1e-5\"\n",
        "    directory = os.path.normpath(os.path.join(os.getcwd(), relative_directory_path))\n",
        "    \n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "    \n",
        "    if best_model:\n",
        "        model_name = \"model_best\"\n",
        "    else:\n",
        "        model_name = \"model_last\"\n",
        "    \n",
        "    model_path = os.path.join(directory, model_name)\n",
        "    model.save_pretrained(model_path)\n",
        "    tokenizer.save_pretrained(model_path)\n",
        "    print(f\"Model saved in {model_path}\")\n",
        "    print(f\"Tokenizer saved in {directory}\")\n",
        "    \n",
        "def calculate_accuracy(predictions, labels):\n",
        "    preds = torch.argmax(predictions, dim=-1)\n",
        "    mask = labels != -100  # Ignore padding index -100\n",
        "    correct = (preds == labels) & mask\n",
        "    accuracy = correct.sum().item() / mask.sum().item()\n",
        "    return accuracy"
      ],
      "outputs": [],
      "execution_count": 34,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717398949863
        }
      },
      "id": "fc959342-26fd-4762-9cfb-ba23575403a5"
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train_model(model, train_dataset, valid_dataset, tokenizer, \n",
        "                EPOCH, early_stopping_patience=None, pretrained_model_name=\"kcbert-base\"):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Optimizer and learning rate scheduler setup\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "    total_steps = len(train_dataset) * EPOCH\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, \n",
        "                                                  base_lr=1e-5, \n",
        "                                                  max_lr=1e-3,  # Adjusted max_lr\n",
        "                                                  step_size_up=total_steps // 4,\n",
        "                                                  step_size_down=total_steps - total_steps // 4,\n",
        "                                                  mode='triangular',\n",
        "                                                  cycle_momentum=False)\n",
        "\n",
        "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=16, collate_fn=data_collator)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=16, collate_fn=data_collator)\n",
        "    \n",
        "    model.train()\n",
        "\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    valid_accuracies = []\n",
        "    best_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(EPOCH):\n",
        "        model.train()\n",
        "        epoch_train_losses = []\n",
        "        \n",
        "        for i, batch in enumerate(tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{EPOCH}\")):\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}  # input_ids, attention_mask, labels           \n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            \n",
        "            # Implementing gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            epoch_train_losses.append(loss.item())\n",
        "        \n",
        "        if epoch_train_losses:\n",
        "            train_loss = sum(epoch_train_losses) / len(epoch_train_losses)\n",
        "        else:\n",
        "            train_loss = 0\n",
        "        \n",
        "        train_losses.append(train_loss)\n",
        "        \n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        epoch_valid_losses = []\n",
        "        epoch_valid_accuracies = []\n",
        "        with torch.no_grad():\n",
        "            for batch in valid_loader:\n",
        "                batch = {k: v.to(device) for k, v in batch.items()}\n",
        "                \n",
        "                outputs = model(**batch)\n",
        "                loss = outputs.loss\n",
        "                logits = outputs.logits.detach().cpu()\n",
        "                \n",
        "                epoch_valid_losses.append(loss.item())\n",
        "                \n",
        "                accuracy = calculate_accuracy(logits, batch['labels'].cpu())\n",
        "                epoch_valid_accuracies.append(accuracy)\n",
        "                torch.cuda.empty_cache()\n",
        "            \n",
        "            if epoch_valid_losses:\n",
        "                valid_loss = sum(epoch_valid_losses) / len(epoch_valid_losses)\n",
        "            else:\n",
        "                valid_loss = 0\n",
        "            if epoch_valid_accuracies:\n",
        "                valid_accuracy = sum(epoch_valid_accuracies) / len(epoch_valid_accuracies)\n",
        "            else:\n",
        "                valid_accuracy = 0\n",
        "        \n",
        "        valid_losses.append(valid_loss)\n",
        "        valid_accuracies.append(valid_accuracy)\n",
        "        \n",
        "        print(f\"Epoch {epoch + 1}/{EPOCH}, Train loss: {train_loss:.4f}, Valid loss: {valid_loss:.4f}, Valid accuracy: {valid_accuracy:.4f}, Current LR: {scheduler.get_last_lr()[0]}\")\n",
        "             \n",
        "        # Early stopping logic\n",
        "        if valid_loss < best_loss:\n",
        "            best_loss = valid_loss\n",
        "            patience_counter = 0\n",
        "            # save the best model\n",
        "            save_model(model, tokenizer, pretrained_model_name, best_model=True)\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if early_stopping_patience and patience_counter >= early_stopping_patience:\n",
        "                print(f\"Early stopping at epoch {epoch + 1} as no improvement in validation loss.\")\n",
        "                break\n",
        "                \n",
        "    save_model(model, tokenizer, pretrained_model_name)\n",
        "    # Plotting the training and validation loss trends\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(valid_losses, label='Validation Loss')\n",
        "    plt.plot(valid_accuracies, label='Validation Acc')\n",
        "    plt.title('Training and Validation Loss Trends')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    plot_path = os.path.join('result', 'training_validation_loss_trends.png')\n",
        "    plt.savefig(plot_path)\n",
        "\n",
        "    return model"
      ],
      "outputs": [],
      "execution_count": 40,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717399183510
        }
      },
      "id": "cd58e274-a294-477b-9906-e214c6efb8c5"
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습 \n",
        "model = train_model(model, train_dataset, val_dataset, after_add_Trained_Tokenizer, \n",
        "                    500, early_stopping_patience=30, pretrained_model_name=\"kcbert-base\")  \n",
        "\n",
        "# 모델 저장\n",
        "# save_model(model, after_add_Trained_Tokenizer, \"beomi/kcbert-base\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Training Epoch 1/500:  18%|█▊        | 3622/19908 [20:09<1:30:40,  2.99it/s]"
        }
      ],
      "execution_count": 41,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717399054289
        }
      },
      "id": "42a2c365-70f5-4d47-a7b1-44a66b747953"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "0b5c083f-2158-47bd-b28c-a35e6e77ee04"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "llm-rag-embeddings",
      "language": "python",
      "display_name": "genai"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.19",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "llm-rag-embeddings"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}