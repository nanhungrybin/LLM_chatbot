{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import uuid\n",
        "\n",
        "# from llama_index.core import SimpleDirectoryReader\n",
        "# from llama_index.core.node_parser import SimpleNodeParser\n",
        "# from llama_index.core.schema import MetadataMode\n",
        "#from llama_index.node_parser import SimpleNodeParser\n",
        "\n",
        "from llama_index import SimpleDirectoryReader\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.schema import MetadataMode\n",
        "from llama_index.core.schema import Document\n",
        "\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"//anaconda/envs/azureml_py38/bin/python\")\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "from openai import OpenAI\n",
        "OPENAI_API_KEY = \"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\"\n",
        "os.environ[\"OPENAI_API_KEY\"]=OPENAI_API_KEY\n",
        "client = OpenAI()\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Fine-Tuning Embedding for RAG with Synthetic Data\n",
        "RAG 성능을 향상시키기 위해 임베딩 모델을 미세 조정\n",
        "\n",
        "1. LLM을 사용하여 합성 데이터세트를 생성\n",
        "2. 오픈소스 임베딩 모델을 미세 조정 \n",
        "3. 최종적으로 미세 조정된 모델을 평가\n",
        "\n",
        "=> 임베딩 모델을 미세 조정하면 검색 성능을 크게 향상시킬 수 있음\n",
        "SentenceTransformer\n",
        "\"\"\""
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'llama_index'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 9\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01muuid\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# from llama_index.core import SimpleDirectoryReader\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# from llama_index.core.node_parser import SimpleNodeParser\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# from llama_index.core.schema import MetadataMode\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#from llama_index.node_parser import SimpleNodeParser\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleDirectoryReader\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnode_parser\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceSplitter\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MetadataMode\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'llama_index'"
          ]
        }
      ],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1715069410474
        }
      },
      "id": "446f9dc1-9099-4845-918f-e8cd2beace4b"
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "print(sys.executable)\n",
        "#! /anaconda/envs/jupyter_env/bin/python -m pip install langchain_experimental\n",
        "#!/anaconda/envs/azureml_py310_sdkv2/bin/python -m pip install torch\n",
        "#!/anaconda/envs/azureml_py310_sdkv2/bin/python -m pip install --upgrade typing_extensions\n",
        "# !/anaconda/envs/azureml_py310_sdkv2/bin/python -m pip install --force-reinstall typing-extensions==4.5\n",
        "# !/anaconda/envs/azureml_py310_sdkv2/bin/python -m pip install --force-reinstall openai==1.8\n",
        "!/anaconda/envs/azureml_py310_sdkv2/bin/python -m pip install packaging==23.0\n",
        "!/anaconda/envs/azureml_py310_sdkv2/bin/python -m pip install azure-storage-blob==12.13.0\n",
        "!/anaconda/envs/azureml_py310_sdkv2/bin/python -m pip install pydantic==1.9.1\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "/anaconda/envs/jupyter_env/bin/python\nCollecting packaging==23.0\n  Using cached packaging-23.0-py3-none-any.whl (42 kB)\nInstalling collected packages: packaging\n  Attempting uninstall: packaging\n    Found existing installation: packaging 23.2\n    Uninstalling packaging-23.2:\n      Successfully uninstalled packaging-23.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlangchain-core 0.1.23 requires packaging<24.0,>=23.2, but you have packaging 23.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed packaging-23.0\nCollecting azure-storage-blob==12.13.0\n  Downloading azure_storage_blob-12.13.0-py3-none-any.whl (377 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.3/377.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: azure-core<2.0.0,>=1.23.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from azure-storage-blob==12.13.0) (1.27.1)\nRequirement already satisfied: msrest>=0.6.21 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from azure-storage-blob==12.13.0) (0.7.1)\nRequirement already satisfied: cryptography>=2.1.4 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from azure-storage-blob==12.13.0) (41.0.1)\nRequirement already satisfied: requests>=2.18.4 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from azure-core<2.0.0,>=1.23.1->azure-storage-blob==12.13.0) (2.31.0)\nRequirement already satisfied: six>=1.11.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from azure-core<2.0.0,>=1.23.1->azure-storage-blob==12.13.0) (1.16.0)\nRequirement already satisfied: typing-extensions>=4.3.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from azure-core<2.0.0,>=1.23.1->azure-storage-blob==12.13.0) (4.11.0)\nRequirement already satisfied: cffi>=1.12 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from cryptography>=2.1.4->azure-storage-blob==12.13.0) (1.15.1)\nRequirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from msrest>=0.6.21->azure-storage-blob==12.13.0) (2024.2.2)\nRequirement already satisfied: isodate>=0.6.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from msrest>=0.6.21->azure-storage-blob==12.13.0) (0.6.1)\nRequirement already satisfied: requests-oauthlib>=0.5.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from msrest>=0.6.21->azure-storage-blob==12.13.0) (1.3.1)\nRequirement already satisfied: pycparser in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob==12.13.0) (2.21)\nRequirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.23.1->azure-storage-blob==12.13.0) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.23.1->azure-storage-blob==12.13.0) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.23.1->azure-storage-blob==12.13.0) (1.26.16)\nRequirement already satisfied: oauthlib>=3.0.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.21->azure-storage-blob==12.13.0) (3.2.2)\nInstalling collected packages: azure-storage-blob\n  Attempting uninstall: azure-storage-blob\n    Found existing installation: azure-storage-blob 12.16.0\n    Uninstalling azure-storage-blob-12.16.0:\n      Successfully uninstalled azure-storage-blob-12.16.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nazure-storage-file-datalake 12.11.0 requires azure-storage-blob<13.0.0,>=12.16.0b1, but you have azure-storage-blob 12.13.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed azure-storage-blob-12.13.0\nCollecting pydantic==1.9.1\n  Downloading pydantic-1.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pydantic==1.9.1) (4.11.0)\nInstalling collected packages: pydantic\n  Attempting uninstall: pydantic\n    Found existing installation: pydantic 2.7.1\n    Uninstalling pydantic-2.7.1:\n      Successfully uninstalled pydantic-2.7.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlangchain-core 0.1.23 requires packaging<24.0,>=23.2, but you have packaging 23.0 which is incompatible.\nllamaindex-py-client 0.1.18 requires pydantic>=1.10, but you have pydantic 1.9.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed pydantic-1.9.1\n"
        }
      ],
      "execution_count": 26,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1715068285649
        }
      },
      "id": "f97dbacb-a415-49cf-80fc-cdeeab3a5b48"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_experimental\n",
        "!pip install llama_index\n",
        "!pip install langchain_openai"
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1715067966241
        }
      },
      "id": "5007e55c-dc9d-4c30-8dbf-8eead1e0a9a9"
    },
    {
      "cell_type": "code",
      "source": [
        "# ! /anaconda/envs/jupyter_env/bin/python -m pip install --upgrade pip\n",
        "# ! /anaconda/envs/jupyter_env/bin/python -m pip install --use-feature=2020-resolver azureml-core langchain-core\n",
        "! /anaconda/envs/jupyter_env/bin/python -m pip install langchain_openai"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Collecting langchain_openai\n  Downloading langchain_openai-0.1.6-py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: langchain-core<0.2.0,>=0.1.46 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from langchain_openai) (0.1.52)\nCollecting openai<2.0.0,>=1.24.0 (from langchain_openai)\n  Downloading openai-1.26.0-py3-none-any.whl.metadata (21 kB)\nCollecting tiktoken<1,>=0.5.2 (from langchain_openai)\n  Downloading tiktoken-0.6.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: PyYAML>=5.3 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from langchain-core<0.2.0,>=0.1.46->langchain_openai) (6.0)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from langchain-core<0.2.0,>=0.1.46->langchain_openai) (1.33)\nRequirement already satisfied: langsmith<0.2.0,>=0.1.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from langchain-core<0.2.0,>=0.1.46->langchain_openai) (0.1.54)\nRequirement already satisfied: packaging<24.0,>=23.2 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from langchain-core<0.2.0,>=0.1.46->langchain_openai) (23.2)\nRequirement already satisfied: pydantic<3,>=1 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from langchain-core<0.2.0,>=0.1.46->langchain_openai) (1.10.9)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from langchain-core<0.2.0,>=0.1.46->langchain_openai) (8.2.3)\nRequirement already satisfied: anyio<5,>=3.5.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (3.7.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (1.8.0)\nCollecting httpx<1,>=0.23.0 (from openai<2.0.0,>=1.24.0->langchain_openai)\n  Using cached httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\nRequirement already satisfied: sniffio in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (1.3.0)\nCollecting tqdm>4 (from openai<2.0.0,>=1.24.0->langchain_openai)\n  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: typing-extensions<5,>=4.7 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (4.11.0)\nCollecting regex>=2022.1.18 (from tiktoken<1,>=0.5.2->langchain_openai)\n  Downloading regex-2024.4.28-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: requests>=2.26.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from tiktoken<1,>=0.5.2->langchain_openai) (2.31.0)\nRequirement already satisfied: idna>=2.8 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.24.0->langchain_openai) (3.4)\nRequirement already satisfied: exceptiongroup in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.24.0->langchain_openai) (1.1.1)\nRequirement already satisfied: certifi in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain_openai) (2023.5.7)\nCollecting httpcore==1.* (from httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain_openai)\n  Using cached httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\nCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain_openai)\n  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\nRequirement already satisfied: jsonpointer>=1.9 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.46->langchain_openai) (2.4)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.46->langchain_openai) (3.10.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from requests>=2.26.0->tiktoken<1,>=0.5.2->langchain_openai) (3.1.0)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from requests>=2.26.0->tiktoken<1,>=0.5.2->langchain_openai) (1.26.16)\nDownloading langchain_openai-0.1.6-py3-none-any.whl (34 kB)\nDownloading openai-1.26.0-py3-none-any.whl (314 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.1/314.1 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading tiktoken-0.6.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hUsing cached httpx-0.27.0-py3-none-any.whl (75 kB)\nUsing cached httpcore-1.0.5-py3-none-any.whl (77 kB)\nDownloading regex-2024.4.28-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (777 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m777.1/777.1 kB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hUsing cached h11-0.14.0-py3-none-any.whl (58 kB)\nInstalling collected packages: tqdm, regex, h11, tiktoken, httpcore, httpx, openai, langchain_openai\nSuccessfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 langchain_openai-0.1.6 openai-1.26.0 regex-2024.4.28 tiktoken-0.6.0 tqdm-4.66.4\n"
        }
      ],
      "execution_count": 18,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1715067677272
        }
      },
      "id": "3c451157-a388-4968-810d-0c79bc5ddd2d"
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"//anaconda/envs/azureml_py38/bin/python\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "7d0e08be-d768-4e73-a1db-a9bd1355da13"
    },
    {
      "cell_type": "code",
      "source": [
        "def load_text_data(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        content = file.readlines()\n",
        "    return content\n",
        "\n",
        "\n",
        "def save_to_json(data, file_path):\n",
        "    with open(file_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "        \n",
        "def split_trainval(text_data):\n",
        "    \n",
        "    # Calculate the index for a 70-30 split\n",
        "    split_index = int(len(text_data) * 0.7)  # 70% for training\n",
        "    train_corpus = text_data[:split_index]\n",
        "    val_corpus = text_data[split_index:]\n",
        "    return train_corpus, val_corpus\n",
        "\n",
        "\n",
        "def save_corpus(corpus, dir_path):\n",
        "    \n",
        "    os.makedirs(dir_path, exist_ok=True)  # Ensure the directory exists\n",
        "    corpus_path = os.path.join(dir_path, 'corpus.txt')\n",
        "    with open(corpus_path, 'w', encoding='utf-8') as file:\n",
        "        for line in corpus:\n",
        "            file.write(str(line) + '\\n')  # Write line as a string, ensuring conversion\n",
        "\n",
        "\n",
        "# def load_corpus(file_list, verbose=False):\n",
        "\n",
        "#     # if verbose:\n",
        "#     #     print(f\"Loading files {file}\")\n",
        "\n",
        "#     # reader = SimpleDirectoryReader(input_files=file)\n",
        "#     # docs = reader.load_data()\n",
        "    \n",
        "#     # if verbose:\n",
        "#     #     print(f'Loaded {len(docs)} docs')\n",
        "\n",
        "#     # documents = [Document(content=str(text)) for text in text_list]\n",
        "#     documents = []\n",
        "#     # 각 파일 경로에 대한 내용을 읽어 Document 객체를 생성\n",
        "#     for file_path in file_list:\n",
        "#         if os.path.exists(file_path):\n",
        "#             with open(file_path, 'r', encoding='utf-8') as file:\n",
        "#                 content = file.read()\n",
        "#                 documents.append(Document(text=content))\n",
        "#         else:\n",
        "#             if verbose:\n",
        "#                 print(f\"File not found: {file_path}\")\n",
        "\n",
        "#     #print(documents)            \n",
        "\n",
        "#     # parse nodes\n",
        "#     parser = SentenceSplitter()\n",
        "#     nodes = parser.get_nodes_from_documents(documents) # 파일에서 직접 읽은 Document 리스트를 파서에 전달\n",
        "\n",
        "#     # parser = SimpleNodeParser.from_defaults()\n",
        "#     # nodes = parser.get_nodes_from_documents(documents, show_progress=verbose) \n",
        "#     print(nodes)\n",
        "\n",
        "#     if verbose:\n",
        "#         print(f'Parsed {len(nodes)} nodes')\n",
        "\n",
        "#     corpus = {node.node_id: node.get_content(metadata_mode=MetadataMode.NONE) for node in nodes}\n",
        "#     return corpus\n",
        "\n",
        "\n",
        "\n",
        "# from langchain_experimental.text_splitter import SemanticChunker   => 버전때문에 사용 불가\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "def semantic_chunking(text):\n",
        "    okt = Okt()\n",
        "    noun_phrases = okt.nouns(text)  # 텍스트에서 명사만 추출\n",
        "    return noun_phrases\n",
        "\n",
        "def load_corpus(file_list, verbose=False):\n",
        "\n",
        "    documents = []\n",
        "    # 각 파일 경로에 대한 내용을 읽어 Document 객체를 생성\n",
        "    for file_path in file_list:\n",
        "        if os.path.exists(file_path):\n",
        "            with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                content = file.read()\n",
        "                documents.append(Document(text=content))\n",
        "        else:\n",
        "            if verbose:\n",
        "                print(f\"File not found: {file_path}\")\n",
        "\n",
        "    # parse nodes\n",
        "    parser = SentenceSplitter()\n",
        "    nodes = parser.get_nodes_from_documents(documents) # 파일에서 직접 읽은 Document 리스트를 파서에 전달\n",
        "   \n",
        "    # 새로운 corpus 딕셔너리: node_id와 추출된 청크를 매핑\n",
        "    corpus = {}\n",
        "    for node in nodes:\n",
        "        # 각 노드(문서)에 대한 내용을 가져와 세맨틱 청킹 적용\n",
        "        chunks = semantic_chunking(node['content'])\n",
        "        # 세맨틱 청크를 corpus에 저장\n",
        "        corpus[node.node_id] = ' '.join(chunks)  # 청크들을 공백으로 연결하여 저장\n",
        "\n",
        "        if verbose:\n",
        "            print(f'Node ID {node.node_id}: Extracted {len(chunks)} semantic chunks')\n",
        "\n",
        "    if verbose:\n",
        "        print(f'Parsed {len(nodes)} nodes, Corpus now has {len(corpus)} entries with semantic chunks.')\n",
        "\n",
        "    return corpus\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'konlpy'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 73\u001b[0m\n\u001b[1;32m     27\u001b[0m             file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;28mstr\u001b[39m(line) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Write line as a string, ensuring conversion\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# def load_corpus(file_list, verbose=False):\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#     # if verbose:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# from langchain_experimental.text_splitter import SemanticChunker   => 버전때문에 사용 불가\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkonlpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Okt\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msemantic_chunking\u001b[39m(text):\n\u001b[1;32m     76\u001b[0m     okt \u001b[38;5;241m=\u001b[39m Okt()\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'konlpy'"
          ]
        }
      ],
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1715061796775
        }
      },
      "id": "b40af3c6-e633-4880-9421-61421d3c7a98"
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import uuid\n",
        "from llama_index.llms import OpenAI\n",
        "from llama_index.schema import MetadataMode\n",
        "\n",
        "def generate_queries(text_data, num_questions_per_chunk=2, prompt_template=None, verbose=False):\n",
        "    \"\"\"\n",
        "    Automatically generate hypothetical questions that could be answered with\n",
        "    doc in the corpus.\n",
        "    \"\"\"\n",
        "    #text_list = list(text_data.values()) if isinstance(text_data, dict) else text_data\n",
        "\n",
        "    if prompt_template is None:\n",
        "        prompt_template = \"Given the context: {context_str}, generate {num_questions_per_chunk} questions in Korean.\"\n",
        "\n",
        "    queries = {}\n",
        "    relevant_docs = {}\n",
        "\n",
        "    for node_id, text in tqdm(text_data.items()): #코퍼스의 노드 아이디와 텍스트 내용 \n",
        "        query = prompt_template.format(context_str=text, num_questions_per_chunk=num_questions_per_chunk)\n",
        "        response = client.completions.create(\n",
        "            model=\"gpt-3.5-turbo-instruct\",\n",
        "            prompt=query,\n",
        "            max_tokens=100,\n",
        "            temperature=0.7\n",
        "        )\n",
        " \n",
        "        result = response.choices[0].text.strip().split(\"\\n\")\n",
        "        questions = [re.sub(r\"^\\d+\\.\\s*\", \"\", question).strip() for question in result if question.strip()]\n",
        "        \n",
        "        for question in questions:\n",
        "            question_id = str(uuid.uuid4())\n",
        "            queries[question_id] = question\n",
        "            relevant_docs[question_id] = [node_id]\n",
        "            \n",
        "    return queries, relevant_docs"
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714025807939
        }
      },
      "id": "98f68027-0108-4793-9459-a700457c7264"
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    \n",
        "   \n",
        "\n",
        "    OPENAI_API_KEY = \"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\"\n",
        "    os.environ[\"OPENAI_API_KEY\"]=OPENAI_API_KEY\n",
        "\n",
        "    \n",
        "\n",
        "    # 파일 경로 설정\n",
        "    DATA_FILE_PATH = '/home/azureuser/cloudfiles/code/Users/hb.suh/사출성형/tokenizer_data'\n",
        "    OUTPUT_DIR = '/home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/generated_QAdata/SementicChunking'\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "    all_texts = []\n",
        "    \n",
        "    # 폴더 내의 모든 txt 파일의 데이터를 모음\n",
        "    for file in os.listdir(DATA_FILE_PATH):\n",
        "       \n",
        "        file_path = os.path.join(DATA_FILE_PATH, file)\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            all_texts.extend(file.readlines())\n",
        "    \n",
        "    # 학습 , 검증으로 나누기\n",
        "    train, val = split_trainval(all_texts)\n",
        "    \n",
        "    # 각각의 디렉터리(train_corpus, val_corpus)에 corpus.txt라는 이름으로 저장\n",
        "    TRAIN_CORPUS_DIR = os.path.join(OUTPUT_DIR, 'train_corpus')\n",
        "    VAL_CORPUS_DIR = os.path.join(OUTPUT_DIR, 'val_corpus')\n",
        "    os.makedirs(TRAIN_CORPUS_DIR, exist_ok=True)\n",
        "    os.makedirs(VAL_CORPUS_DIR, exist_ok=True)\n",
        "\n",
        "    save_corpus(train, TRAIN_CORPUS_DIR)\n",
        "    save_corpus(val, VAL_CORPUS_DIR)"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {
        "gather": {
          "logged": 1714025812195
        }
      },
      "id": "9b14fbfa-c624-462b-b0f7-a400ea762b01"
    },
    {
      "cell_type": "code",
      "source": [
        "# 코퍼스 로드\n",
        "TRAIN_CORPUS_path = \"/home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/generated_QAdata/SementicChunking/train_corpus/corpus.txt\"\n",
        "VAL_CORPUS_path = \"/home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/generated_QAdata/SementicChunking/val_corpus/corpus.txt\"\n",
        "\n",
        "\n",
        "TRAIN_FILES = [\"/home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/generated_QAdata/SementicChunking/train_corpus/corpus.txt\"]\n",
        "VAL_FILES = [\"/home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/generated_QAdata/SementicChunking/val_corpus/corpus.txt\"]\n",
        "\n",
        "train_corpus = load_corpus(TRAIN_FILES, verbose=True) # 딕셔너리\n",
        "#print(train_corpus)\n",
        "val_corpus = load_corpus(VAL_FILES, verbose=True)\n",
        "\n",
        "\n",
        "# Generate queries\n",
        "# 질문 & 질문에 관련성 높은 문서\n",
        "train_queries, train_relevant_docs = generate_queries(train_corpus)\n",
        "val_queries, val_relevant_docs = generate_queries(val_corpus)\n",
        "\n",
        "\n",
        "# Save generated data\n",
        "save_to_json(train_queries, os.path.join(OUTPUT_DIR, 'train_queries.json'))\n",
        "save_to_json(train_relevant_docs, os.path.join(OUTPUT_DIR, 'train_relevant_docs.json'))\n",
        "save_to_json(val_queries, os.path.join(OUTPUT_DIR, 'val_queries.json'))\n",
        "save_to_json(val_relevant_docs, os.path.join(OUTPUT_DIR, 'val_relevant_docs.json'))\n",
        "\n",
        "# Define and save combined datasets\n",
        "TRAIN_DATASET_FPATH = '/home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/data/SementicChunking/train_dataset.json'\n",
        "VAL_DATASET_FPATH = '/home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/data/SementicChunking/val_dataset.json'\n",
        "train_dataset = {\n",
        "    'queries': train_queries,\n",
        "    'corpus': train_corpus,\n",
        "    'relevant_docs': train_relevant_docs,\n",
        "}\n",
        "val_dataset = {\n",
        "    'queries': val_queries,\n",
        "    'corpus': val_corpus,\n",
        "    'relevant_docs': val_relevant_docs,\n",
        "}\n",
        "\n",
        "os.makedirs('/home/azureuser/cloudfiles/code/Users/hb.suh/OUR_BERT/data/SementicChunking', exist_ok=True)\n",
        "with open(TRAIN_DATASET_FPATH, 'w+') as f:\n",
        "    json.dump(train_dataset, f, ensure_ascii=False)\n",
        "with open(VAL_DATASET_FPATH, 'w+') as f:\n",
        "    json.dump(val_dataset, f, ensure_ascii=False)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "IOPub data rate exceeded.\nThe Jupyter server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--ServerApp.iopub_data_rate_limit`.\n\nCurrent values:\nServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\nServerApp.rate_limit_window=3.0 (secs)\n\n100%|██████████| 4717/4717 [2:25:28<00:00,  1.85s/it]  \n  2%|▏         | 19/1029 [00:29<26:09,  1.55s/it]"
        }
      ],
      "execution_count": 18,
      "metadata": {
        "tags": [],
        "gather": {
          "logged": 1714009939086
        }
      },
      "id": "312d92cd-d679-4032-99f3-080d74cf8a09"
    },
    {
      "cell_type": "code",
      "source": [
        "!echo y | pip uninstall langchain\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\u001b[33mWARNING: Skipping langchain as it is not installed.\u001b[0m\r\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "88478db8-bb01-4648-9605-74678af70700"
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install \"langchain>=0.0.262,<=0.0.266\""
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Collecting langchain<=0.0.266,>=0.0.262\n  Using cached langchain-0.0.266-py3-none-any.whl (1.5 MB)\nRequirement already satisfied: numpy<2,>=1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from langchain<=0.0.266,>=0.0.262) (1.21.6)\nRequirement already satisfied: pydantic<2,>=1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from langchain<=0.0.266,>=0.0.262) (1.10.8)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0; python_version < \"3.11\" in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from langchain<=0.0.266,>=0.0.262) (4.0.2)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from langchain<=0.0.266,>=0.0.262) (2.0.29)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from langchain<=0.0.266,>=0.0.262) (3.9.5)\nRequirement already satisfied: requests<3,>=2 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from langchain<=0.0.266,>=0.0.262) (2.31.0)\nRequirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from langchain<=0.0.266,>=0.0.262) (0.5.14)\nRequirement already satisfied: PyYAML>=5.3 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from langchain<=0.0.266,>=0.0.262) (6.0.1)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from langchain<=0.0.266,>=0.0.262) (8.2.2)\nRequirement already satisfied: numexpr<3.0.0,>=2.8.4 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from langchain<=0.0.266,>=0.0.262) (2.8.4)\nRequirement already satisfied: langsmith<0.1.0,>=0.0.21 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from langchain<=0.0.266,>=0.0.262) (0.0.92)\nRequirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from langchain<=0.0.266,>=0.0.262) (1.2.4)\nRequirement already satisfied: typing-extensions>=4.2.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from pydantic<2,>=1->langchain<=0.0.266,>=0.0.262) (4.6.0)\nRequirement already satisfied: greenlet!=0.4.17; platform_machine == \"aarch64\" or (platform_machine == \"ppc64le\" or (platform_machine == \"x86_64\" or (platform_machine == \"amd64\" or (platform_machine == \"AMD64\" or (platform_machine == \"win32\" or platform_machine == \"WIN32\"))))) in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from SQLAlchemy<3,>=1.4->langchain<=0.0.266,>=0.0.262) (2.0.2)\nRequirement already satisfied: multidict<7.0,>=4.5 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<=0.0.266,>=0.0.262) (6.0.4)\nRequirement already satisfied: attrs>=17.3.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<=0.0.266,>=0.0.262) (23.1.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<=0.0.266,>=0.0.262) (1.9.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<=0.0.266,>=0.0.262) (1.3.1)\nRequirement already satisfied: frozenlist>=1.1.1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<=0.0.266,>=0.0.262) (1.3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from requests<3,>=2->langchain<=0.0.266,>=0.0.262) (2022.9.24)\nRequirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from requests<3,>=2->langchain<=0.0.266,>=0.0.262) (3.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from requests<3,>=2->langchain<=0.0.266,>=0.0.262) (3.1.0)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from requests<3,>=2->langchain<=0.0.266,>=0.0.262) (1.26.16)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain<=0.0.266,>=0.0.262) (0.9.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain<=0.0.266,>=0.0.262) (3.21.1)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain<=0.0.266,>=0.0.262) (1.0.0)\nRequirement already satisfied: packaging>=17.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain<=0.0.266,>=0.0.262) (23.2)\n\u001b[31mERROR: langchain-experimental 0.0.57 has requirement langchain<0.2.0,>=0.1.15, but you'll have langchain 0.0.266 which is incompatible.\u001b[0m\nInstalling collected packages: langchain\nSuccessfully installed langchain-0.0.266\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "dcb64998-986e-49b0-b108-5dc42d2176cf"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Collecting konlpy\n  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n\u001b[K     |████████████████████████████████| 19.4 MB 3.2 MB/s eta 0:00:01\n\u001b[?25hCollecting JPype1>=0.7.0\n  Downloading JPype1-1.5.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (488 kB)\n\u001b[K     |████████████████████████████████| 488 kB 71.1 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: numpy>=1.6 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from konlpy) (1.21.6)\nCollecting lxml>=4.1.0\n  Downloading lxml-5.2.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.0 MB)\n\u001b[K     |████████████████████████████████| 5.0 MB 73.6 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: packaging in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from JPype1>=0.7.0->konlpy) (23.2)\nInstalling collected packages: JPype1, lxml, konlpy\nSuccessfully installed JPype1-1.5.0 konlpy-0.6.0 lxml-5.2.1\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "7a139338-ec9a-43cf-a055-7c180c051882"
    },
    {
      "cell_type": "code",
      "source": [
        "pip list"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Package            Version\r\n------------------ -----------\r\nasttokens          2.4.1\r\nbackcall           0.2.0\r\ncomm               0.2.2\r\ndebugpy            1.8.1\r\ndecorator          5.1.1\r\nexecuting          2.0.1\r\nimportlib_metadata 7.1.0\r\nipykernel          6.29.4\r\nipython            8.12.3\r\njedi               0.19.1\r\njupyter_client     8.6.1\r\njupyter_core       5.7.2\r\nmatplotlib-inline  0.1.7\r\nnest-asyncio       1.6.0\r\npackaging          24.0\r\nparso              0.8.4\r\npexpect            4.9.0\r\npickleshare        0.7.5\r\npip                23.3.1\r\nplatformdirs       4.2.1\r\nprompt-toolkit     3.0.43\r\npsutil             5.9.8\r\nptyprocess         0.7.0\r\npure-eval          0.2.2\r\nPygments           2.18.0\r\npython-dateutil    2.9.0.post0\r\npyzmq              26.0.3\r\nsetuptools         69.5.1\r\nsix                1.16.0\r\nstack-data         0.6.3\r\ntornado            6.4\r\ntraitlets          5.14.3\r\ntyping_extensions  4.11.0\r\nwcwidth            0.2.13\r\nwheel              0.43.0\r\nzipp               3.18.1\r\nNote: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1715069343677
        }
      },
      "id": "d85b08b1-6837-4089-a7df-e542c58647ef"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "e8d37aa7-9084-42dd-af26-0294511502d5"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "generation",
      "language": "python",
      "display_name": "generation"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.19",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "generation"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}